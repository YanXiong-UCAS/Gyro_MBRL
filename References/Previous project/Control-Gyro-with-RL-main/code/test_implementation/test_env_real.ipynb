{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test gyroscope implementation in Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test environment class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import and load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy.integrate import solve_ivp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read data directly from gyro experiment\n",
    "'''\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "import random\n",
    "import time\n",
    "\n",
    "class GyroscopeRealEnv(gym.Env):\n",
    "\n",
    "    \"\"\"\n",
    "    GyroscopeEnv:\n",
    "        GyroscopeEnv is a GYM environment for Quanser 3-DOF gyroscope. The gyroscope consists of a disk mounted \n",
    "        inside an inner gimbal which in turn is mounted inside an outer gimbal.\n",
    "        The two gimbals are controlled by a RL controller, and the disk is controlled by a PID controller.\n",
    "    \n",
    "    State: \n",
    "        state = [x1, x2, x3, x4, x1_ref, x3_ref, w] (7 dimensions)\n",
    "        Outer red gimbal:\n",
    "            x1, or theta: angular position [rad]\n",
    "            x2, or dot(theta): angular velocity [rad/s]\n",
    "            x1_ref: angular position reference [rad]\n",
    "            u1: motor voltage [V]\n",
    "        Inner blue gimbal:\n",
    "            x3, or phi: angular position [rad]\n",
    "            x4, or dot(phi): angular velocity [rad/s]\n",
    "            x3_ref: angular position reference [rad]\n",
    "            u2: motor voltage [V]\n",
    "        Golden disk:\n",
    "            w: angular velocity [rad/s]\n",
    "            u3: motor voltage [V]\n",
    "        Mechanical constraints:\n",
    "            motor voltage: [-10, 10] [V]\n",
    "            gimbal velocity: [-100, 100] [rpm]\n",
    "            disk velocity: [-300, 300] [rpm]\n",
    "    \n",
    "    Observation:\n",
    "        observation = [cos(x1), sin(x1), x2, cos(x3), sin(x3), x4, x1_ref, x3_ref, w] (9 dimensions)\n",
    "        The angles have been replaced with their cosine and sine to prevent the discontinuity at -pi and pi.\n",
    "        The observation space is thus larger than the state space.\n",
    "        \n",
    "    Action:\n",
    "        action = [a1, a2]\n",
    "        Note: a1, a2 are normalized voltages\n",
    "              u1, u2 = 10*a1, 10*a2 are actual voltages\n",
    "              T1, T2 = KtotRed*u1, KtotBlue*u2 are motor torques\n",
    "        \n",
    "    Initialization:\n",
    "        Some versions of Gym may not support initialization with arguments, so initialize it manully with: \n",
    "        # create env\n",
    "        env = GyroscopeEnv()\n",
    "        env.init(simu_args = simu_args, reward_func = reward_func, reward_args = reward_args)\n",
    "        # simu_args, with optional simulation step (dt), episode length (ep_len), and random seed (seed)\n",
    "        simu_args = {'dt': 0.05, 'ep_len': 100, 'seed': 2}\n",
    "        # reward_func, optional reward function, default value is 'Quadratic'\n",
    "        reward_func = 'Quadratic'\n",
    "        # reward_args, optional reward parameters\n",
    "        reward_args = {'qx1': 1, 'qx2': 0.01, 'qx3': 1, 'qx4': 0.01, 'pu1': 0, 'pu2': 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Initialization ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #        \n",
    "                \n",
    "    def init(self, simu_args = {}, reward_func = 'PE', reward_args = {}):\n",
    "        \n",
    "        # Initialize mechanical parameters of the gyroscope\n",
    "        self.init_gyro()\n",
    "\n",
    "        # Initialize simulation parameters\n",
    "        self.init_simu(**simu_args)\n",
    "        \n",
    "        # Initialize reward parameters\n",
    "        self.init_reward(reward_func, reward_args)\n",
    "        \n",
    "        # State space, 7D\n",
    "        self.state_bound = np.array([self.maxAngle, self.maxGimbalSpeed, self.maxAngle, self.maxGimbalSpeed, \n",
    "                                   self.maxAngle, self.maxAngle, self.maxDiskSpeed], dtype = np.float32)\n",
    "        self.state_space = spaces.Box(low = -self.state_bound, high = self.state_bound, dtype = np.float32)\n",
    "        \n",
    "        # Observation space (normalized), 9D\n",
    "        self.observation_bound = np.array([1.0] * 9, dtype = np.float32) \n",
    "        self.observation_space = spaces.Box(low = -self.observation_bound, high = self.observation_bound, \n",
    "                                            dtype = np.float32)\n",
    "        \n",
    "        # Action space (normalized), 2D\n",
    "        self.action_bound = np.array([1.0] * 2, dtype = np.float32) \n",
    "        self.action_space = spaces.Box(low = -self.action_bound, high = self.action_bound, dtype = np.float32)\n",
    "    \n",
    "        # Reset\n",
    "        self.reset()\n",
    "        \n",
    "    # Initialize fixed parameters of the gyroscope\n",
    "    def init_gyro(self):\n",
    "        \n",
    "        # Mechanical constraints\n",
    "        self.maxVoltage = 10 # V\n",
    "        self.maxAngle = np.pi # rad\n",
    "        self.maxGimbalSpeed = 100 * 2 * np.pi / 60 # rad/s\n",
    "        self.maxDiskSpeed = 300 * 2 * np.pi / 60 # rad/s\n",
    "        \n",
    "    # Initialize simulation parameters\n",
    "    def init_simu(self, data, ep_len = 100):\n",
    "\n",
    "        # Save experiment data\n",
    "        self.data = data\n",
    "        \n",
    "        # Episode length and current step\n",
    "        self.ep_len = ep_len\n",
    "        self.ep_cur = 0\n",
    "        \n",
    "        # Seed for random number generation\n",
    "        self.seed(0)\n",
    "        self.viewer = None\n",
    "\n",
    "        # Gyroscope state and observation\n",
    "        self.state = np.array([0] * 7)\n",
    "        self.observe()\n",
    "        \n",
    "        # Pointer to indicate which episode we should use\n",
    "        self.epi_cur = -1\n",
    "        self.epi_len = len(data)\n",
    "        \n",
    "    # Initialize reward parameters\n",
    "    def init_reward(self, reward_func, reward_args):\n",
    "                \n",
    "        reward_dict = {\n",
    "            # continuous reward functions, part one\n",
    "            'Quadratic': self.quad_reward,\n",
    "            'Quadratic with bonus':self.quad_bon_reward,\n",
    "            'Quadratic with exponential': self.quad_exp_reward,\n",
    "            'Quadratic with ending penalty': self.quad_end_pen_reward,\n",
    "            'Quadratic with penalty': self.quad_pen_reward,\n",
    "            'Absolute': self.abs_reward,\n",
    "            'Normalized': self.norm_reward,\n",
    "            'Normalized with bonus': self.norm_bon_reward,\n",
    "            # continuous reward functions, part two\n",
    "            'Power':self.power_reward,\n",
    "            'Exponential': self.exp_reward,\n",
    "            'PE': self.power_exp_reward,\n",
    "        }\n",
    "        self.reward_func = reward_dict[reward_func]\n",
    "        self.reward_args = reward_args\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ----------------------------------------------- Step ----------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "    # Ignore the computed action signal\n",
    "    # Because we are training in an off-policy way\n",
    "    def step(self, a):\n",
    "\n",
    "        # action at t\n",
    "        a1, a2 = self.data[self.epi_cur][self.ep_cur][7:]\n",
    "           \n",
    "        # Increment counter of current episode\n",
    "        self.ep_cur += 1\n",
    "        \n",
    "        # state at t+1\n",
    "        x1, x2, x3, x4, x1_ref, x3_ref, w = self.data[self.epi_cur][self.ep_cur+1][:7]     \n",
    "        \n",
    "        # Angle error (normalized between pi and -pi to get smallest distance)\n",
    "        x1_diff = self.angle_normalize(x1 - x1_ref)\n",
    "        x3_diff = self.angle_normalize(x3 - x3_ref)\n",
    "\n",
    "        # update state and observation\n",
    "        self.state = np.array([x1, x2, x3, x4, x1_ref, x3_ref, w])\n",
    "        self.observe()\n",
    "\n",
    "        # Reward(float), normalized everything in advance\n",
    "        reward = self.reward_func(x1_diff/self.maxAngle, x3_diff/self.maxAngle, \n",
    "                                  x2/self.maxGimbalSpeed, x4/self.maxGimbalSpeed, \n",
    "                                  a1, a2, **self.reward_args)\n",
    "\n",
    "        # Done(bool): whether itâ€™s time to reset the environment again.\n",
    "        done = self.ep_cur > self.ep_len\n",
    "        \n",
    "        # Info(dict): diagnostic information useful for debugging. \n",
    "        info = {'state': self.state, 'observation': self.observation}\n",
    "                \n",
    "        return self.observation, reward, done, info\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part I ------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def abs_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*abs(x1_diff) + qx3*abs(x3_diff) + qx2*abs(x2) + qx4*abs(x4) + pu1*abs(u1) + pu2*abs(u2))\n",
    "\n",
    "    def norm_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def norm_bon_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0, bound = 0.001, bonus = 1):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    def quad_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def quad_exp_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, eax1 = 10, ebx1 = 10, eax3 = 10, ebx3 = 10):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + eax1*(1-np.exp(-ebx1*(x1_diff**2))) + eax3*(1-np.exp(-ebx3*(x3_diff**2))))\n",
    "\n",
    "    def quad_end_pen_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, sx1 = 10, sx3 = 10, end_horizon = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + end_horizon*(sx1*(x1_diff**2)+sx3*(x3_diff**2)))\n",
    "\n",
    "    def quad_pen_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, penalty = 50):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) - penalty*(abs(x1_diff) >= bound or abs(x3_diff) >= bound)\n",
    "\n",
    "    def quad_bon_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, bonus = 5):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part II ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def power_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.5):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p)\n",
    "        \n",
    "    def exp_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    def power_exp_reward(self, x1_diff, x3_diff, x2, x4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.1, e = 10):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p) -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------- Helper ---------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    # reset system to a given or random initial state\n",
    "    def reset(self, x_0 = None):\n",
    "       \n",
    "        # Avoid repeating reset that may waste data\n",
    "        if self.epi_cur == -1 or self.ep_cur > 0:\n",
    "            self.epi_cur += 1\n",
    "            self.ep_cur = 0\n",
    "            self.state = self.data[self.epi_cur][self.ep_cur]\n",
    "            self.observe()\n",
    "        \n",
    "        return self.observation\n",
    "        \n",
    "    # return normalized observation\n",
    "    def observe(self):\n",
    "        s = self.state\n",
    "        self.observation = np.array([np.cos(s[0]), np.sin(s[0]), s[1]/self.maxGimbalSpeed, \n",
    "                                     np.cos(s[2]), np.sin(s[2]), s[3]/self.maxGimbalSpeed, \n",
    "                                     s[4]/self.maxAngle, s[5]/self.maxAngle, s[6]/self.maxDiskSpeed])\n",
    "        return self.observation\n",
    "    \n",
    "    # Keep the angles between -lim and lim\n",
    "    def angle_normalize(self, x, lim = np.pi):\n",
    "        return ((x + lim) % (2 * lim)) - lim\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            \n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'real_gyro.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-c644e7de71da>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpkl_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'real_gyro.pkl'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpkl_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mpkl_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'real_gyro.pkl'"
     ]
    }
   ],
   "source": [
    "pkl_file = open('real_gyro.pkl', 'rb')\n",
    "data = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization args\n",
    "simu_args = {\n",
    "    'ep_len': 100,\n",
    "    'data': data\n",
    "}\n",
    "reward_func = 'PE'\n",
    "reward_args = {\n",
    "    'qx1': 1, \n",
    "    'qx2': 0.2, \n",
    "    'qx3': 1, \n",
    "    'qx4': 0.2, \n",
    "    'pu1': 0.1, \n",
    "    'pu2': 0.1,\n",
    "    'p': 0.1,\n",
    "    'e': 40\n",
    "}\n",
    "\n",
    "# Create env\n",
    "env = GyroscopeRealEnv()\n",
    "env.init(simu_args = simu_args, reward_func = reward_func, reward_args = reward_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run tests with simply input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial state\n",
    "observation = env.reset()\n",
    "\n",
    "time = np.arange(0, 4.95, 0.05)\n",
    "state_record = np.empty([len(time), len(x_0)])\n",
    "reward_record = np.empty([len(time), 1])\n",
    "action_record = np.empty([len(time), 2])\n",
    "\n",
    "for i in range(len(time)):\n",
    "    action = [0,0]     \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    state_record[i] = env.state\n",
    "    reward_record[i] = reward\n",
    "    action_record[i] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(4,2,figsize=(19,19))\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.title('Red gimbal angle',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\theta$ [rad]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,0],'r-')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "plt.title('Blue gimbal angle',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\phi$ [rad]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,2],'b-')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.title('Red gimbal speed',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\dot \\theta$ [rad/s]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,1],'r-')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "plt.title('Blue gimbal speed',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\dot \\phi$ [rad/s]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,3],'b-')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.title('Red gimbal input',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel('voltage [V]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,action_record[:,0]*10,'r-')\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "plt.title('Blue gimbal input',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel('voltage [V]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,action_record[:,1]*10,'b-')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.title('Red gimbal tracking error',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\theta$ error [rad]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,0]-state_record[:,4],'r-')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "plt.title('Blue gimbal tracking error',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel(r'$\\phi$ error [rad]',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,2]-state_record[:,5],'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Reward',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel('Reward',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,reward_record,'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Disk speed',fontsize=20)\n",
    "plt.xlabel('time [s]',fontsize=16)\n",
    "plt.ylabel('Disk speed',fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(time,state_record[:,-1],'r-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}