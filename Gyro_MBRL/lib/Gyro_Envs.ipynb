{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义环境 --- Custom environment #\n",
    "\n",
    "'''\n",
    "Difference with GyroscopeEnvV0:\n",
    "-- Gimbal voelocity no longer clipped into [-max velocity, max velocity] range\n",
    "-- Add several reward functions: exponential, power, sparse, etc...\n",
    "-- Add a termination condition for sparse reward funciton\n",
    "'''\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy.integrate import solve_ivp\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c697e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GyroscopeEnvV1(gym.Env):\n",
    "\n",
    "    \"\"\"\n",
    "    GyroscopeEnv:\n",
    "        GyroscopeEnv is a GYM environment for Quanser 3-DOF gyroscope. The gyroscope consists of a disk mounted \n",
    "        inside an inner gimbal which in turn is mounted inside an outer gimbal.\n",
    "        The two gimbals are controlled by a RL controller, and the disk is controlled by a PID controller.\n",
    "    \n",
    "    State:   # 状态值 --- 7D   都是原始值？？？？？？？\n",
    "        state = [x1, x2, x3, x4, x1_ref, x3_ref, w] (7 dimensions)   # 状态空间\n",
    "        Outer red gimbal:   # 外部红色的万向节\n",
    "            x1, or theta: angular position [rad]   # 角度位置\n",
    "            x2, or dot(theta): angular velocity [rad/s]   # 角速度\n",
    "            x1_ref: angular position reference [rad]   # 目标角度位置\n",
    "            u1: motor voltage [V]   # 电机电压\n",
    "        Inner blue gimbal:   # 内部蓝色万向节\n",
    "            x3, or phi: angular position [rad]   # 角度位置\n",
    "            x4, or dot(phi): angular velocity [rad/s]   # 角速度\n",
    "            x3_ref: angular position reference [rad]   # 目标角度位置\n",
    "            u2: motor voltage [V]   # 电机电压\n",
    "        Golden disk:   # 金黄色转盘\n",
    "            w: angular velocity [rad/s]   # 角速度\n",
    "            u3: motor voltage [V]   # 电机电压\n",
    "        Mechanical constraints:   # 机械约束，参数边界\n",
    "            motor voltage: [-10, 10] [V]   # 电机电压\n",
    "            gimbal velocity: [-100, 100] [rpm]   # 万向节转速\n",
    "            disk velocity: [-300, 300] [rpm]   # 转盘转速\n",
    "    \n",
    "    Observation:   # 观测值 --- 9D   所有数字需要归一化？？？？？？？\n",
    "        observation = [cos(x1), sin(x1), x2, cos(x3), sin(x3), x4, x1_ref, x3_ref, w] (9 dimensions)   # 观测空间\n",
    "        The angles have been replaced with their cosine and sine to prevent the discontinuity at -pi and pi.\n",
    "        The observation space is thus larger than the state space.\n",
    "        \n",
    "    Action:   # 动作空间\n",
    "        action = [a1, a2]   # 动作空间\n",
    "        Note: a1, a2 are normalized voltages   # 归一化电压\n",
    "              u1, u2 = 10*a1, 10*a2 are actual voltages   # 实际电压指\n",
    "              T1, T2 = KtotRed*u1, KtotBlue*u2 are motor torques   # 电机扭矩，KtoRed和KtoBlue是电机属性参数\n",
    "        \n",
    "    Initialization:   # 初始化\n",
    "        Some versions of Gym may not support initialization with arguments, so initialize it manully with: \n",
    "        # create env   # 创建环境\n",
    "        env = GyroscopeEnv()\n",
    "        env.init(simu_args = simu_args, reward_func = reward_func, reward_args = reward_args)\n",
    "        # simu_args, with optional simulation step (dt), episode length (ep_len), and random seed (seed)   # 仿真三个基本参数\n",
    "        simu_args = {'dt': 0.05, 'ep_len': 100, 'seed': 2， ‘friction’: False}   # 未考虑摩擦\n",
    "        # reward_func, optional reward function, default value is 'Quadratic'   # 奖励函数，修改后没有默认值，必须附值\n",
    "        reward_func = 'Quadratic'\n",
    "        # reward_args, optional reward parameters  # 用于计算奖励函数的权重参数\n",
    "        reward_args = {'qx1': 1, 'qx2': 0.01, 'qx3': 1, 'qx4': 0.01, 'pu1': 0, 'pu2': 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Initialization ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #        \n",
    "                \n",
    "    def __init__(self, env_config):\n",
    "        \n",
    "        # Initialize mechanical parameters of the gyroscope   # 初始化Gyro机械参数\n",
    "        self.init_gyro()\n",
    "        \n",
    "        # Initialize simulation parameters   # 初始化仿真参数\n",
    "        self.init_simu(**env_config[\"simu_args\"])\n",
    "        \n",
    "        # Initialize reward parameters   # 初始化奖励函数参数，用于选择奖励函数类型\n",
    "        self.init_reward(env_config[\"reward_func\"], env_config[\"reward_args\"])\n",
    "        \n",
    "        # State space, 7D   # 状态空间初始化设置\n",
    "        self.state_bound = np.array([self.maxAngle, self.maxGimbalSpeed, self.maxAngle, self.maxGimbalSpeed, \n",
    "                                   self.maxAngle, self.maxAngle, self.maxDiskSpeed], dtype = np.float32)   # 状态空间初始化空间大小，设置最大值\n",
    "        self.state_space = spaces.Box(low = -self.state_bound, high = self.state_bound, dtype = np.float32)   # 状态空间上下限设置\n",
    "        \n",
    "        # Observation space (normalized), 9D   # 观测空间初始化设置（归一化）\n",
    "        self.observation_bound = np.array([1.0] * 9, dtype = np.float32)   # 观测空间初始化空间大小，设置空间维数 \n",
    "        self.observation_space = spaces.Box(low = -self.observation_bound, high = self.observation_bound, \n",
    "                                            dtype = np.float32)   # 观测空间上下限设置\n",
    "#         print(\"self.observation_bound：\", self.observation_bound)\n",
    "#         print(\"self.observation_space：\", self.observation_space)\n",
    "        \n",
    "        # Action space (normalized), 2D   # 动作空间（归一化）\n",
    "        self.action_bound = np.array([1.0] * 2, dtype = np.float32)   # 动作空间设置，设置空间维数大小\n",
    "        self.action_space = spaces.Box(low = -self.action_bound, high = self.action_bound, dtype = np.float32)   # 动作空间上下限设置\n",
    "    \n",
    "    # Initialize fixed parameters of the gyroscope   # 初始化Gyro的特定机械参数\n",
    "    def init_gyro(self):\n",
    "        \n",
    "        # Inertias in Kg*m2, from SP report page 23, table 2   # 设备固定参数\n",
    "        self.Jrx1 = 0.0179\n",
    "        self.Jbx1 = 0.0019\n",
    "        self.Jbx2 = 0.0008\n",
    "        self.Jbx3 = 0.0012\n",
    "        self.Jdx1 = 0.0028\n",
    "        self.Jdx2 = 0.0056\n",
    "        self.Jdx3 = 0.0056\n",
    "#         print(\"Gyro的特定机械参数：\")\n",
    "#         print(\"self.Jrx1=\" + str(self.Jrx1),\n",
    "#              \"self.Jbx1=\" + str(self.Jbx1),\n",
    "#              \"self.Jbx2\" + str(self.Jbx2),\n",
    "#              \"self.Jbx3\" + str(self.Jbx3),\n",
    "#              \"self.Jdx1\" + str(self.Jdx1),\n",
    "#              \"self.Jdx2\" + str(self.Jdx2),\n",
    "#              \"self.Jdx3\" + str(self.Jdx3))\n",
    "    \n",
    "\n",
    "        # Combined inertias to simplify equations, from SP report page 22, state space equations   # 状态空间空间方程，组合惯量\n",
    "        self.J1 = self.Jbx1 - self.Jbx3 + self.Jdx1 - self.Jdx3\n",
    "        self.J2 = self.Jbx1 + self.Jdx1 + self.Jrx1\n",
    "        self.J3 = self.Jbx2 + self.Jdx1\n",
    "\n",
    "        # Motor constants, from SP report page 23, table 1   # 电机参数\n",
    "        self.Kamp = 0.5 # current gain, A/V   # 电流增益\n",
    "        self.Ktorque = 0.0704 # motor gain, Nm/A   # 电机增益\n",
    "        self.eff = 0.86 # motor efficiency   # 电机效率\n",
    "        self.nRed = 1.5 # red gearbox eatio   # 红色万向节变速箱传动比\n",
    "        self.nBlue = 1 # blue gearbox eatio   # 蓝色万向节变速箱传动比\n",
    "        self.KtotRed = self.Kamp * self.Ktorque * self.eff * self.nRed # Nm/V   # 红色万向节扭矩，特定计算公式\n",
    "        self.KtotBlue = self.Kamp * self.Ktorque * self.eff * self.nBlue # Nm/V   # 蓝色万向节扭矩，特定计算公式\n",
    "        \n",
    "        # Mechanical constraints   机械系统约束\n",
    "        self.maxVoltage = 10 # V   # 最大电压\n",
    "        self.maxAngle = np.pi # rad   # 最大角度\n",
    "        self.maxGimbalSpeed = 100 * 2 * np.pi / 60 # rad/s   # 最大万向节转速\n",
    "        self.maxDiskSpeed = 300 * 2 * np.pi / 60 # rad/s   # 最大转盘转速\n",
    "#         print(\"Max speed of Gimbal:\", self.maxGimbalSpeed )\n",
    "#         print(\"Max speed of Disk:\", self.maxDiskSpeed)\n",
    "        \n",
    "    # Initialize simulation parameters   # 初始化仿真参数\n",
    "    def init_simu(self, dt = 0.05, ep_len = 100, seed = 2, friction = False):\n",
    "        \n",
    "        # Gyroscope state and observation   # Gyro状态空间与观测值空间\n",
    "        self.state = np.array([0] * 7)   # Gyro状态空间\n",
    "        self.observe()   # Gyro观测空间   自动打印出观测空间？？？？？？\n",
    "\n",
    "        # Time step in s   # 时间步长\n",
    "        self.dt = dt\n",
    "        self.eval_per_dt = int(dt / 0.01) # run evaluation every 0.01s   # ??????????????????????????????????\n",
    "        \n",
    "        # Episode length and current episode   # 剧集长度，一个epoch由多个episode组成，所有episode运行结束进行一次判断\n",
    "        self.ep_len = ep_len   # 剧集长度设置\n",
    "        self.ep_cur = 0   # 当前剧集数\n",
    "#         print(\"ep_len：\" + str(ep_len))\n",
    "        \n",
    "        # Seed for random number generation   # 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed()值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。\n",
    "        self.seed(seed)\n",
    "        self.viewer = None\n",
    "#         print(\"seed：\" + str(seed))\n",
    "        \n",
    "        # Friction   摩擦力\n",
    "        self.fvr = 0.002679 if friction else 0   # 存在选项，如果考虑摩擦，则赋值，如果不考虑摩擦则归零\n",
    "        self.fcr = 0\n",
    "        self.fvb = 0.005308 if friction else 0   # 存在选项，如果考虑摩擦，则赋值，如果不考虑摩擦则归零\n",
    "        self.fcb = 0\n",
    "        \n",
    "    # Initialize reward parameters   # 初始化奖励参数，用于定义各类奖励函数的ID，便于调用\n",
    "    def init_reward(self, reward_func, reward_args):\n",
    "                \n",
    "        reward_dict = {\n",
    "            # continuous reward functions, part one   # 连续奖励函数，part one\n",
    "            'Quadratic': self.quad_reward,\n",
    "            'Quadratic with bonus':self.quad_bon_reward,\n",
    "            'Quadratic with exponential': self.quad_exp_reward,\n",
    "            'Quadratic with ending penalty': self.quad_end_pen_reward,\n",
    "            'Quadratic with penalty': self.quad_pen_reward,\n",
    "            'Absolute': self.abs_reward,\n",
    "            'Normalized': self.norm_reward,\n",
    "            'Normalized with bonus': self.norm_bon_reward,\n",
    "            # continuous reward functions, part two   # 连续奖励函数，part two\n",
    "            'Power':self.power_reward,\n",
    "            'Exponential': self.exp_reward,\n",
    "            'PE': self.power_exp_reward,\n",
    "            # sparse reward functions   # 稀疏奖励函数\n",
    "            'Sparse':self.sparse_reward,\n",
    "            'Sparse with exp': self.sparse_reward_with_exp,\n",
    "            'Sparse with exp 2': self.sparse_reward_with_exp_2\n",
    "        }\n",
    "        if reward_func in ['Sparse']: # 'Sparse with exp'\n",
    "            self.sparse = True\n",
    "        else:\n",
    "            self.sparse = False\n",
    "        self.reward_func = reward_dict[reward_func]\n",
    "        self.reward_args = reward_args\n",
    "#         print(\"reward_dict[reward_func]:\" + str(reward_dict[reward_func]))\n",
    "#         print(\"reward_args:\" + str(reward_args))\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ----------------------------------------------- Step ----------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    # Simulate the environment fot one step dt   # 计算一个步长(step)的仿真环境，也就是计算了ep_len(100)次\n",
    "    def step(self, a):\n",
    "        \n",
    "        # extract states and actions   # 提取状态值和动作值\n",
    "        x1, x2, x3, x4, x1_ref, x3_ref, w = self.state   # 状态空间取值\n",
    "        a1, a2 = a   # 动作空间取值\n",
    "        u1, u2 = self.maxVoltage * a1, self.maxVoltage * a2   # 输出电压赋值\n",
    "\n",
    "        # Increment episode   # 递增量，每次递增1\n",
    "        self.ep_cur += 1\n",
    "#         print('/'+'-'*42+f'{datetime.now()} 开始训练' + '-'*36 + '\\\\')\n",
    "#         print(f\"Episode {self.ep_cur}\\n >> >> >> >> >> >> >>\")\n",
    "        \n",
    "\n",
    "        # For quad_end_pen_reward, check if terminal state is reached   # 检查是否达到终止状态\n",
    "        if self.reward_func == self.quad_end_pen_reward and self.ep_cur == self.ep_len:   # ？？？？？？\n",
    "            self.reward_args['end_horizon'] = 1   # ？？？？？？？？？？\n",
    "            \n",
    "#         print(self.quad_end_pen_reward)   # ???????\n",
    "\n",
    "        # run simulation for a step   # 跑一个步长的仿真\n",
    "        results = solve_ivp(   # solve_ivp, 该函数对给定初始值的常微分方程组进行数值积分, https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html\n",
    "                    fun = self.dxdt,    # 调用函数fun(t, y)\n",
    "                    t_span = (0, self.dt), # solver starts with t = 0 and integrates until it reaches t = self.dt\n",
    "                    y0 = [x1, x2, x3, x4], # initial state\n",
    "                    method = 'RK45',   # ODE solver --- Explicit Runge-Kutta method of order 5(4)\n",
    "                    t_eval = np.linspace(0, self.dt, self.eval_per_dt), # times at which to store the computed solution\n",
    "                    args = (u1, u2) \n",
    "                )\n",
    "        # print(\"results:\", results)   # 计算了ep_len(100)次，\n",
    "        \n",
    "        # evaluated states, each contains eval_per_dt points   # 已评估的状态，每个都包含eval_per_dt点\n",
    "        x1_eval = results.y[0]\n",
    "        x2_eval = results.y[1]\n",
    "        x3_eval = results.y[2]\n",
    "        x4_eval = results.y[3]\n",
    "#         print(\"已评估的状态：\")\n",
    "#         print(\"x2_eval：\" + str(x2_eval))\n",
    "#         print(\"x4_eval：\" + str(x4_eval))\n",
    "\n",
    "        # change in velocity, or acceleration   # 速度变化，评估值与状态值之间的变化\n",
    "        dx2 = x2_eval[-1] - x2\n",
    "        dx4 = x4_eval[-1] - x4\n",
    "#         print(\"万向节转速变化：\")\n",
    "#         print(\"x2_eval[-1]：\" +str(x2_eval[-1]))\n",
    "#         print(\"x2：\" + str(x2))\n",
    "#         print(\"dx2：\" + str(dx2))\n",
    "#         print(\"x4_eval[-1]：\" +str(x4_eval[-1]))\n",
    "#         print(\"x4：\" + str(x4))\n",
    "#         print(\"dx4：\" + str(dx4))        \n",
    "        \n",
    "        \n",
    "        # keep only the last evaluation value   # 只保留最后的评估值\n",
    "        x1 = x1_eval[-1]\n",
    "        x2 = x2_eval[-1]\n",
    "        x3 = x3_eval[-1]\n",
    "        x4 = x4_eval[-1]\n",
    "#         print(\"只保留最后的评估值：\")\n",
    "#         print(\"x2_eval[-1]:\" + str(x2_eval[-1]))\n",
    "#         print(\"x4_eval[-1]:\" + str(x4_eval[-1]))\n",
    "        \n",
    "        # Angle error (normalized between pi and -pi to get smallest distance)   # 角度误差（在pi和-pi之间归一化，以得到最小的距离\n",
    "        x1_diff = self.angle_normalize(x1 - x1_ref)\n",
    "        x3_diff = self.angle_normalize(x3 - x3_ref)\n",
    "#         print(\"角度误差归一化：\")\n",
    "#         print(\"x1_diff:\" + str(x1_diff))\n",
    "#         print(\"x3_diff:\" + str(x3_diff))\n",
    "        \n",
    "        # update state and observation   # 更新状态空间和观测空间，其中的观测空间应该归一化？？？？？？？\n",
    "        self.state = np.array([x1, x2, x3, x4, x1_ref, x3_ref, w])   # 状态空间\n",
    "        self.observe()   # 观测空间，自动打印出观测空间？？？？？？\n",
    "#         print(\"更新状态空间和观测空间：\")\n",
    "#         print(\"self.state：\" + str(self.state))\n",
    "#         print(\"self.observe():\" + str(self.observe()))\n",
    "\n",
    "        # Reward(float), normalized everything in advance\n",
    "        reward = self.reward_func(x1_diff/self.maxAngle, x3_diff/self.maxAngle, \n",
    "                                  x2/self.maxGimbalSpeed, x4/self.maxGimbalSpeed, \n",
    "                                  dx2/self.maxGimbalSpeed, dx4/self.maxGimbalSpeed,\n",
    "                                  a1, a2, **self.reward_args)\n",
    "#         print(\"奖励函数类型：\" + str(env_config[\"reward_func\"]))\n",
    "#         print(\"reward：\" + str(reward))\n",
    "#         print('/'+'-'*42+f'{datetime.now()} 训练结束' + '-'*36 + '\\\\')\n",
    "        \n",
    "        # Done(bool): whether it’s time to reset the environment again.\n",
    "        if self.sparse:\n",
    "            # in sparse reward functions, terminate the episode when the speed is too large\n",
    "            # otherwise the exploration will happen mainly in high speed area, which is not desired\n",
    "            done = self.ep_cur > self.ep_len or x2 > 2*self.maxGimbalSpeed or x4 > 2 * self.maxGimbalSpeed\n",
    "        else:\n",
    "            # in other reward functions, terminating the episode early will encourage the agent to \n",
    "            # speed up the gyroscope and end the episode, because the reward is negative\n",
    "            done = self.ep_cur > self.ep_len\n",
    "        \n",
    "        # Info(dict): diagnostic information useful for debugging. \n",
    "        info = {'state': self.state, 'observation': self.observation}\n",
    "        \n",
    "        return self.scaled_observation(), reward, done, info #return self.observation, reward, done, info\n",
    "    \n",
    "    # Compute the derivative of the state, here u is NOT normalized   # 计算状态值的导数，此处的u未归一化\n",
    "    def dxdt(self, t, x, u1, u2):\n",
    "\n",
    "        J1, J2, J3, Jdx3 = self.J1, self.J2, self.J3, self.Jdx3\n",
    "        w = self.state[-1]\n",
    "\n",
    "        # Convert input voltage to input torque   # 将输入电压转换为输入扭矩\n",
    "        T1, T2 = self.KtotRed * u1, self.KtotBlue * u2   # 此处的计算方式是设备固定特定计算公式\n",
    "        \n",
    "        # Friction   # 摩擦力\n",
    "        T1 = T1 - self.fvr*x[1] - self.fcr*np.sign(x[1])\n",
    "        T2 = T2 - self.fvb*x[3] - self.fcb*np.sign(x[3])\n",
    "        \n",
    "        # Equations of motion   # 运动方程\n",
    "        dx_dt = [0, 0, 0, 0]\n",
    "        dx_dt[0] = x[1]\n",
    "        dx_dt[1] = (T1+J1*np.sin(2*x[2])*x[1]*x[3]-Jdx3*np.cos(x[2])*x[3]*w)/(J2 + J1*np.power(np.sin(x[2]),2))\n",
    "        dx_dt[2] = x[3]\n",
    "        dx_dt[3] = (T2-J1*np.cos(x[2])*np.sin(x[2])*np.power(x[1],2)+Jdx3*np.cos(x[2])*x[1]*w)/J3\n",
    "        \n",
    "        return dx_dt\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part I ------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def abs_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*abs(x1_diff) + qx3*abs(x3_diff) + qx2*abs(x2) + qx4*abs(x4) + pu1*abs(u1) + pu2*abs(u2))\n",
    "\n",
    "    def norm_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def norm_bon_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0, bound = 0.001, bonus = 1):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    def quad_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def quad_exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, eax1 = 10, ebx1 = 10, eax3 = 10, ebx3 = 10):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + eax1*(1-np.exp(-ebx1*(x1_diff**2))) + eax3*(1-np.exp(-ebx3*(x3_diff**2))))\n",
    "\n",
    "    def quad_end_pen_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, sx1 = 10, sx3 = 10, end_horizon = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + end_horizon*(sx1*(x1_diff**2)+sx3*(x3_diff**2)))\n",
    "\n",
    "    def quad_pen_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, penalty = 50):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) - penalty*(abs(x1_diff) >= bound or abs(x3_diff) >= bound)\n",
    "\n",
    "    def quad_bon_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, bonus = 5):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part II ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def power_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.5):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p)\n",
    "        \n",
    "    def exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    def power_exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.1, e = 10):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p) -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part III ----------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "    def sparse_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, bx = 0.01, rx = 1, bv = 0.01, rv = 0, bu = 0.01, ru = 0):\n",
    "        r = 0\n",
    "        if abs(x1_diff) <= bx and abs(x3_diff) <= bx:\n",
    "            if abs(x2) <= bv and abs(x4) <= bv:\n",
    "                r += rv\n",
    "            if abs(dx2) <= bu and abs(dx4) <= bu:\n",
    "                r += ru\n",
    "            r += rx\n",
    "        return r\n",
    "\n",
    "    def sparse_reward_with_exp(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10, bound = 0.01, reward = 1):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2)))) + reward*(abs(x1_diff) <= bound and abs(x3_diff) <= bound)\n",
    "    \n",
    "    def sparse_reward_with_exp_2(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10, bound = 0.01, reward = 1):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2)))) + reward*(abs(x1_diff) <= bound) + reward*(abs(x3_diff) <= bound)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------- Helper ---------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    # reset system to a given or random initial state\n",
    "    def reset(self, x_0 = None):\n",
    "        \n",
    "        # reset state\n",
    "        if x_0 is None:\n",
    "            self.state = self.state_space.sample()\n",
    "        else:\n",
    "            self.state = x_0\n",
    "        # update observation\n",
    "        self.observe()\n",
    "        #print(self.observe())\n",
    "        # reset counter\n",
    "        self.ep_cur = 0\n",
    "        \n",
    "        return self.observation\n",
    "        \n",
    "    # return normalized observation  返回归一化观测值，从状态空间求解得到观测值空间\n",
    "    def observe(self):\n",
    "        s = self.state\n",
    "        self.observation = np.array([np.cos(s[0]), np.sin(s[0]), s[1]/self.maxGimbalSpeed, \n",
    "                                     np.cos(s[2]), np.sin(s[2]), s[3]/self.maxGimbalSpeed, \n",
    "                                     s[4]/self.maxAngle, s[5]/self.maxAngle, s[6]/self.maxDiskSpeed])\n",
    "#         print(\"s[1]:\" + str(s[1]))\n",
    "#         print(\"self.maxGimbalSpeed:\" + str(self.maxGimbalSpeed))\n",
    "#         print(\"s[3]:\" + str(s[3]))\n",
    "#         print(\"self.maxGimbalSpeed:\" + str(self.maxGimbalSpeed))\n",
    "#         print(\"self.observation:\", self.observation)\n",
    "#         print(\"self.observation_space:\", self.observation_space)\n",
    "        return self.observation\n",
    "    \n",
    "    def scaled_observation(self):\n",
    "        return self.observation * 0.01  # Keep the angles between -lim and lim   # 将角度设定在上下限之间，归一化，是否可以采用其他归一化方法呢？？？？？？\n",
    "    \n",
    "    def angle_normalize(self, x, lim = np.pi):\n",
    "        return ((x + lim) % (2 * lim)) - lim   # % --- 取模 - 返回除法的余数\n",
    "    \n",
    "    def seed(self, seed=None):   # # 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed()值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            \n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
