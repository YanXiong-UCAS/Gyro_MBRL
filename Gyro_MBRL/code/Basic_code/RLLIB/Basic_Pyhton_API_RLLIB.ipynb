{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "2021-08-05 17:52:52,385\tINFO services.py:1247 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8267\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.0/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 PENDING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m 2021-08-05 17:52:58,722\tINFO trainer.py:706 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m 2021-08-05 17:52:58,722\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m 2021-08-05 17:52:58,902\tINFO trainer.py:706 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m 2021-08-05 17:52:58,902\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m 2021-08-05 17:52:58,970\tINFO trainer.py:706 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m 2021-08-05 17:52:58,970\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156928)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156923)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156930)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m /home/xiongyan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001B[2m\u001B[36m(pid=156937)\u001B[0m 2021-08-05 17:53:05,853\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=156935)\u001B[0m 2021-08-05 17:53:05,990\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=156931)\u001B[0m 2021-08-05 17:53:06,071\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-14\n",
      "  done: false\n",
      "  episode_len_mean: 22.320224719101123\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 87.0\n",
      "  episode_reward_mean: 22.320224719101123\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 178\n",
      "  episodes_total: 178\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.6533623337745667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04126758500933647\n",
      "          model: {}\n",
      "          policy_loss: -0.05519212409853935\n",
      "          total_loss: 78.09415435791016\n",
      "          vf_explained_var: 0.3044193387031555\n",
      "          vf_loss: 78.1410903930664\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.2923076923077\n",
      "    ram_util_percent: 70.66923076923078\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04962836763495894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05382688246319157\n",
      "    mean_inference_ms: 0.7385413249234383\n",
      "    mean_raw_obs_processing_ms: 0.08034706115722656\n",
      "  time_since_restore: 8.55289888381958\n",
      "  time_this_iter_s: 8.55289888381958\n",
      "  time_total_s: 8.55289888381958\n",
      "  timers:\n",
      "    learn_throughput: 860.147\n",
      "    learn_time_ms: 4650.368\n",
      "    load_throughput: 58337.678\n",
      "    load_time_ms: 68.566\n",
      "    sample_throughput: 1067.964\n",
      "    sample_time_ms: 3745.444\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1628178794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          8.5529</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.3202</td><td style=\"text-align: right;\">                  87</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           22.3202</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-14\n",
      "  done: false\n",
      "  episode_len_mean: 22.988505747126435\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 81.0\n",
      "  episode_reward_mean: 22.988505747126435\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6588622331619263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03592158481478691\n",
      "          model: {}\n",
      "          policy_loss: -0.04675012454390526\n",
      "          total_loss: 86.91474914550781\n",
      "          vf_explained_var: 0.2793940007686615\n",
      "          vf_loss: 86.95431518554688\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.47692307692309\n",
      "    ram_util_percent: 70.67692307692309\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05174058581912141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055099302338111784\n",
      "    mean_inference_ms: 0.7555052150162838\n",
      "    mean_raw_obs_processing_ms: 0.08031661079633658\n",
      "  time_since_restore: 8.489008665084839\n",
      "  time_this_iter_s: 8.489008665084839\n",
      "  time_total_s: 8.489008665084839\n",
      "  timers:\n",
      "    learn_throughput: 892.063\n",
      "    learn_time_ms: 4483.99\n",
      "    load_throughput: 43477.243\n",
      "    load_time_ms: 92.002\n",
      "    sample_throughput: 1045.681\n",
      "    sample_time_ms: 3825.257\n",
      "    update_time_ms: 2.137\n",
      "  timestamp: 1628178794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-14\n",
      "  done: false\n",
      "  episode_len_mean: 22.754285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.0\n",
      "  episode_reward_mean: 22.754285714285714\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 175\n",
      "  episodes_total: 175\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6616836786270142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031831808388233185\n",
      "          model: {}\n",
      "          policy_loss: -0.046551864594221115\n",
      "          total_loss: 82.29020690917969\n",
      "          vf_explained_var: 0.23701079189777374\n",
      "          vf_loss: 82.33039093017578\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.54615384615384\n",
      "    ram_util_percent: 70.67692307692309\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0499397240648029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05475910447055593\n",
      "    mean_inference_ms: 0.7465524513761391\n",
      "    mean_raw_obs_processing_ms: 0.0797488396836948\n",
      "  time_since_restore: 8.58452558517456\n",
      "  time_this_iter_s: 8.58452558517456\n",
      "  time_total_s: 8.58452558517456\n",
      "  timers:\n",
      "    learn_throughput: 857.638\n",
      "    learn_time_ms: 4663.974\n",
      "    load_throughput: 56887.153\n",
      "    load_time_ms: 70.315\n",
      "    sample_throughput: 1059.097\n",
      "    sample_time_ms: 3776.803\n",
      "    update_time_ms: 1.746\n",
      "  timestamp: 1628178794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 43.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 189.0\n",
      "  episode_reward_mean: 43.57\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 255\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5845183730125427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02348373271524906\n",
      "          model: {}\n",
      "          policy_loss: -0.029369423165917397\n",
      "          total_loss: 436.6054382324219\n",
      "          vf_explained_var: 0.16372504830360413\n",
      "          vf_loss: 436.6277160644531\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.88181818181819\n",
      "    ram_util_percent: 70.77272727272727\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049239844892490066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05447807687918234\n",
      "    mean_inference_ms: 0.7278638298647195\n",
      "    mean_raw_obs_processing_ms: 0.07621392386061836\n",
      "  time_since_restore: 16.208592414855957\n",
      "  time_this_iter_s: 7.655693531036377\n",
      "  time_total_s: 16.208592414855957\n",
      "  timers:\n",
      "    learn_throughput: 918.284\n",
      "    learn_time_ms: 4355.95\n",
      "    load_throughput: 114753.277\n",
      "    load_time_ms: 34.857\n",
      "    sample_throughput: 1091.363\n",
      "    sample_time_ms: 3665.142\n",
      "    update_time_ms: 2.514\n",
      "  timestamp: 1628178802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.48901</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.9885</td><td style=\"text-align: right;\">                  81</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.9885</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        16.2086 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 43.57  </td><td style=\"text-align: right;\">                 189</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">           43.57  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.58453</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.7543</td><td style=\"text-align: right;\">                  95</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.7543</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 43.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 150.0\n",
      "  episode_reward_mean: 43.87\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 259\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5980153679847717\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024630779400467873\n",
      "          model: {}\n",
      "          policy_loss: -0.030269023030996323\n",
      "          total_loss: 274.22918701171875\n",
      "          vf_explained_var: 0.22839325666427612\n",
      "          vf_loss: 274.25201416015625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.11000000000001\n",
      "    ram_util_percent: 70.77000000000001\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05117818536670489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05561682315625365\n",
      "    mean_inference_ms: 0.7403990080564352\n",
      "    mean_raw_obs_processing_ms: 0.07594936384073367\n",
      "  time_since_restore: 16.076986074447632\n",
      "  time_this_iter_s: 7.587977409362793\n",
      "  time_total_s: 16.076986074447632\n",
      "  timers:\n",
      "    learn_throughput: 949.06\n",
      "    learn_time_ms: 4214.696\n",
      "    load_throughput: 85786.465\n",
      "    load_time_ms: 46.627\n",
      "    sample_throughput: 1072.48\n",
      "    sample_time_ms: 3729.672\n",
      "    update_time_ms: 1.834\n",
      "  timestamp: 1628178802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 41.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 41.48\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 89\n",
      "  episodes_total: 264\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6090700626373291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018502013757824898\n",
      "          model: {}\n",
      "          policy_loss: -0.029060689732432365\n",
      "          total_loss: 248.19906616210938\n",
      "          vf_explained_var: 0.20518217980861664\n",
      "          vf_loss: 248.22256469726562\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.97272727272728\n",
      "    ram_util_percent: 70.77272727272727\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049589131988378166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05465415327161298\n",
      "    mean_inference_ms: 0.7437692912225162\n",
      "    mean_raw_obs_processing_ms: 0.07582386099338648\n",
      "  time_since_restore: 16.13528037071228\n",
      "  time_this_iter_s: 7.55075478553772\n",
      "  time_total_s: 16.13528037071228\n",
      "  timers:\n",
      "    learn_throughput: 941.421\n",
      "    learn_time_ms: 4248.896\n",
      "    load_throughput: 111682.816\n",
      "    load_time_ms: 35.816\n",
      "    sample_throughput: 1068.847\n",
      "    sample_time_ms: 3742.35\n",
      "    update_time_ms: 1.612\n",
      "  timestamp: 1628178802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 75.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 75.67\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 287\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5628094673156738\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008642684668302536\n",
      "          model: {}\n",
      "          policy_loss: -0.014214309863746166\n",
      "          total_loss: 563.735107421875\n",
      "          vf_explained_var: 0.20104119181632996\n",
      "          vf_loss: 563.7454223632812\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.29166666666667\n",
      "    ram_util_percent: 70.80833333333332\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04882015863602475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05422263994192529\n",
      "    mean_inference_ms: 0.7183959495488142\n",
      "    mean_raw_obs_processing_ms: 0.0735770016984767\n",
      "  time_since_restore: 24.5575110912323\n",
      "  time_this_iter_s: 8.348918676376343\n",
      "  time_total_s: 24.5575110912323\n",
      "  timers:\n",
      "    learn_throughput: 875.279\n",
      "    learn_time_ms: 4569.972\n",
      "    load_throughput: 169414.92\n",
      "    load_time_ms: 23.611\n",
      "    sample_throughput: 1124.505\n",
      "    sample_time_ms: 3557.12\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1628178810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.077 </td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">   43.87</td><td style=\"text-align: right;\">                 150</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             43.87</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.5575</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   75.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             75.67</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.1353</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">   41.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             41.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 66.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 66.29\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 299\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5730690360069275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014659317210316658\n",
      "          model: {}\n",
      "          policy_loss: -0.015974082052707672\n",
      "          total_loss: 432.9205322265625\n",
      "          vf_explained_var: 0.30792659521102905\n",
      "          vf_loss: 432.9298400878906\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.30833333333334\n",
      "    ram_util_percent: 70.80833333333332\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.050810927378050035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055192289942148545\n",
      "    mean_inference_ms: 0.7289743875783324\n",
      "    mean_raw_obs_processing_ms: 0.07351900621802065\n",
      "  time_since_restore: 24.448415517807007\n",
      "  time_this_iter_s: 8.371429443359375\n",
      "  time_total_s: 24.448415517807007\n",
      "  timers:\n",
      "    learn_throughput: 895.975\n",
      "    learn_time_ms: 4464.411\n",
      "    load_throughput: 127036.282\n",
      "    load_time_ms: 31.487\n",
      "    sample_throughput: 1106.478\n",
      "    sample_time_ms: 3615.075\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1628178810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 69.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 69.46\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 301\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5713247656822205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009940036572515965\n",
      "          model: {}\n",
      "          policy_loss: -0.016950396820902824\n",
      "          total_loss: 542.0545654296875\n",
      "          vf_explained_var: 0.18472039699554443\n",
      "          vf_loss: 542.0685424804688\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.30000000000001\n",
      "    ram_util_percent: 70.8090909090909\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04929014120433204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054257964856562514\n",
      "    mean_inference_ms: 0.7359321427161456\n",
      "    mean_raw_obs_processing_ms: 0.0736827101175407\n",
      "  time_since_restore: 24.429233074188232\n",
      "  time_this_iter_s: 8.293952703475952\n",
      "  time_total_s: 24.429233074188232\n",
      "  timers:\n",
      "    learn_throughput: 897.972\n",
      "    learn_time_ms: 4454.481\n",
      "    load_throughput: 164580.936\n",
      "    load_time_ms: 24.304\n",
      "    sample_throughput: 1100.621\n",
      "    sample_time_ms: 3634.31\n",
      "    update_time_ms: 2.031\n",
      "  timestamp: 1628178810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 104.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 104.94\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 309\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5598281621932983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008883132599294186\n",
      "          model: {}\n",
      "          policy_loss: -0.0118827810510993\n",
      "          total_loss: 577.163330078125\n",
      "          vf_explained_var: 0.32274436950683594\n",
      "          vf_loss: 577.1712646484375\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.51818181818182\n",
      "    ram_util_percent: 70.71818181818182\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04858274595395876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05384960536905248\n",
      "    mean_inference_ms: 0.7132404004253249\n",
      "    mean_raw_obs_processing_ms: 0.072152113376368\n",
      "  time_since_restore: 32.4894585609436\n",
      "  time_this_iter_s: 7.931947469711304\n",
      "  time_total_s: 32.4894585609436\n",
      "  timers:\n",
      "    learn_throughput: 880.001\n",
      "    learn_time_ms: 4545.449\n",
      "    load_throughput: 221990.586\n",
      "    load_time_ms: 18.019\n",
      "    sample_throughput: 1132.971\n",
      "    sample_time_ms: 3530.541\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1628178818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.4484</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             66.29</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.4895</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  104.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            104.94</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.4292</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   69.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             69.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 96.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 96.08\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 322\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5640788674354553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01242230273783207\n",
      "          model: {}\n",
      "          policy_loss: -0.009687225334346294\n",
      "          total_loss: 319.33599853515625\n",
      "          vf_explained_var: 0.6074909567832947\n",
      "          vf_loss: 319.34002685546875\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.30833333333334\n",
      "    ram_util_percent: 70.72500000000001\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05057792973111912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054780367290916315\n",
      "    mean_inference_ms: 0.7222838011625293\n",
      "    mean_raw_obs_processing_ms: 0.07205500460082855\n",
      "  time_since_restore: 32.36358904838562\n",
      "  time_this_iter_s: 7.915173530578613\n",
      "  time_total_s: 32.36358904838562\n",
      "  timers:\n",
      "    learn_throughput: 896.223\n",
      "    learn_time_ms: 4463.175\n",
      "    load_throughput: 167156.606\n",
      "    load_time_ms: 23.93\n",
      "    sample_throughput: 1119.447\n",
      "    sample_time_ms: 3573.194\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1628178818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 99.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 99.48\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 324\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5817723274230957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008661270141601562\n",
      "          model: {}\n",
      "          policy_loss: -0.005638697650283575\n",
      "          total_loss: 390.3354797363281\n",
      "          vf_explained_var: 0.42157262563705444\n",
      "          vf_loss: 390.3385925292969\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.25833333333333\n",
      "    ram_util_percent: 70.72500000000001\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049062649356223495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0539372104646942\n",
      "    mean_inference_ms: 0.7294753836881334\n",
      "    mean_raw_obs_processing_ms: 0.07217216799611237\n",
      "  time_since_restore: 32.30596685409546\n",
      "  time_this_iter_s: 7.876733779907227\n",
      "  time_total_s: 32.30596685409546\n",
      "  timers:\n",
      "    learn_throughput: 900.496\n",
      "    learn_time_ms: 4441.997\n",
      "    load_throughput: 215307.13\n",
      "    load_time_ms: 18.578\n",
      "    sample_throughput: 1113.749\n",
      "    sample_time_ms: 3591.474\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1628178818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 126.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 126.03\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 346\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5399239659309387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016325591132044792\n",
      "          model: {}\n",
      "          policy_loss: -0.012684697285294533\n",
      "          total_loss: 234.8885498046875\n",
      "          vf_explained_var: 0.5765531659126282\n",
      "          vf_loss: 234.8939208984375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4090909090909\n",
      "    ram_util_percent: 70.65454545454547\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.050069330686238034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054050373688944726\n",
      "    mean_inference_ms: 0.7105881004594659\n",
      "    mean_raw_obs_processing_ms: 0.06990443392381863\n",
      "  time_since_restore: 39.871448040008545\n",
      "  time_this_iter_s: 7.507858991622925\n",
      "  time_total_s: 39.871448040008545\n",
      "  timers:\n",
      "    learn_throughput: 897.865\n",
      "    learn_time_ms: 4455.012\n",
      "    load_throughput: 205830.156\n",
      "    load_time_ms: 19.433\n",
      "    sample_throughput: 1151.571\n",
      "    sample_time_ms: 3473.515\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1628178825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         39.8714</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  126.03</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            126.03</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.4895</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  104.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            104.94</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.306 </td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   99.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             99.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 133.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 133.5\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 331\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5556095242500305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0063644167967140675\n",
      "          model: {}\n",
      "          policy_loss: -0.010108624584972858\n",
      "          total_loss: 320.59771728515625\n",
      "          vf_explained_var: 0.41839176416397095\n",
      "          vf_loss: 320.6050109863281\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.44545454545454\n",
      "    ram_util_percent: 70.65454545454547\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0481336278997744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05321107370370333\n",
      "    mean_inference_ms: 0.7041602645014577\n",
      "    mean_raw_obs_processing_ms: 0.0701772434533398\n",
      "  time_since_restore: 40.023640155792236\n",
      "  time_this_iter_s: 7.534181594848633\n",
      "  time_total_s: 40.023640155792236\n",
      "  timers:\n",
      "    learn_throughput: 883.691\n",
      "    learn_time_ms: 4526.468\n",
      "    load_throughput: 274074.819\n",
      "    load_time_ms: 14.595\n",
      "    sample_throughput: 1162.986\n",
      "    sample_time_ms: 3439.423\n",
      "    update_time_ms: 2.295\n",
      "  timestamp: 1628178825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 127.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 127.9\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 347\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5404673218727112\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006407022941857576\n",
      "          model: {}\n",
      "          policy_loss: -0.003959064371883869\n",
      "          total_loss: 298.5135498046875\n",
      "          vf_explained_var: 0.42259663343429565\n",
      "          vf_loss: 298.5155944824219\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.37272727272727\n",
      "    ram_util_percent: 70.65454545454547\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048619586786610276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05338063441709553\n",
      "    mean_inference_ms: 0.7187839149202213\n",
      "    mean_raw_obs_processing_ms: 0.07010977603668056\n",
      "  time_since_restore: 39.88814949989319\n",
      "  time_this_iter_s: 7.5821826457977295\n",
      "  time_total_s: 39.88814949989319\n",
      "  timers:\n",
      "    learn_throughput: 900.685\n",
      "    learn_time_ms: 4441.066\n",
      "    load_throughput: 264010.222\n",
      "    load_time_ms: 15.151\n",
      "    sample_throughput: 1143.158\n",
      "    sample_time_ms: 3499.078\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1628178826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 150.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 150.3\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 369\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5647294521331787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014403828419744968\n",
      "          model: {}\n",
      "          policy_loss: -0.014890707097947598\n",
      "          total_loss: 188.3265838623047\n",
      "          vf_explained_var: 0.7656508088111877\n",
      "          vf_loss: 188.33497619628906\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.27272727272727\n",
      "    ram_util_percent: 70.60909090909091\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04971595657997845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05353104868868359\n",
      "    mean_inference_ms: 0.7025417334347291\n",
      "    mean_raw_obs_processing_ms: 0.06818683548395177\n",
      "  time_since_restore: 47.930933713912964\n",
      "  time_this_iter_s: 8.059485673904419\n",
      "  time_total_s: 47.930933713912964\n",
      "  timers:\n",
      "    learn_throughput: 897.454\n",
      "    learn_time_ms: 4457.055\n",
      "    load_throughput: 242519.499\n",
      "    load_time_ms: 16.494\n",
      "    sample_throughput: 1145.615\n",
      "    sample_time_ms: 3491.575\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1628178834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         47.9309</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   150.3</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             150.3</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         40.0236</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   133.5</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">             133.5</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         39.8881</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   127.9</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             127.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 162.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 162.58\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 352\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5479677319526672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009513678029179573\n",
      "          model: {}\n",
      "          policy_loss: -0.012486390769481659\n",
      "          total_loss: 337.74957275390625\n",
      "          vf_explained_var: 0.5252503156661987\n",
      "          vf_loss: 337.7577819824219\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.26363636363637\n",
      "    ram_util_percent: 70.61818181818182\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04779591536669204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05268271585763168\n",
      "    mean_inference_ms: 0.6967533483692009\n",
      "    mean_raw_obs_processing_ms: 0.06822702342493082\n",
      "  time_since_restore: 48.09009122848511\n",
      "  time_this_iter_s: 8.066451072692871\n",
      "  time_total_s: 48.09009122848511\n",
      "  timers:\n",
      "    learn_throughput: 885.826\n",
      "    learn_time_ms: 4515.56\n",
      "    load_throughput: 322680.138\n",
      "    load_time_ms: 12.396\n",
      "    sample_throughput: 1154.308\n",
      "    sample_time_ms: 3465.281\n",
      "    update_time_ms: 2.345\n",
      "  timestamp: 1628178834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 155.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 155.07\n",
      "  episode_reward_min: 21.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 367\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5494316816329956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033321105875074863\n",
      "          model: {}\n",
      "          policy_loss: -0.004235463682562113\n",
      "          total_loss: 234.67462158203125\n",
      "          vf_explained_var: 0.5445424318313599\n",
      "          vf_loss: 234.67784118652344\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.28181818181818\n",
      "    ram_util_percent: 70.60909090909091\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04832828194628247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053018272914144966\n",
      "    mean_inference_ms: 0.7110156609410777\n",
      "    mean_raw_obs_processing_ms: 0.06838370920996165\n",
      "  time_since_restore: 48.01586389541626\n",
      "  time_this_iter_s: 8.127714395523071\n",
      "  time_total_s: 48.01586389541626\n",
      "  timers:\n",
      "    learn_throughput: 898.772\n",
      "    learn_time_ms: 4450.518\n",
      "    load_throughput: 311902.138\n",
      "    load_time_ms: 12.825\n",
      "    sample_throughput: 1136.582\n",
      "    sample_time_ms: 3519.324\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1628178834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 174.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 174.94\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 373\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5307971239089966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008139245212078094\n",
      "          model: {}\n",
      "          policy_loss: -0.011161530390381813\n",
      "          total_loss: 121.86518096923828\n",
      "          vf_explained_var: 0.8151803016662598\n",
      "          vf_loss: 121.87267303466797\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.08181818181818\n",
      "    ram_util_percent: 70.61818181818181\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04767479651885411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052440164723781366\n",
      "    mean_inference_ms: 0.6934351571653738\n",
      "    mean_raw_obs_processing_ms: 0.0670117359696711\n",
      "  time_since_restore: 55.25416016578674\n",
      "  time_this_iter_s: 7.164068937301636\n",
      "  time_total_s: 55.25416016578674\n",
      "  timers:\n",
      "    learn_throughput: 911.855\n",
      "    learn_time_ms: 4386.662\n",
      "    load_throughput: 370334.612\n",
      "    load_time_ms: 10.801\n",
      "    sample_throughput: 1150.677\n",
      "    sample_time_ms: 3476.214\n",
      "    update_time_ms: 2.215\n",
      "  timestamp: 1628178841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         47.9309</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  150.3 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            150.3 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         55.2542</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  174.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            174.94</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         48.0159</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  155.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            155.07</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 167.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 167.25\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 391\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5808486342430115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016799138858914375\n",
      "          model: {}\n",
      "          policy_loss: -0.013266538269817829\n",
      "          total_loss: 198.99172973632812\n",
      "          vf_explained_var: 0.6992032527923584\n",
      "          vf_loss: 198.9974365234375\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.50909090909092\n",
      "    ram_util_percent: 70.63636363636363\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049514049930807234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05323569278526012\n",
      "    mean_inference_ms: 0.6984165513996764\n",
      "    mean_raw_obs_processing_ms: 0.0669976627967463\n",
      "  time_since_restore: 55.215476751327515\n",
      "  time_this_iter_s: 7.284543037414551\n",
      "  time_total_s: 55.215476751327515\n",
      "  timers:\n",
      "    learn_throughput: 920.567\n",
      "    learn_time_ms: 4345.147\n",
      "    load_throughput: 279549.048\n",
      "    load_time_ms: 14.309\n",
      "    sample_throughput: 1140.412\n",
      "    sample_time_ms: 3507.503\n",
      "    update_time_ms: 2.302\n",
      "  timestamp: 1628178841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 174.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 174.87\n",
      "  episode_reward_min: 21.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 387\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5480425357818604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00628312723711133\n",
      "          model: {}\n",
      "          policy_loss: -0.007173626683652401\n",
      "          total_loss: 208.71885681152344\n",
      "          vf_explained_var: 0.6097842454910278\n",
      "          vf_loss: 208.72506713867188\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0909090909091\n",
      "    ram_util_percent: 70.63636363636363\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0482090511703251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05288675817448366\n",
      "    mean_inference_ms: 0.7074253551363104\n",
      "    mean_raw_obs_processing_ms: 0.06737615926150856\n",
      "  time_since_restore: 55.22873306274414\n",
      "  time_this_iter_s: 7.212869167327881\n",
      "  time_total_s: 55.22873306274414\n",
      "  timers:\n",
      "    learn_throughput: 924.787\n",
      "    learn_time_ms: 4325.32\n",
      "    load_throughput: 357815.798\n",
      "    load_time_ms: 11.179\n",
      "    sample_throughput: 1131.431\n",
      "    sample_time_ms: 3535.347\n",
      "    update_time_ms: 1.963\n",
      "  timestamp: 1628178841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 188.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.81\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 394\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5419415831565857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007888205349445343\n",
      "          model: {}\n",
      "          policy_loss: -0.011746658012270927\n",
      "          total_loss: 247.4580841064453\n",
      "          vf_explained_var: 0.59712815284729\n",
      "          vf_loss: 247.46629333496094\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.89090909090909\n",
      "    ram_util_percent: 70.84545454545453\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04759954492720387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05225944345930641\n",
      "    mean_inference_ms: 0.6911269473959026\n",
      "    mean_raw_obs_processing_ms: 0.06600424550917389\n",
      "  time_since_restore: 62.98194432258606\n",
      "  time_this_iter_s: 7.727784156799316\n",
      "  time_total_s: 62.98194432258606\n",
      "  timers:\n",
      "    learn_throughput: 916.116\n",
      "    learn_time_ms: 4366.261\n",
      "    load_throughput: 416864.081\n",
      "    load_time_ms: 9.595\n",
      "    sample_throughput: 1149.929\n",
      "    sample_time_ms: 3478.474\n",
      "    update_time_ms: 2.304\n",
      "  timestamp: 1628178849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         55.2155</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  167.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            167.25</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         62.9819</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  188.81</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            188.81</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         55.2287</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  174.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            174.87</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 178.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 178.04\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 412\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5540826916694641\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011423363350331783\n",
      "          model: {}\n",
      "          policy_loss: -0.014692360535264015\n",
      "          total_loss: 174.00784301757812\n",
      "          vf_explained_var: 0.7675331830978394\n",
      "          vf_loss: 174.0174102783203\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.39999999999999\n",
      "    ram_util_percent: 70.86363636363636\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04936853735943469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05301317251733529\n",
      "    mean_inference_ms: 0.6952957763417931\n",
      "    mean_raw_obs_processing_ms: 0.06605633092836659\n",
      "  time_since_restore: 63.10865378379822\n",
      "  time_this_iter_s: 7.893177032470703\n",
      "  time_total_s: 63.10865378379822\n",
      "  timers:\n",
      "    learn_throughput: 918.932\n",
      "    learn_time_ms: 4352.877\n",
      "    load_throughput: 315594.785\n",
      "    load_time_ms: 12.674\n",
      "    sample_throughput: 1141.835\n",
      "    sample_time_ms: 3503.132\n",
      "    update_time_ms: 2.543\n",
      "  timestamp: 1628178849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 189.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.42\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 407\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5164365172386169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005725353490561247\n",
      "          model: {}\n",
      "          policy_loss: -0.002640391932800412\n",
      "          total_loss: 163.99032592773438\n",
      "          vf_explained_var: 0.7067844867706299\n",
      "          vf_loss: 163.99212646484375\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.84545454545456\n",
      "    ram_util_percent: 70.85454545454544\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04813664764138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05280361861657212\n",
      "    mean_inference_ms: 0.7050390558299031\n",
      "    mean_raw_obs_processing_ms: 0.06645723906680912\n",
      "  time_since_restore: 63.309853076934814\n",
      "  time_this_iter_s: 8.081120014190674\n",
      "  time_total_s: 63.309853076934814\n",
      "  timers:\n",
      "    learn_throughput: 922.377\n",
      "    learn_time_ms: 4336.623\n",
      "    load_throughput: 401317.199\n",
      "    load_time_ms: 9.967\n",
      "    sample_throughput: 1126.598\n",
      "    sample_time_ms: 3550.512\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1628178849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 192.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.71\n",
      "  episode_reward_min: 66.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 414\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5387442111968994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012030906975269318\n",
      "          model: {}\n",
      "          policy_loss: -0.017730865627527237\n",
      "          total_loss: 289.94879150390625\n",
      "          vf_explained_var: 0.511776864528656\n",
      "          vf_loss: 289.9610595703125\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.98181818181818\n",
      "    ram_util_percent: 70.79999999999998\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04763363207836512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052225903704228596\n",
      "    mean_inference_ms: 0.6909918891308773\n",
      "    mean_raw_obs_processing_ms: 0.06543470180229934\n",
      "  time_since_restore: 70.71595549583435\n",
      "  time_this_iter_s: 7.734011173248291\n",
      "  time_total_s: 70.71595549583435\n",
      "  timers:\n",
      "    learn_throughput: 921.962\n",
      "    learn_time_ms: 4338.576\n",
      "    load_throughput: 461882.537\n",
      "    load_time_ms: 8.66\n",
      "    sample_throughput: 1145.209\n",
      "    sample_time_ms: 3492.813\n",
      "    update_time_ms: 2.313\n",
      "  timestamp: 1628178856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         63.1087</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  178.04</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            178.04</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         70.716 </td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  192.71</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  66</td><td style=\"text-align: right;\">            192.71</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         63.3099</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  189.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            189.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 181.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 181.73\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 432\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5381043553352356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012393375858664513\n",
      "          model: {}\n",
      "          policy_loss: -0.00527392840012908\n",
      "          total_loss: 189.11453247070312\n",
      "          vf_explained_var: 0.7089013457298279\n",
      "          vf_loss: 189.1141815185547\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.95454545454547\n",
      "    ram_util_percent: 70.79999999999998\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04939290012844849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0529781632121214\n",
      "    mean_inference_ms: 0.6953003938105344\n",
      "    mean_raw_obs_processing_ms: 0.06556231441468728\n",
      "  time_since_restore: 70.86585855484009\n",
      "  time_this_iter_s: 7.75720477104187\n",
      "  time_total_s: 70.86585855484009\n",
      "  timers:\n",
      "    learn_throughput: 923.635\n",
      "    learn_time_ms: 4330.715\n",
      "    load_throughput: 350827.129\n",
      "    load_time_ms: 11.402\n",
      "    sample_throughput: 1138.722\n",
      "    sample_time_ms: 3512.71\n",
      "    update_time_ms: 2.509\n",
      "  timestamp: 1628178857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 194.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.18\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 427\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5264274477958679\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004741955082863569\n",
      "          model: {}\n",
      "          policy_loss: -0.0031317847315222025\n",
      "          total_loss: 111.93209838867188\n",
      "          vf_explained_var: 0.8179740309715271\n",
      "          vf_loss: 111.93452453613281\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.94545454545454\n",
      "    ram_util_percent: 70.79090909090908\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0481659355377622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052845941824353954\n",
      "    mean_inference_ms: 0.7053079295201092\n",
      "    mean_raw_obs_processing_ms: 0.06589156757545124\n",
      "  time_since_restore: 71.28227686882019\n",
      "  time_this_iter_s: 7.972423791885376\n",
      "  time_total_s: 71.28227686882019\n",
      "  timers:\n",
      "    learn_throughput: 925.215\n",
      "    learn_time_ms: 4323.319\n",
      "    load_throughput: 444002.223\n",
      "    load_time_ms: 9.009\n",
      "    sample_throughput: 1119.76\n",
      "    sample_time_ms: 3572.195\n",
      "    update_time_ms: 1.964\n",
      "  timestamp: 1628178857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 195.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.46\n",
      "  episode_reward_min: 94.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 434\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.515791654586792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009213306941092014\n",
      "          model: {}\n",
      "          policy_loss: -0.015869878232479095\n",
      "          total_loss: 281.0982666015625\n",
      "          vf_explained_var: 0.5166258215904236\n",
      "          vf_loss: 281.1099853515625\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.72999999999999\n",
      "    ram_util_percent: 70.84\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04773847293736754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05229504234437923\n",
      "    mean_inference_ms: 0.6925054127679862\n",
      "    mean_raw_obs_processing_ms: 0.06511573368826214\n",
      "  time_since_restore: 78.10503458976746\n",
      "  time_this_iter_s: 7.3890790939331055\n",
      "  time_total_s: 78.10503458976746\n",
      "  timers:\n",
      "    learn_throughput: 926.835\n",
      "    learn_time_ms: 4315.764\n",
      "    load_throughput: 505904.688\n",
      "    load_time_ms: 7.907\n",
      "    sample_throughput: 1152.61\n",
      "    sample_time_ms: 3470.385\n",
      "    update_time_ms: 2.332\n",
      "  timestamp: 1628178864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         70.8659</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  181.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            181.73</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         78.105 </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  195.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  94</td><td style=\"text-align: right;\">            195.46</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         71.2823</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  194.18</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            194.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 188.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.4\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 453\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5598463416099548\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013772767968475819\n",
      "          model: {}\n",
      "          policy_loss: -0.014951050281524658\n",
      "          total_loss: 260.8135070800781\n",
      "          vf_explained_var: 0.6579248905181885\n",
      "          vf_loss: 260.822265625\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.19090909090909\n",
      "    ram_util_percent: 70.84545454545454\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04942929055313201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05296633775664704\n",
      "    mean_inference_ms: 0.6956348580162545\n",
      "    mean_raw_obs_processing_ms: 0.06516158696993982\n",
      "  time_since_restore: 78.33157277107239\n",
      "  time_this_iter_s: 7.4657142162323\n",
      "  time_total_s: 78.33157277107239\n",
      "  timers:\n",
      "    learn_throughput: 924.496\n",
      "    learn_time_ms: 4326.68\n",
      "    load_throughput: 384355.113\n",
      "    load_time_ms: 10.407\n",
      "    sample_throughput: 1150.046\n",
      "    sample_time_ms: 3478.123\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1628178864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-25\n",
      "  done: false\n",
      "  episode_len_mean: 199.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.39\n",
      "  episode_reward_min: 150.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 447\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5098451375961304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006104445084929466\n",
      "          model: {}\n",
      "          policy_loss: -0.0039965808391571045\n",
      "          total_loss: 305.1475830078125\n",
      "          vf_explained_var: 0.5913848876953125\n",
      "          vf_loss: 305.15118408203125\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.17272727272727\n",
      "    ram_util_percent: 70.86363636363636\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04831310494184853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053010900745439626\n",
      "    mean_inference_ms: 0.7080226257482374\n",
      "    mean_raw_obs_processing_ms: 0.06561782027974702\n",
      "  time_since_restore: 78.7593879699707\n",
      "  time_this_iter_s: 7.477111101150513\n",
      "  time_total_s: 78.7593879699707\n",
      "  timers:\n",
      "    learn_throughput: 930.542\n",
      "    learn_time_ms: 4298.572\n",
      "    load_throughput: 486708.054\n",
      "    load_time_ms: 8.218\n",
      "    sample_throughput: 1125.399\n",
      "    sample_time_ms: 3554.296\n",
      "    update_time_ms: 1.929\n",
      "  timestamp: 1628178865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 197.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.33\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 454\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.523929238319397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008636831305921078\n",
      "          model: {}\n",
      "          policy_loss: -0.01547552552074194\n",
      "          total_loss: 223.6693878173828\n",
      "          vf_explained_var: 0.6146178841590881\n",
      "          vf_loss: 223.6809539794922\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.70909090909092\n",
      "    ram_util_percent: 70.8090909090909\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047768833147761036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052293641830994934\n",
      "    mean_inference_ms: 0.6932714818284044\n",
      "    mean_raw_obs_processing_ms: 0.06481004266218306\n",
      "  time_since_restore: 85.73507285118103\n",
      "  time_this_iter_s: 7.630038261413574\n",
      "  time_total_s: 85.73507285118103\n",
      "  timers:\n",
      "    learn_throughput: 939.054\n",
      "    learn_time_ms: 4259.605\n",
      "    load_throughput: 3506356.797\n",
      "    load_time_ms: 1.141\n",
      "    sample_throughput: 1159.81\n",
      "    sample_time_ms: 3448.84\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1628178871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         78.3316</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  188.4 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            188.4 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         85.7351</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  197.33</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            197.33</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         78.7594</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  199.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 150</td><td style=\"text-align: right;\">            199.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 192.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.39\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 473\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5463250279426575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012010903097689152\n",
      "          model: {}\n",
      "          policy_loss: -0.009280704893171787\n",
      "          total_loss: 332.7838134765625\n",
      "          vf_explained_var: 0.4913463294506073\n",
      "          vf_loss: 332.78765869140625\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.7909090909091\n",
      "    ram_util_percent: 70.7818181818182\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04940269386407954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05290345135033338\n",
      "    mean_inference_ms: 0.6953197440932726\n",
      "    mean_raw_obs_processing_ms: 0.06478380027582639\n",
      "  time_since_restore: 86.00380039215088\n",
      "  time_this_iter_s: 7.672227621078491\n",
      "  time_total_s: 86.00380039215088\n",
      "  timers:\n",
      "    learn_throughput: 931.023\n",
      "    learn_time_ms: 4296.349\n",
      "    load_throughput: 3020200.9\n",
      "    load_time_ms: 1.324\n",
      "    sample_throughput: 1161.504\n",
      "    sample_time_ms: 3443.812\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1628178872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 199.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.93\n",
      "  episode_reward_min: 195.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 467\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5263704061508179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004362180829048157\n",
      "          model: {}\n",
      "          policy_loss: -0.0033905720338225365\n",
      "          total_loss: 370.2275390625\n",
      "          vf_explained_var: 0.49090108275413513\n",
      "          vf_loss: 370.23065185546875\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.7818181818182\n",
      "    ram_util_percent: 70.7818181818182\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04838205571333856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053074894387610386\n",
      "    mean_inference_ms: 0.7099396190046451\n",
      "    mean_raw_obs_processing_ms: 0.0653623777181149\n",
      "  time_since_restore: 86.34122705459595\n",
      "  time_this_iter_s: 7.581839084625244\n",
      "  time_total_s: 86.34122705459595\n",
      "  timers:\n",
      "    learn_throughput: 944.963\n",
      "    learn_time_ms: 4232.968\n",
      "    load_throughput: 3040011.597\n",
      "    load_time_ms: 1.316\n",
      "    sample_throughput: 1132.116\n",
      "    sample_time_ms: 3533.205\n",
      "    update_time_ms: 1.921\n",
      "  timestamp: 1628178872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 199.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.89\n",
      "  episode_reward_min: 189.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 474\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4962107837200165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010185081511735916\n",
      "          model: {}\n",
      "          policy_loss: -0.015049965120851994\n",
      "          total_loss: 232.3898468017578\n",
      "          vf_explained_var: 0.5988683104515076\n",
      "          vf_loss: 232.40028381347656\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.50000000000001\n",
      "    ram_util_percent: 70.78999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047759936816497525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05226413386945559\n",
      "    mean_inference_ms: 0.6937861335293138\n",
      "    mean_raw_obs_processing_ms: 0.06451438648625223\n",
      "  time_since_restore: 92.5093469619751\n",
      "  time_this_iter_s: 6.774274110794067\n",
      "  time_total_s: 92.5093469619751\n",
      "  timers:\n",
      "    learn_throughput: 957.552\n",
      "    learn_time_ms: 4177.32\n",
      "    load_throughput: 3444023.484\n",
      "    load_time_ms: 1.161\n",
      "    sample_throughput: 1161.754\n",
      "    sample_time_ms: 3443.071\n",
      "    update_time_ms: 2.444\n",
      "  timestamp: 1628178878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         86.0038</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  192.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            192.39</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         92.5093</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  199.89</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 189</td><td style=\"text-align: right;\">            199.89</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         86.3412</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  199.93</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 195</td><td style=\"text-align: right;\">            199.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 194.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.15\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 494\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5336911082267761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01548173651099205\n",
      "          model: {}\n",
      "          policy_loss: -0.008098946884274483\n",
      "          total_loss: 362.95697021484375\n",
      "          vf_explained_var: 0.5207858085632324\n",
      "          vf_loss: 362.95806884765625\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.38888888888889\n",
      "    ram_util_percent: 70.8\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04932689108937629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05279615466141529\n",
      "    mean_inference_ms: 0.6944782421673297\n",
      "    mean_raw_obs_processing_ms: 0.06441693272397808\n",
      "  time_since_restore: 92.75082111358643\n",
      "  time_this_iter_s: 6.747020721435547\n",
      "  time_total_s: 92.75082111358643\n",
      "  timers:\n",
      "    learn_throughput: 946.459\n",
      "    learn_time_ms: 4226.281\n",
      "    load_throughput: 3086373.186\n",
      "    load_time_ms: 1.296\n",
      "    sample_throughput: 1166.286\n",
      "    sample_time_ms: 3429.691\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1628178879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 199.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.98\n",
      "  episode_reward_min: 198.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 487\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4995354413986206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011271006427705288\n",
      "          model: {}\n",
      "          policy_loss: -0.004554182756692171\n",
      "          total_loss: 174.9586944580078\n",
      "          vf_explained_var: 0.696628987789154\n",
      "          vf_loss: 174.96282958984375\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.9\n",
      "    ram_util_percent: 70.79999999999998\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04839735378640565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05308109447517578\n",
      "    mean_inference_ms: 0.7112797420004284\n",
      "    mean_raw_obs_processing_ms: 0.06510830408592243\n",
      "  time_since_restore: 93.09836935997009\n",
      "  time_this_iter_s: 6.7571423053741455\n",
      "  time_total_s: 93.09836935997009\n",
      "  timers:\n",
      "    learn_throughput: 960.112\n",
      "    learn_time_ms: 4166.179\n",
      "    load_throughput: 3035501.357\n",
      "    load_time_ms: 1.318\n",
      "    sample_throughput: 1136.14\n",
      "    sample_time_ms: 3520.692\n",
      "    update_time_ms: 1.924\n",
      "  timestamp: 1628178879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 199.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.07\n",
      "  episode_reward_min: 107.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 494\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.49563777446746826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011041764169931412\n",
      "          model: {}\n",
      "          policy_loss: -0.01870168372988701\n",
      "          total_loss: 318.15325927734375\n",
      "          vf_explained_var: 0.43761464953422546\n",
      "          vf_loss: 318.1669921875\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.69090909090909\n",
      "    ram_util_percent: 70.84545454545453\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04778739796800825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052277994320084266\n",
      "    mean_inference_ms: 0.6948801567639497\n",
      "    mean_raw_obs_processing_ms: 0.06430142550966528\n",
      "  time_since_restore: 100.47320914268494\n",
      "  time_this_iter_s: 7.963862180709839\n",
      "  time_total_s: 100.47320914268494\n",
      "  timers:\n",
      "    learn_throughput: 974.782\n",
      "    learn_time_ms: 4103.481\n",
      "    load_throughput: 3411389.996\n",
      "    load_time_ms: 1.173\n",
      "    sample_throughput: 1149.958\n",
      "    sample_time_ms: 3478.388\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1628178886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         92.7508</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  194.15</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            194.15</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">        100.473 </td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  199.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 107</td><td style=\"text-align: right;\">            199.07</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>RUNNING </td><td>10.10.33.168:156931</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         93.0984</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  199.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 198</td><td style=\"text-align: right;\">            199.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 196.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.06\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 514\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5504868030548096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01355358399450779\n",
      "          model: {}\n",
      "          policy_loss: -0.010080601088702679\n",
      "          total_loss: 292.4569091796875\n",
      "          vf_explained_var: 0.5368101000785828\n",
      "          vf_loss: 292.4609069824219\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.55\n",
      "    ram_util_percent: 70.84999999999998\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049284506691793906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052724871752129275\n",
      "    mean_inference_ms: 0.6942656077000448\n",
      "    mean_raw_obs_processing_ms: 0.06414886699257831\n",
      "  time_since_restore: 100.81624460220337\n",
      "  time_this_iter_s: 8.065423488616943\n",
      "  time_total_s: 100.81624460220337\n",
      "  timers:\n",
      "    learn_throughput: 958.891\n",
      "    learn_time_ms: 4171.485\n",
      "    load_throughput: 3084954.398\n",
      "    load_time_ms: 1.297\n",
      "    sample_throughput: 1157.713\n",
      "    sample_time_ms: 3455.088\n",
      "    update_time_ms: 2.36\n",
      "  timestamp: 1628178887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00002:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-47\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 507\n",
      "  experiment_id: 332ec630b4a9425cabb3e8441a6f818f\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4757068157196045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005990668665617704\n",
      "          model: {}\n",
      "          policy_loss: -0.005448805633932352\n",
      "          total_loss: 478.69708251953125\n",
      "          vf_explained_var: 0.3753880560398102\n",
      "          vf_loss: 478.7023010253906\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.05454545454545\n",
      "    ram_util_percent: 70.85454545454544\n",
      "  pid: 156931\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04840946488402988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.053085518339957456\n",
      "    mean_inference_ms: 0.7123328129014511\n",
      "    mean_raw_obs_processing_ms: 0.06489295419134794\n",
      "  time_since_restore: 101.06789207458496\n",
      "  time_this_iter_s: 7.969522714614868\n",
      "  time_total_s: 101.06789207458496\n",
      "  timers:\n",
      "    learn_throughput: 974.055\n",
      "    learn_time_ms: 4106.545\n",
      "    load_throughput: 2987183.249\n",
      "    load_time_ms: 1.339\n",
      "    sample_throughput: 1127.414\n",
      "    sample_time_ms: 3547.942\n",
      "    update_time_ms: 1.841\n",
      "  timestamp: 1628178887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 332a6_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-53\n",
      "  done: false\n",
      "  episode_len_mean: 197.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.38\n",
      "  episode_reward_min: 31.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 515\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.517893373966217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011689062230288982\n",
      "          model: {}\n",
      "          policy_loss: -0.021074306219816208\n",
      "          total_loss: 282.56256103515625\n",
      "          vf_explained_var: 0.4604979157447815\n",
      "          vf_loss: 282.5783996582031\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.52\n",
      "    ram_util_percent: 69.35\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04779923560324109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05225658540025398\n",
      "    mean_inference_ms: 0.6955448124177568\n",
      "    mean_raw_obs_processing_ms: 0.06409308741072826\n",
      "  time_since_restore: 107.4838616847992\n",
      "  time_this_iter_s: 7.010652542114258\n",
      "  time_total_s: 107.4838616847992\n",
      "  timers:\n",
      "    learn_throughput: 999.783\n",
      "    learn_time_ms: 4000.87\n",
      "    load_throughput: 3381071.724\n",
      "    load_time_ms: 1.183\n",
      "    sample_throughput: 1146.487\n",
      "    sample_time_ms: 3488.918\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1628178893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         100.816</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  196.06</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            196.06</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         107.484</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  197.38</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            197.38</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-54-53\n",
      "  done: false\n",
      "  episode_len_mean: 192.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.7\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 536\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5385305881500244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011330914683640003\n",
      "          model: {}\n",
      "          policy_loss: -0.015681065618991852\n",
      "          total_loss: 249.7288818359375\n",
      "          vf_explained_var: 0.6165691018104553\n",
      "          vf_loss: 249.7394256591797\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.95\n",
      "    ram_util_percent: 69.16\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04921646614820771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052633482801578896\n",
      "    mean_inference_ms: 0.6936316880498573\n",
      "    mean_raw_obs_processing_ms: 0.06387811913729269\n",
      "  time_since_restore: 107.62318468093872\n",
      "  time_this_iter_s: 6.806940078735352\n",
      "  time_total_s: 107.62318468093872\n",
      "  timers:\n",
      "    learn_throughput: 984.412\n",
      "    learn_time_ms: 4063.339\n",
      "    load_throughput: 3109944.204\n",
      "    load_time_ms: 1.286\n",
      "    sample_throughput: 1158.627\n",
      "    sample_time_ms: 3452.362\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1628178893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 196.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.99\n",
      "  episode_reward_min: 31.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 535\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5141956210136414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010659306310117245\n",
      "          model: {}\n",
      "          policy_loss: -0.023087993264198303\n",
      "          total_loss: 286.0377197265625\n",
      "          vf_explained_var: 0.4234565794467926\n",
      "          vf_loss: 286.0559997558594\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.03333333333333\n",
      "    ram_util_percent: 69.01111111111112\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047788615196187045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052219013518755804\n",
      "    mean_inference_ms: 0.6955041753508269\n",
      "    mean_raw_obs_processing_ms: 0.06389391310908271\n",
      "  time_since_restore: 113.78446960449219\n",
      "  time_this_iter_s: 6.300607919692993\n",
      "  time_total_s: 113.78446960449219\n",
      "  timers:\n",
      "    learn_throughput: 1028.749\n",
      "    learn_time_ms: 3888.216\n",
      "    load_throughput: 3388446.671\n",
      "    load_time_ms: 1.18\n",
      "    sample_throughput: 1150.006\n",
      "    sample_time_ms: 3478.244\n",
      "    update_time_ms: 2.322\n",
      "  timestamp: 1628178900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_a189e714d76d72c020d0449fcdcc24fc, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         107.623</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  192.7 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            192.7 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         113.784</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  196.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            196.99</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 192.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.34\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 557\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5376700162887573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014346467331051826\n",
      "          model: {}\n",
      "          policy_loss: -0.006966204382479191\n",
      "          total_loss: 147.47021484375\n",
      "          vf_explained_var: 0.7830773591995239\n",
      "          vf_loss: 147.4707489013672\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61111111111111\n",
      "    ram_util_percent: 69.03333333333333\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0491380863368952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052529413287088975\n",
      "    mean_inference_ms: 0.6926333097299612\n",
      "    mean_raw_obs_processing_ms: 0.06363277735708894\n",
      "  time_since_restore: 114.00656867027283\n",
      "  time_this_iter_s: 6.3833839893341064\n",
      "  time_total_s: 114.00656867027283\n",
      "  timers:\n",
      "    learn_throughput: 1009.072\n",
      "    learn_time_ms: 3964.039\n",
      "    load_throughput: 3181178.255\n",
      "    load_time_ms: 1.257\n",
      "    sample_throughput: 1162.978\n",
      "    sample_time_ms: 3439.446\n",
      "    update_time_ms: 2.234\n",
      "  timestamp: 1628178900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-06\n",
      "  done: false\n",
      "  episode_len_mean: 195.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.48\n",
      "  episode_reward_min: 31.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 556\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5111220479011536\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011133006773889065\n",
      "          model: {}\n",
      "          policy_loss: -0.022958075627684593\n",
      "          total_loss: 215.04495239257812\n",
      "          vf_explained_var: 0.6800439357757568\n",
      "          vf_loss: 215.0628662109375\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.63\n",
      "    ram_util_percent: 68.92999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04776165364865102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05216732143684632\n",
      "    mean_inference_ms: 0.6948355660358122\n",
      "    mean_raw_obs_processing_ms: 0.06371075077089827\n",
      "  time_since_restore: 120.51444864273071\n",
      "  time_this_iter_s: 6.729979038238525\n",
      "  time_total_s: 120.51444864273071\n",
      "  timers:\n",
      "    learn_throughput: 1058.824\n",
      "    learn_time_ms: 3777.776\n",
      "    load_throughput: 3451391.895\n",
      "    load_time_ms: 1.159\n",
      "    sample_throughput: 1157.679\n",
      "    sample_time_ms: 3455.19\n",
      "    update_time_ms: 2.235\n",
      "  timestamp: 1628178906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         114.007</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  192.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            192.34</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         120.514</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  195.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            195.48</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 192.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.34\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 577\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5473939776420593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013305667787790298\n",
      "          model: {}\n",
      "          policy_loss: -0.009837416000664234\n",
      "          total_loss: 139.8699188232422\n",
      "          vf_explained_var: 0.7909511923789978\n",
      "          vf_loss: 139.8737335205078\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.14444444444445\n",
      "    ram_util_percent: 68.89999999999999\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04906413317346456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05243077694561391\n",
      "    mean_inference_ms: 0.6913082688025605\n",
      "    mean_raw_obs_processing_ms: 0.06340997367113566\n",
      "  time_since_restore: 120.65826225280762\n",
      "  time_this_iter_s: 6.65169358253479\n",
      "  time_total_s: 120.65826225280762\n",
      "  timers:\n",
      "    learn_throughput: 1039.631\n",
      "    learn_time_ms: 3847.519\n",
      "    load_throughput: 3353364.114\n",
      "    load_time_ms: 1.193\n",
      "    sample_throughput: 1171.216\n",
      "    sample_time_ms: 3415.254\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1628178907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-13\n",
      "  done: false\n",
      "  episode_len_mean: 194.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.05\n",
      "  episode_reward_min: 31.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 576\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5076213479042053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011136497370898724\n",
      "          model: {}\n",
      "          policy_loss: -0.024202166125178337\n",
      "          total_loss: 262.10980224609375\n",
      "          vf_explained_var: 0.5267096161842346\n",
      "          vf_loss: 262.1290283203125\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.82222222222224\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04771517817990569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.052098248462441675\n",
      "    mean_inference_ms: 0.6935806598325617\n",
      "    mean_raw_obs_processing_ms: 0.06354260556593014\n",
      "  time_since_restore: 126.94285750389099\n",
      "  time_this_iter_s: 6.428408861160278\n",
      "  time_total_s: 126.94285750389099\n",
      "  timers:\n",
      "    learn_throughput: 1073.47\n",
      "    learn_time_ms: 3726.233\n",
      "    load_throughput: 3403362.545\n",
      "    load_time_ms: 1.175\n",
      "    sample_throughput: 1165.063\n",
      "    sample_time_ms: 3433.292\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1628178913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         120.658</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  192.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            192.34</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         126.943</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  194.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            194.05</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-13\n",
      "  done: false\n",
      "  episode_len_mean: 194.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.13\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 597\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5282894968986511\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010539709590375423\n",
      "          model: {}\n",
      "          policy_loss: -0.009204311296343803\n",
      "          total_loss: 144.47596740722656\n",
      "          vf_explained_var: 0.7905533313751221\n",
      "          vf_loss: 144.4804229736328\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.14444444444445\n",
      "    ram_util_percent: 68.88888888888889\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04898301165384046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0523257796737284\n",
      "    mean_inference_ms: 0.6895683491271657\n",
      "    mean_raw_obs_processing_ms: 0.06318610854699726\n",
      "  time_since_restore: 127.02027416229248\n",
      "  time_this_iter_s: 6.362011909484863\n",
      "  time_total_s: 127.02027416229248\n",
      "  timers:\n",
      "    learn_throughput: 1057.915\n",
      "    learn_time_ms: 3781.02\n",
      "    load_throughput: 3362774.047\n",
      "    load_time_ms: 1.189\n",
      "    sample_throughput: 1180.095\n",
      "    sample_time_ms: 3389.556\n",
      "    update_time_ms: 2.132\n",
      "  timestamp: 1628178913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-19\n",
      "  done: false\n",
      "  episode_len_mean: 191.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.87\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 598\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5081056356430054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013022533617913723\n",
      "          model: {}\n",
      "          policy_loss: -0.025923890992999077\n",
      "          total_loss: 426.6427917480469\n",
      "          vf_explained_var: 0.4373873472213745\n",
      "          vf_loss: 426.6629333496094\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.83333333333334\n",
      "    ram_util_percent: 68.9111111111111\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04756373901069086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05192207139148849\n",
      "    mean_inference_ms: 0.6905032726193184\n",
      "    mean_raw_obs_processing_ms: 0.06325787259652638\n",
      "  time_since_restore: 133.30567002296448\n",
      "  time_this_iter_s: 6.362812519073486\n",
      "  time_total_s: 133.30567002296448\n",
      "  timers:\n",
      "    learn_throughput: 1101.266\n",
      "    learn_time_ms: 3632.182\n",
      "    load_throughput: 3402741.304\n",
      "    load_time_ms: 1.176\n",
      "    sample_throughput: 1179.617\n",
      "    sample_time_ms: 3390.932\n",
      "    update_time_ms: 2.174\n",
      "  timestamp: 1628178919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         127.02 </td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  194.13</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            194.13</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         133.306</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  191.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            191.87</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-19\n",
      "  done: false\n",
      "  episode_len_mean: 193.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.53\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 617\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5214589834213257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015025685541331768\n",
      "          model: {}\n",
      "          policy_loss: -0.010907801799476147\n",
      "          total_loss: 132.3511505126953\n",
      "          vf_explained_var: 0.8219375610351562\n",
      "          vf_loss: 132.35533142089844\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.54444444444444\n",
      "    ram_util_percent: 68.92222222222223\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048834912161588466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05215466977396899\n",
      "    mean_inference_ms: 0.6866348814689587\n",
      "    mean_raw_obs_processing_ms: 0.06289332912792708\n",
      "  time_since_restore: 133.42582607269287\n",
      "  time_this_iter_s: 6.405551910400391\n",
      "  time_total_s: 133.42582607269287\n",
      "  timers:\n",
      "    learn_throughput: 1089.988\n",
      "    learn_time_ms: 3669.765\n",
      "    load_throughput: 3365269.788\n",
      "    load_time_ms: 1.189\n",
      "    sample_throughput: 1193.124\n",
      "    sample_time_ms: 3352.544\n",
      "    update_time_ms: 1.871\n",
      "  timestamp: 1628178919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 190.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.17\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 620\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5158483386039734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013815480284392834\n",
      "          model: {}\n",
      "          policy_loss: -0.02548326924443245\n",
      "          total_loss: 421.8796691894531\n",
      "          vf_explained_var: 0.3935040533542633\n",
      "          vf_loss: 421.8989562988281\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.17000000000002\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04739728999189987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051744730975683356\n",
      "    mean_inference_ms: 0.6873329918820158\n",
      "    mean_raw_obs_processing_ms: 0.06298506125677175\n",
      "  time_since_restore: 139.9738028049469\n",
      "  time_this_iter_s: 6.668132781982422\n",
      "  time_total_s: 139.9738028049469\n",
      "  timers:\n",
      "    learn_throughput: 1124.65\n",
      "    learn_time_ms: 3556.661\n",
      "    load_throughput: 3432257.114\n",
      "    load_time_ms: 1.165\n",
      "    sample_throughput: 1190.503\n",
      "    sample_time_ms: 3359.926\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1628178926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         133.426</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  193.53</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            193.53</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         139.974</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  190.17</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            190.17</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 196.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.65\n",
      "  episode_reward_min: 113.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 637\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5087315440177917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017374476417899132\n",
      "          model: {}\n",
      "          policy_loss: -0.012729218229651451\n",
      "          total_loss: 271.5484313964844\n",
      "          vf_explained_var: 0.7694469690322876\n",
      "          vf_loss: 271.5533142089844\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.20000000000002\n",
      "    ram_util_percent: 68.79999999999998\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048686597603836465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05198870844106789\n",
      "    mean_inference_ms: 0.6836775152591499\n",
      "    mean_raw_obs_processing_ms: 0.06261192456916505\n",
      "  time_since_restore: 140.05194640159607\n",
      "  time_this_iter_s: 6.626120328903198\n",
      "  time_total_s: 140.05194640159607\n",
      "  timers:\n",
      "    learn_throughput: 1115.386\n",
      "    learn_time_ms: 3586.201\n",
      "    load_throughput: 3338483.703\n",
      "    load_time_ms: 1.198\n",
      "    sample_throughput: 1203.49\n",
      "    sample_time_ms: 3323.667\n",
      "    update_time_ms: 1.798\n",
      "  timestamp: 1628178926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 187.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.2\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 642\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5068562626838684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013459140434861183\n",
      "          model: {}\n",
      "          policy_loss: -0.025304963812232018\n",
      "          total_loss: 326.17547607421875\n",
      "          vf_explained_var: 0.4618555009365082\n",
      "          vf_loss: 326.1947326660156\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.0125\n",
      "    ram_util_percent: 68.875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04727174982941587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05160700913367448\n",
      "    mean_inference_ms: 0.6849886881851946\n",
      "    mean_raw_obs_processing_ms: 0.06278612806705267\n",
      "  time_since_restore: 145.90490794181824\n",
      "  time_this_iter_s: 5.931105136871338\n",
      "  time_total_s: 145.90490794181824\n",
      "  timers:\n",
      "    learn_throughput: 1170.051\n",
      "    learn_time_ms: 3418.655\n",
      "    load_throughput: 3488639.454\n",
      "    load_time_ms: 1.147\n",
      "    sample_throughput: 1193.194\n",
      "    sample_time_ms: 3352.348\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1628178932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         140.052</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  196.65</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 113</td><td style=\"text-align: right;\">            196.65</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         145.905</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  187.2 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            187.2 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 197.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.51\n",
      "  episode_reward_min: 144.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 658\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5200053453445435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013410224579274654\n",
      "          model: {}\n",
      "          policy_loss: -0.0060739233158528805\n",
      "          total_loss: 157.48199462890625\n",
      "          vf_explained_var: 0.8080584406852722\n",
      "          vf_loss: 157.4820098876953\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.99999999999999\n",
      "    ram_util_percent: 68.8875\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04858169309004649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05187172189462816\n",
      "    mean_inference_ms: 0.6814739585601157\n",
      "    mean_raw_obs_processing_ms: 0.0624023185692092\n",
      "  time_since_restore: 145.9772527217865\n",
      "  time_this_iter_s: 5.92530632019043\n",
      "  time_total_s: 145.9772527217865\n",
      "  timers:\n",
      "    learn_throughput: 1164.547\n",
      "    learn_time_ms: 3434.813\n",
      "    load_throughput: 3461432.256\n",
      "    load_time_ms: 1.156\n",
      "    sample_throughput: 1204.39\n",
      "    sample_time_ms: 3321.184\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1628178932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 183.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 183.2\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 665\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.48923468589782715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01299908198416233\n",
      "          model: {}\n",
      "          policy_loss: -0.02696330100297928\n",
      "          total_loss: 337.1488342285156\n",
      "          vf_explained_var: 0.43335941433906555\n",
      "          vf_loss: 337.1700134277344\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.3777777777778\n",
      "    ram_util_percent: 68.85555555555555\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04712176850981008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0514441579066365\n",
      "    mean_inference_ms: 0.6822554886847142\n",
      "    mean_raw_obs_processing_ms: 0.062558437420168\n",
      "  time_since_restore: 152.2425298690796\n",
      "  time_this_iter_s: 6.3376219272613525\n",
      "  time_total_s: 152.2425298690796\n",
      "  timers:\n",
      "    learn_throughput: 1201.262\n",
      "    learn_time_ms: 3329.833\n",
      "    load_throughput: 3466653.442\n",
      "    load_time_ms: 1.154\n",
      "    sample_throughput: 1207.634\n",
      "    sample_time_ms: 3312.263\n",
      "    update_time_ms: 1.757\n",
      "  timestamp: 1628178938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         145.977</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  197.51</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">            197.51</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         152.243</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  183.2 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">            183.2 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 197.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.51\n",
      "  episode_reward_min: 144.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 678\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5328854918479919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011713121086359024\n",
      "          model: {}\n",
      "          policy_loss: -0.010027818381786346\n",
      "          total_loss: 233.86476135253906\n",
      "          vf_explained_var: 0.6983699202537537\n",
      "          vf_loss: 233.86953735351562\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.0\n",
      "    ram_util_percent: 68.84\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04846288409371114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051743125185257755\n",
      "    mean_inference_ms: 0.6791282341247716\n",
      "    mean_raw_obs_processing_ms: 0.062188459609959314\n",
      "  time_since_restore: 152.32965421676636\n",
      "  time_this_iter_s: 6.352401494979858\n",
      "  time_total_s: 152.32965421676636\n",
      "  timers:\n",
      "    learn_throughput: 1198.194\n",
      "    learn_time_ms: 3338.358\n",
      "    load_throughput: 3341875.187\n",
      "    load_time_ms: 1.197\n",
      "    sample_throughput: 1217.374\n",
      "    sample_time_ms: 3285.761\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1628178938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 185.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.52\n",
      "  episode_reward_min: 45.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 685\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5111478567123413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0131230428814888\n",
      "          model: {}\n",
      "          policy_loss: -0.023351473733782768\n",
      "          total_loss: 363.9329528808594\n",
      "          vf_explained_var: 0.3461693525314331\n",
      "          vf_loss: 363.9504089355469\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.75\n",
      "    ram_util_percent: 68.77000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04700208326361862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051314041106319534\n",
      "    mean_inference_ms: 0.680078529164978\n",
      "    mean_raw_obs_processing_ms: 0.06237533073199236\n",
      "  time_since_restore: 158.77009415626526\n",
      "  time_this_iter_s: 6.527564287185669\n",
      "  time_total_s: 158.77009415626526\n",
      "  timers:\n",
      "    learn_throughput: 1197.958\n",
      "    learn_time_ms: 3339.016\n",
      "    load_throughput: 3533607.7\n",
      "    load_time_ms: 1.132\n",
      "    sample_throughput: 1220.19\n",
      "    sample_time_ms: 3278.177\n",
      "    update_time_ms: 1.972\n",
      "  timestamp: 1628178945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         152.33 </td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  197.51</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">            197.51</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         158.77 </td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  185.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">            185.52</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 197.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.76\n",
      "  episode_reward_min: 144.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 698\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5059352517127991\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01825273036956787\n",
      "          model: {}\n",
      "          policy_loss: -0.007759007625281811\n",
      "          total_loss: 153.94058227539062\n",
      "          vf_explained_var: 0.7965815663337708\n",
      "          vf_loss: 153.94009399414062\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.07777777777778\n",
      "    ram_util_percent: 68.77777777777777\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048334052024903054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051607534198396754\n",
      "    mean_inference_ms: 0.6766798003186045\n",
      "    mean_raw_obs_processing_ms: 0.061965401498261345\n",
      "  time_since_restore: 158.8322389125824\n",
      "  time_this_iter_s: 6.50258469581604\n",
      "  time_total_s: 158.8322389125824\n",
      "  timers:\n",
      "    learn_throughput: 1195.968\n",
      "    learn_time_ms: 3344.571\n",
      "    load_throughput: 3155569.432\n",
      "    load_time_ms: 1.268\n",
      "    sample_throughput: 1228.794\n",
      "    sample_time_ms: 3255.225\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1628178945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 187.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.66\n",
      "  episode_reward_min: 45.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 705\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4838162660598755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014362773858010769\n",
      "          model: {}\n",
      "          policy_loss: -0.0285346619784832\n",
      "          total_loss: 144.0735626220703\n",
      "          vf_explained_var: 0.718225359916687\n",
      "          vf_loss: 144.0956573486328\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.925\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04685683778121491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.051155319427564495\n",
      "    mean_inference_ms: 0.6774606153690166\n",
      "    mean_raw_obs_processing_ms: 0.062153409214060706\n",
      "  time_since_restore: 164.8273046016693\n",
      "  time_this_iter_s: 6.057210445404053\n",
      "  time_total_s: 164.8273046016693\n",
      "  timers:\n",
      "    learn_throughput: 1236.154\n",
      "    learn_time_ms: 3235.842\n",
      "    load_throughput: 3567646.834\n",
      "    load_time_ms: 1.121\n",
      "    sample_throughput: 1253.572\n",
      "    sample_time_ms: 3190.882\n",
      "    update_time_ms: 1.98\n",
      "  timestamp: 1628178951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         158.832</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  197.76</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 144</td><td style=\"text-align: right;\">            197.76</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         164.827</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  187.66</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">            187.66</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-51\n",
      "  done: false\n",
      "  episode_len_mean: 194.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.59\n",
      "  episode_reward_min: 51.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 720\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.49499866366386414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015387668274343014\n",
      "          model: {}\n",
      "          policy_loss: -0.011805622838437557\n",
      "          total_loss: 193.64622497558594\n",
      "          vf_explained_var: 0.7694704532623291\n",
      "          vf_loss: 193.65110778808594\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.6\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04817238995292203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05144112460879063\n",
      "    mean_inference_ms: 0.6737765870905613\n",
      "    mean_raw_obs_processing_ms: 0.061704012631213175\n",
      "  time_since_restore: 164.90145421028137\n",
      "  time_this_iter_s: 6.069215297698975\n",
      "  time_total_s: 164.90145421028137\n",
      "  timers:\n",
      "    learn_throughput: 1240.204\n",
      "    learn_time_ms: 3225.275\n",
      "    load_throughput: 3204021.16\n",
      "    load_time_ms: 1.248\n",
      "    sample_throughput: 1259.788\n",
      "    sample_time_ms: 3175.138\n",
      "    update_time_ms: 1.589\n",
      "  timestamp: 1628178951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-57\n",
      "  done: false\n",
      "  episode_len_mean: 185.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.52\n",
      "  episode_reward_min: 45.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 727\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4773971140384674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014291729778051376\n",
      "          model: {}\n",
      "          policy_loss: -0.029293697327375412\n",
      "          total_loss: 320.918212890625\n",
      "          vf_explained_var: 0.47545525431632996\n",
      "          vf_loss: 320.9410705566406\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.10999999999999\n",
      "    ram_util_percent: 68.78\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04670040540197544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050987223545617404\n",
      "    mean_inference_ms: 0.6746423158553307\n",
      "    mean_raw_obs_processing_ms: 0.061914716713589814\n",
      "  time_since_restore: 171.34140968322754\n",
      "  time_this_iter_s: 6.5141050815582275\n",
      "  time_total_s: 171.34140968322754\n",
      "  timers:\n",
      "    learn_throughput: 1241.529\n",
      "    learn_time_ms: 3221.835\n",
      "    load_throughput: 3726696.728\n",
      "    load_time_ms: 1.073\n",
      "    sample_throughput: 1267.707\n",
      "    sample_time_ms: 3155.304\n",
      "    update_time_ms: 2.008\n",
      "  timestamp: 1628178957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         164.901</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  194.59</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  51</td><td style=\"text-align: right;\">            194.59</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         171.341</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  185.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">            185.52</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 189.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.26\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 743\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48611703515052795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016226815059781075\n",
      "          model: {}\n",
      "          policy_loss: -0.01555129699409008\n",
      "          total_loss: 203.2527313232422\n",
      "          vf_explained_var: 0.7903364300727844\n",
      "          vf_loss: 203.26097106933594\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.63333333333334\n",
      "    ram_util_percent: 68.77777777777777\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048014275334907895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0512781811030163\n",
      "    mean_inference_ms: 0.6709729203163866\n",
      "    mean_raw_obs_processing_ms: 0.06144974521750995\n",
      "  time_since_restore: 171.45693135261536\n",
      "  time_this_iter_s: 6.555477142333984\n",
      "  time_total_s: 171.45693135261536\n",
      "  timers:\n",
      "    learn_throughput: 1243.448\n",
      "    learn_time_ms: 3216.862\n",
      "    load_throughput: 3110347.794\n",
      "    load_time_ms: 1.286\n",
      "    sample_throughput: 1266.442\n",
      "    sample_time_ms: 3158.455\n",
      "    update_time_ms: 1.586\n",
      "  timestamp: 1628178958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 187.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.95\n",
      "  episode_reward_min: 45.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 749\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4887058436870575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013799069449305534\n",
      "          model: {}\n",
      "          policy_loss: -0.024712322279810905\n",
      "          total_loss: 350.03350830078125\n",
      "          vf_explained_var: 0.4857853353023529\n",
      "          vf_loss: 350.052001953125\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.5375\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04656769431430824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05084385385314124\n",
      "    mean_inference_ms: 0.6722168785246248\n",
      "    mean_raw_obs_processing_ms: 0.06170919940459019\n",
      "  time_since_restore: 177.36408352851868\n",
      "  time_this_iter_s: 6.022673845291138\n",
      "  time_total_s: 177.36408352851868\n",
      "  timers:\n",
      "    learn_throughput: 1264.388\n",
      "    learn_time_ms: 3163.585\n",
      "    load_throughput: 3657158.801\n",
      "    load_time_ms: 1.094\n",
      "    sample_throughput: 1255.546\n",
      "    sample_time_ms: 3185.865\n",
      "    update_time_ms: 1.984\n",
      "  timestamp: 1628178963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         171.457</td><td style=\"text-align: right;\"> 96000</td><td style=\"text-align: right;\">  189.26</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">            189.26</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         177.364</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  187.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  45</td><td style=\"text-align: right;\">            187.95</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 189.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.05\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 763\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48991551995277405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015393159352242947\n",
      "          model: {}\n",
      "          policy_loss: -0.00811542198061943\n",
      "          total_loss: 261.047607421875\n",
      "          vf_explained_var: 0.6690120697021484\n",
      "          vf_loss: 261.04876708984375\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.65\n",
      "    ram_util_percent: 68.8\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047898090456018796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05115717454909354\n",
      "    mean_inference_ms: 0.6689583746347197\n",
      "    mean_raw_obs_processing_ms: 0.06126209064801195\n",
      "  time_since_restore: 177.4072482585907\n",
      "  time_this_iter_s: 5.950316905975342\n",
      "  time_total_s: 177.4072482585907\n",
      "  timers:\n",
      "    learn_throughput: 1273.536\n",
      "    learn_time_ms: 3140.86\n",
      "    load_throughput: 3094000.184\n",
      "    load_time_ms: 1.293\n",
      "    sample_throughput: 1253.46\n",
      "    sample_time_ms: 3191.167\n",
      "    update_time_ms: 1.546\n",
      "  timestamp: 1628178964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 190.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.83\n",
      "  episode_reward_min: 66.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 769\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4700356721878052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018873386085033417\n",
      "          model: {}\n",
      "          policy_loss: -0.03260192275047302\n",
      "          total_loss: 241.8124542236328\n",
      "          vf_explained_var: 0.5995423197746277\n",
      "          vf_loss: 241.83653259277344\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.53999999999999\n",
      "    ram_util_percent: 68.93999999999998\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04647410637005017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050740854994628\n",
      "    mean_inference_ms: 0.6704181550241871\n",
      "    mean_raw_obs_processing_ms: 0.06155116349452779\n",
      "  time_since_restore: 184.2418065071106\n",
      "  time_this_iter_s: 6.877722978591919\n",
      "  time_total_s: 184.2418065071106\n",
      "  timers:\n",
      "    learn_throughput: 1256.842\n",
      "    learn_time_ms: 3182.58\n",
      "    load_throughput: 3664267.68\n",
      "    load_time_ms: 1.092\n",
      "    sample_throughput: 1257.177\n",
      "    sample_time_ms: 3181.732\n",
      "    update_time_ms: 1.998\n",
      "  timestamp: 1628178970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         177.407</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  189.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">            189.05</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         184.242</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  190.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  66</td><td style=\"text-align: right;\">            190.83</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 188.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.48\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 784\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48433417081832886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013957208022475243\n",
      "          model: {}\n",
      "          policy_loss: -0.011478912085294724\n",
      "          total_loss: 184.51954650878906\n",
      "          vf_explained_var: 0.7350727319717407\n",
      "          vf_loss: 184.52474975585938\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.28\n",
      "    ram_util_percent: 68.93999999999998\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04780615272879426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05106040782269122\n",
      "    mean_inference_ms: 0.6673073063206829\n",
      "    mean_raw_obs_processing_ms: 0.061106406513467416\n",
      "  time_since_restore: 184.32711720466614\n",
      "  time_this_iter_s: 6.9198689460754395\n",
      "  time_total_s: 184.32711720466614\n",
      "  timers:\n",
      "    learn_throughput: 1261.628\n",
      "    learn_time_ms: 3170.507\n",
      "    load_throughput: 2751536.065\n",
      "    load_time_ms: 1.454\n",
      "    sample_throughput: 1254.566\n",
      "    sample_time_ms: 3188.354\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1628178970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 190.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.01\n",
      "  episode_reward_min: 66.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 790\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4733695387840271\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014927005395293236\n",
      "          model: {}\n",
      "          policy_loss: -0.025954818353056908\n",
      "          total_loss: 209.9027099609375\n",
      "          vf_explained_var: 0.6400018334388733\n",
      "          vf_loss: 209.9219207763672\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.23\n",
      "    ram_util_percent: 68.89999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641651377172417\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05067617708933003\n",
      "    mean_inference_ms: 0.6692906026449472\n",
      "    mean_raw_obs_processing_ms: 0.06144596940978202\n",
      "  time_since_restore: 191.24715375900269\n",
      "  time_this_iter_s: 7.00534725189209\n",
      "  time_total_s: 191.24715375900269\n",
      "  timers:\n",
      "    learn_throughput: 1239.588\n",
      "    learn_time_ms: 3226.878\n",
      "    load_throughput: 3717365.949\n",
      "    load_time_ms: 1.076\n",
      "    sample_throughput: 1251.923\n",
      "    sample_time_ms: 3195.086\n",
      "    update_time_ms: 2.015\n",
      "  timestamp: 1628178977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         184.327</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  188.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">            188.48</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         191.247</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  190.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  66</td><td style=\"text-align: right;\">            190.01</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 188.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.61\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 804\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4859400987625122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015628142282366753\n",
      "          model: {}\n",
      "          policy_loss: -0.008607923984527588\n",
      "          total_loss: 229.96719360351562\n",
      "          vf_explained_var: 0.7048779726028442\n",
      "          vf_loss: 229.96876525878906\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.19\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047759412506372216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05100920336185057\n",
      "    mean_inference_ms: 0.666448543475561\n",
      "    mean_raw_obs_processing_ms: 0.06101518772401601\n",
      "  time_since_restore: 191.27043914794922\n",
      "  time_this_iter_s: 6.943321943283081\n",
      "  time_total_s: 191.27043914794922\n",
      "  timers:\n",
      "    learn_throughput: 1242.81\n",
      "    learn_time_ms: 3218.512\n",
      "    load_throughput: 2756916.605\n",
      "    load_time_ms: 1.451\n",
      "    sample_throughput: 1250.619\n",
      "    sample_time_ms: 3198.416\n",
      "    update_time_ms: 1.548\n",
      "  timestamp: 1628178977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 190.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.96\n",
      "  episode_reward_min: 66.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 810\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4726998805999756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014033469371497631\n",
      "          model: {}\n",
      "          policy_loss: -0.02295459806919098\n",
      "          total_loss: 362.708740234375\n",
      "          vf_explained_var: 0.3854067027568817\n",
      "          vf_loss: 362.7253112792969\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.31\n",
      "    ram_util_percent: 68.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046388197104154594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050642101803399876\n",
      "    mean_inference_ms: 0.6686550748518286\n",
      "    mean_raw_obs_processing_ms: 0.06138108311066465\n",
      "  time_since_restore: 198.1115701198578\n",
      "  time_this_iter_s: 6.8644163608551025\n",
      "  time_total_s: 198.1115701198578\n",
      "  timers:\n",
      "    learn_throughput: 1220.838\n",
      "    learn_time_ms: 3276.439\n",
      "    load_throughput: 3817949.617\n",
      "    load_time_ms: 1.048\n",
      "    sample_throughput: 1251.656\n",
      "    sample_time_ms: 3195.766\n",
      "    update_time_ms: 1.948\n",
      "  timestamp: 1628178984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         191.27 </td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  188.61</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">            188.61</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         198.112</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  190.96</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  66</td><td style=\"text-align: right;\">            190.96</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 190.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.96\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 824\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.47867095470428467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015810811892151833\n",
      "          model: {}\n",
      "          policy_loss: -0.01862136274576187\n",
      "          total_loss: 206.54986572265625\n",
      "          vf_explained_var: 0.7761250734329224\n",
      "          vf_loss: 206.56134033203125\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.34\n",
      "    ram_util_percent: 68.91999999999999\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047728972361249226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05097441265973129\n",
      "    mean_inference_ms: 0.6657817794018945\n",
      "    mean_raw_obs_processing_ms: 0.06094669884955794\n",
      "  time_since_restore: 198.13624691963196\n",
      "  time_this_iter_s: 6.865807771682739\n",
      "  time_total_s: 198.13624691963196\n",
      "  timers:\n",
      "    learn_throughput: 1221.714\n",
      "    learn_time_ms: 3274.088\n",
      "    load_throughput: 2779203.207\n",
      "    load_time_ms: 1.439\n",
      "    sample_throughput: 1254.349\n",
      "    sample_time_ms: 3188.906\n",
      "    update_time_ms: 1.552\n",
      "  timestamp: 1628178984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 194.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.65\n",
      "  episode_reward_min: 66.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 830\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4703579246997833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016203029081225395\n",
      "          model: {}\n",
      "          policy_loss: -0.03159931302070618\n",
      "          total_loss: 289.38958740234375\n",
      "          vf_explained_var: 0.48571866750717163\n",
      "          vf_loss: 289.41387939453125\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.48181818181818\n",
      "    ram_util_percent: 68.89999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046387469405846565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05063541254953194\n",
      "    mean_inference_ms: 0.6684627466985197\n",
      "    mean_raw_obs_processing_ms: 0.06135061605477169\n",
      "  time_since_restore: 205.4150745868683\n",
      "  time_this_iter_s: 7.303504467010498\n",
      "  time_total_s: 205.4150745868683\n",
      "  timers:\n",
      "    learn_throughput: 1204.918\n",
      "    learn_time_ms: 3319.727\n",
      "    load_throughput: 3661388.853\n",
      "    load_time_ms: 1.092\n",
      "    sample_throughput: 1243.783\n",
      "    sample_time_ms: 3215.995\n",
      "    update_time_ms: 1.948\n",
      "  timestamp: 1628178992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         198.136</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  190.96</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">            190.96</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         205.415</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  194.65</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  66</td><td style=\"text-align: right;\">            194.65</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 195.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.57\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 845\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.46709880232810974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01765323057770729\n",
      "          model: {}\n",
      "          policy_loss: -0.009106539189815521\n",
      "          total_loss: 136.77613830566406\n",
      "          vf_explained_var: 0.8330937623977661\n",
      "          vf_loss: 136.7772979736328\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.43636363636364\n",
      "    ram_util_percent: 68.89090909090909\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047723870797844674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05096442033557741\n",
      "    mean_inference_ms: 0.6655309715461329\n",
      "    mean_raw_obs_processing_ms: 0.06091259925149628\n",
      "  time_since_restore: 205.4417359828949\n",
      "  time_this_iter_s: 7.3054890632629395\n",
      "  time_total_s: 205.4417359828949\n",
      "  timers:\n",
      "    learn_throughput: 1205.357\n",
      "    learn_time_ms: 3318.52\n",
      "    load_throughput: 2789507.848\n",
      "    load_time_ms: 1.434\n",
      "    sample_throughput: 1245.226\n",
      "    sample_time_ms: 3212.269\n",
      "    update_time_ms: 1.592\n",
      "  timestamp: 1628178992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 197.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.66\n",
      "  episode_reward_min: 133.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 850\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.47074759006500244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015587147325277328\n",
      "          model: {}\n",
      "          policy_loss: -0.028155235573649406\n",
      "          total_loss: 267.5048828125\n",
      "          vf_explained_var: 0.4699432849884033\n",
      "          vf_loss: 267.5260314941406\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.75555555555556\n",
      "    ram_util_percent: 68.98888888888888\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046403512092267396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0506479519733381\n",
      "    mean_inference_ms: 0.668527294864366\n",
      "    mean_raw_obs_processing_ms: 0.06134225144343231\n",
      "  time_since_restore: 212.2777054309845\n",
      "  time_this_iter_s: 6.862630844116211\n",
      "  time_total_s: 212.2777054309845\n",
      "  timers:\n",
      "    learn_throughput: 1181.217\n",
      "    learn_time_ms: 3386.337\n",
      "    load_throughput: 3549005.987\n",
      "    load_time_ms: 1.127\n",
      "    sample_throughput: 1233.635\n",
      "    sample_time_ms: 3242.45\n",
      "    update_time_ms: 1.926\n",
      "  timestamp: 1628178998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         205.442</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  195.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            195.57</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         212.278</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  197.66</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 133</td><td style=\"text-align: right;\">            197.66</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 197.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.05\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 865\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.45842114090919495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015388299711048603\n",
      "          model: {}\n",
      "          policy_loss: -0.00775099266320467\n",
      "          total_loss: 317.7195129394531\n",
      "          vf_explained_var: 0.7282341122627258\n",
      "          vf_loss: 317.7203369140625\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.06666666666666\n",
      "    ram_util_percent: 69.01111111111112\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0477379128754368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05097667126931322\n",
      "    mean_inference_ms: 0.6655856950983909\n",
      "    mean_raw_obs_processing_ms: 0.06090494436302562\n",
      "  time_since_restore: 212.25999069213867\n",
      "  time_this_iter_s: 6.818254709243774\n",
      "  time_total_s: 212.25999069213867\n",
      "  timers:\n",
      "    learn_throughput: 1185.467\n",
      "    learn_time_ms: 3374.197\n",
      "    load_throughput: 2760954.481\n",
      "    load_time_ms: 1.449\n",
      "    sample_throughput: 1232.325\n",
      "    sample_time_ms: 3245.897\n",
      "    update_time_ms: 1.596\n",
      "  timestamp: 1628178998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 198.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.45\n",
      "  episode_reward_min: 143.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 870\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4692208170890808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01932750828564167\n",
      "          model: {}\n",
      "          policy_loss: -0.03070206008851528\n",
      "          total_loss: 237.67001342773438\n",
      "          vf_explained_var: 0.5686130523681641\n",
      "          vf_loss: 237.6920166015625\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.67\n",
      "    ram_util_percent: 69.28\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046405043586298385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050645596145039565\n",
      "    mean_inference_ms: 0.6683940429460398\n",
      "    mean_raw_obs_processing_ms: 0.06131825019386123\n",
      "  time_since_restore: 218.93823552131653\n",
      "  time_this_iter_s: 6.660530090332031\n",
      "  time_total_s: 218.93823552131653\n",
      "  timers:\n",
      "    learn_throughput: 1171.648\n",
      "    learn_time_ms: 3413.994\n",
      "    load_throughput: 3499554.87\n",
      "    load_time_ms: 1.143\n",
      "    sample_throughput: 1231.909\n",
      "    sample_time_ms: 3246.994\n",
      "    update_time_ms: 1.926\n",
      "  timestamp: 1628179005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         212.26 </td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  197.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            197.05</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         218.938</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  198.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 143</td><td style=\"text-align: right;\">            198.45</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 197.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.22\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 885\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48942166566848755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013010112568736076\n",
      "          model: {}\n",
      "          policy_loss: -0.01044501457363367\n",
      "          total_loss: 304.96478271484375\n",
      "          vf_explained_var: 0.694467306137085\n",
      "          vf_loss: 304.9693603515625\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.34\n",
      "    ram_util_percent: 69.28999999999999\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04773816561429122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050974280360017075\n",
      "    mean_inference_ms: 0.6654971299297737\n",
      "    mean_raw_obs_processing_ms: 0.06088844951223936\n",
      "  time_since_restore: 218.89818787574768\n",
      "  time_this_iter_s: 6.638197183609009\n",
      "  time_total_s: 218.89818787574768\n",
      "  timers:\n",
      "    learn_throughput: 1178.52\n",
      "    learn_time_ms: 3394.086\n",
      "    load_throughput: 2810018.591\n",
      "    load_time_ms: 1.423\n",
      "    sample_throughput: 1229.074\n",
      "    sample_time_ms: 3254.483\n",
      "    update_time_ms: 1.593\n",
      "  timestamp: 1628179005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 199.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.41\n",
      "  episode_reward_min: 173.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 890\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.461927592754364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015139340423047543\n",
      "          model: {}\n",
      "          policy_loss: -0.02771943435072899\n",
      "          total_loss: 278.73822021484375\n",
      "          vf_explained_var: 0.5061262249946594\n",
      "          vf_loss: 278.7591552734375\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.84\n",
      "    ram_util_percent: 69.21000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04639810361457547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05063258238698452\n",
      "    mean_inference_ms: 0.6680724892169855\n",
      "    mean_raw_obs_processing_ms: 0.06128130070890748\n",
      "  time_since_restore: 225.7139937877655\n",
      "  time_this_iter_s: 6.775758266448975\n",
      "  time_total_s: 225.7139937877655\n",
      "  timers:\n",
      "    learn_throughput: 1167.859\n",
      "    learn_time_ms: 3425.071\n",
      "    load_throughput: 3472752.789\n",
      "    load_time_ms: 1.152\n",
      "    sample_throughput: 1226.68\n",
      "    sample_time_ms: 3260.834\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1628179012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         218.898</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  197.22</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            197.22</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         225.714</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">  199.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 173</td><td style=\"text-align: right;\">            199.41</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 198.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.7\n",
      "  episode_reward_min: 141.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 905\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4716759920120239\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016324911266565323\n",
      "          model: {}\n",
      "          policy_loss: -0.010216062888503075\n",
      "          total_loss: 195.91134643554688\n",
      "          vf_explained_var: 0.713370680809021\n",
      "          vf_loss: 195.91416931152344\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.83000000000001\n",
      "    ram_util_percent: 69.22\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04772852878594387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05096303866297422\n",
      "    mean_inference_ms: 0.6652446728205174\n",
      "    mean_raw_obs_processing_ms: 0.06086297487786977\n",
      "  time_since_restore: 225.68410086631775\n",
      "  time_this_iter_s: 6.785912990570068\n",
      "  time_total_s: 225.68410086631775\n",
      "  timers:\n",
      "    learn_throughput: 1173.17\n",
      "    learn_time_ms: 3409.564\n",
      "    load_throughput: 2906303.116\n",
      "    load_time_ms: 1.376\n",
      "    sample_throughput: 1224.214\n",
      "    sample_time_ms: 3267.401\n",
      "    update_time_ms: 1.578\n",
      "  timestamp: 1628179012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 332a6_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 197.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.54\n",
      "  episode_reward_min: 91.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 926\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.47960004210472107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016852959990501404\n",
      "          model: {}\n",
      "          policy_loss: -0.010698175989091396\n",
      "          total_loss: 349.6728820800781\n",
      "          vf_explained_var: 0.5627517700195312\n",
      "          vf_loss: 349.6759033203125\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.56666666666666\n",
      "    ram_util_percent: 69.32222222222222\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04772640405339509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050959363258962824\n",
      "    mean_inference_ms: 0.6651336498361536\n",
      "    mean_raw_obs_processing_ms: 0.0608502581815713\n",
      "  time_since_restore: 232.04264760017395\n",
      "  time_this_iter_s: 6.358546733856201\n",
      "  time_total_s: 232.04264760017395\n",
      "  timers:\n",
      "    learn_throughput: 1172.175\n",
      "    learn_time_ms: 3412.46\n",
      "    load_throughput: 2894020.562\n",
      "    load_time_ms: 1.382\n",
      "    sample_throughput: 1214.576\n",
      "    sample_time_ms: 3293.331\n",
      "    update_time_ms: 1.597\n",
      "  timestamp: 1628179018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         232.043</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  197.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  91</td><td style=\"text-align: right;\">            197.54</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         225.714</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">  199.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 173</td><td style=\"text-align: right;\">            199.41</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 198.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.49\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 910\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.46486741304397583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013968742452561855\n",
      "          model: {}\n",
      "          policy_loss: -0.032236214727163315\n",
      "          total_loss: 265.2813415527344\n",
      "          vf_explained_var: 0.5637696385383606\n",
      "          vf_loss: 265.3072814941406\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.39999999999999\n",
      "    ram_util_percent: 69.3111111111111\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04639574658610261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050626908228341684\n",
      "    mean_inference_ms: 0.6678789075883277\n",
      "    mean_raw_obs_processing_ms: 0.06125479340985193\n",
      "  time_since_restore: 232.21240520477295\n",
      "  time_this_iter_s: 6.498411417007446\n",
      "  time_total_s: 232.21240520477295\n",
      "  timers:\n",
      "    learn_throughput: 1163.102\n",
      "    learn_time_ms: 3439.079\n",
      "    load_throughput: 3535767.334\n",
      "    load_time_ms: 1.131\n",
      "    sample_throughput: 1215.478\n",
      "    sample_time_ms: 3290.886\n",
      "    update_time_ms: 1.759\n",
      "  timestamp: 1628179018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 198.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.69\n",
      "  episode_reward_min: 91.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 946\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4742071032524109\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015636736527085304\n",
      "          model: {}\n",
      "          policy_loss: -0.009847583249211311\n",
      "          total_loss: 301.3443908691406\n",
      "          vf_explained_var: 0.5731571912765503\n",
      "          vf_loss: 301.3472595214844\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.88888888888889\n",
      "    ram_util_percent: 69.34444444444446\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04770780066872508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050939480736620746\n",
      "    mean_inference_ms: 0.6647419527047816\n",
      "    mean_raw_obs_processing_ms: 0.060812923485138025\n",
      "  time_since_restore: 238.76015853881836\n",
      "  time_this_iter_s: 6.717510938644409\n",
      "  time_total_s: 238.76015853881836\n",
      "  timers:\n",
      "    learn_throughput: 1168.777\n",
      "    learn_time_ms: 3422.382\n",
      "    load_throughput: 2982616.178\n",
      "    load_time_ms: 1.341\n",
      "    sample_throughput: 1212.271\n",
      "    sample_time_ms: 3299.593\n",
      "    update_time_ms: 1.607\n",
      "  timestamp: 1628179025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         238.76 </td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">  198.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  91</td><td style=\"text-align: right;\">            198.69</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         232.212</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  198.49</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.49</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 198.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.25\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 931\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.45865532755851746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015751317143440247\n",
      "          model: {}\n",
      "          policy_loss: -0.03036605566740036\n",
      "          total_loss: 301.280029296875\n",
      "          vf_explained_var: 0.5099173784255981\n",
      "          vf_loss: 301.3033447265625\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.61\n",
      "    ram_util_percent: 69.33000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04637748937451235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050605767936294364\n",
      "    mean_inference_ms: 0.6674285278292651\n",
      "    mean_raw_obs_processing_ms: 0.06121250944806599\n",
      "  time_since_restore: 238.91669940948486\n",
      "  time_this_iter_s: 6.704294204711914\n",
      "  time_total_s: 238.91669940948486\n",
      "  timers:\n",
      "    learn_throughput: 1161.354\n",
      "    learn_time_ms: 3444.256\n",
      "    load_throughput: 3441621.4\n",
      "    load_time_ms: 1.162\n",
      "    sample_throughput: 1210.379\n",
      "    sample_time_ms: 3304.749\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1628179025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-12\n",
      "  done: false\n",
      "  episode_len_mean: 198.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.1\n",
      "  episode_reward_min: 91.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 966\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48510754108428955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011670752428472042\n",
      "          model: {}\n",
      "          policy_loss: -0.011164366267621517\n",
      "          total_loss: 248.50257873535156\n",
      "          vf_explained_var: 0.5877789855003357\n",
      "          vf_loss: 248.50848388671875\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.66\n",
      "    ram_util_percent: 69.20000000000002\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047677390359501665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05090729918539628\n",
      "    mean_inference_ms: 0.664152788851505\n",
      "    mean_raw_obs_processing_ms: 0.06076105435747278\n",
      "  time_since_restore: 245.36999034881592\n",
      "  time_this_iter_s: 6.609831809997559\n",
      "  time_total_s: 245.36999034881592\n",
      "  timers:\n",
      "    learn_throughput: 1147.667\n",
      "    learn_time_ms: 3485.331\n",
      "    load_throughput: 3006292.401\n",
      "    load_time_ms: 1.331\n",
      "    sample_throughput: 1211.151\n",
      "    sample_time_ms: 3302.643\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1628179032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         245.37 </td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  198.1 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  91</td><td style=\"text-align: right;\">            198.1 </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         238.917</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">  198.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.25</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-12\n",
      "  done: false\n",
      "  episode_len_mean: 197.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.71\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 951\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4516945779323578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01393092516809702\n",
      "          model: {}\n",
      "          policy_loss: -0.027872402220964432\n",
      "          total_loss: 389.576416015625\n",
      "          vf_explained_var: 0.332857221364975\n",
      "          vf_loss: 389.5979919433594\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.05555555555554\n",
      "    ram_util_percent: 69.2\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04634866096000671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05057271618774757\n",
      "    mean_inference_ms: 0.6668280597842624\n",
      "    mean_raw_obs_processing_ms: 0.06115838723575122\n",
      "  time_since_restore: 245.53989720344543\n",
      "  time_this_iter_s: 6.623197793960571\n",
      "  time_total_s: 245.53989720344543\n",
      "  timers:\n",
      "    learn_throughput: 1142.237\n",
      "    learn_time_ms: 3501.899\n",
      "    load_throughput: 3392146.222\n",
      "    load_time_ms: 1.179\n",
      "    sample_throughput: 1209.459\n",
      "    sample_time_ms: 3307.264\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1628179032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 195.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.98\n",
      "  episode_reward_min: 91.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 987\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4889105260372162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015719469636678696\n",
      "          model: {}\n",
      "          policy_loss: -0.01008165068924427\n",
      "          total_loss: 165.18226623535156\n",
      "          vf_explained_var: 0.7378417253494263\n",
      "          vf_loss: 165.1852569580078\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.32222222222222\n",
      "    ram_util_percent: 69.27777777777777\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04763058346690229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05085769460353333\n",
      "    mean_inference_ms: 0.6632380846418487\n",
      "    mean_raw_obs_processing_ms: 0.06068097407040044\n",
      "  time_since_restore: 251.72097158432007\n",
      "  time_this_iter_s: 6.35098123550415\n",
      "  time_total_s: 251.72097158432007\n",
      "  timers:\n",
      "    learn_throughput: 1153.703\n",
      "    learn_time_ms: 3467.098\n",
      "    load_throughput: 3438658.742\n",
      "    load_time_ms: 1.163\n",
      "    sample_throughput: 1225.441\n",
      "    sample_time_ms: 3264.132\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1628179038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.6/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         251.721</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  195.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  91</td><td style=\"text-align: right;\">            195.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         245.54 </td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  197.71</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            197.71</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 198.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.15\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 971\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4520763158798218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01385513972491026\n",
      "          model: {}\n",
      "          policy_loss: -0.029157467186450958\n",
      "          total_loss: 318.5737609863281\n",
      "          vf_explained_var: 0.39835217595100403\n",
      "          vf_loss: 318.5965881347656\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.3\n",
      "    ram_util_percent: 69.27777777777777\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046305104700119414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05052408198685084\n",
      "    mean_inference_ms: 0.6660140535617133\n",
      "    mean_raw_obs_processing_ms: 0.061085658763533016\n",
      "  time_since_restore: 251.93815517425537\n",
      "  time_this_iter_s: 6.3982579708099365\n",
      "  time_total_s: 251.93815517425537\n",
      "  timers:\n",
      "    learn_throughput: 1145.699\n",
      "    learn_time_ms: 3491.32\n",
      "    load_throughput: 3355711.657\n",
      "    load_time_ms: 1.192\n",
      "    sample_throughput: 1223.284\n",
      "    sample_time_ms: 3269.886\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1628179038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 193.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.45\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1008\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4637436270713806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019986802712082863\n",
      "          model: {}\n",
      "          policy_loss: -0.01162801869213581\n",
      "          total_loss: 151.9436492919922\n",
      "          vf_explained_var: 0.7686829566955566\n",
      "          vf_loss: 151.9462432861328\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16\n",
      "    ram_util_percent: 69.33000000000001\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04759401592765279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050819117704380454\n",
      "    mean_inference_ms: 0.6625030813783104\n",
      "    mean_raw_obs_processing_ms: 0.060615510694695535\n",
      "  time_since_restore: 259.0805621147156\n",
      "  time_this_iter_s: 7.359590530395508\n",
      "  time_total_s: 259.0805621147156\n",
      "  timers:\n",
      "    learn_throughput: 1138.878\n",
      "    learn_time_ms: 3512.229\n",
      "    load_throughput: 3443599.343\n",
      "    load_time_ms: 1.162\n",
      "    sample_throughput: 1226.786\n",
      "    sample_time_ms: 3260.552\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1628179045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         259.081</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  193.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">            193.45</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         251.938</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  198.15</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.15</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 198.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.15\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 991\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4684497117996216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016661187633872032\n",
      "          model: {}\n",
      "          policy_loss: -0.03253411874175072\n",
      "          total_loss: 202.54544067382812\n",
      "          vf_explained_var: 0.5680208802223206\n",
      "          vf_loss: 202.57049560546875\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.78181818181818\n",
      "    ram_util_percent: 69.33636363636363\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046266766242266194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05048248891562245\n",
      "    mean_inference_ms: 0.6653388739098348\n",
      "    mean_raw_obs_processing_ms: 0.061025560780854686\n",
      "  time_since_restore: 259.288503408432\n",
      "  time_this_iter_s: 7.350348234176636\n",
      "  time_total_s: 259.288503408432\n",
      "  timers:\n",
      "    learn_throughput: 1133.083\n",
      "    learn_time_ms: 3530.192\n",
      "    load_throughput: 3259800.649\n",
      "    load_time_ms: 1.227\n",
      "    sample_throughput: 1224.938\n",
      "    sample_time_ms: 3265.472\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1628179046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-32\n",
      "  done: false\n",
      "  episode_len_mean: 191.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.33\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.46478769183158875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01438430417329073\n",
      "          model: {}\n",
      "          policy_loss: -0.008885260671377182\n",
      "          total_loss: 89.54183197021484\n",
      "          vf_explained_var: 0.8574172258377075\n",
      "          vf_loss: 89.54425048828125\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.36666666666667\n",
      "    ram_util_percent: 69.32222222222222\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047579272829095506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05080564826785162\n",
      "    mean_inference_ms: 0.6622484819179912\n",
      "    mean_raw_obs_processing_ms: 0.06058006926570474\n",
      "  time_since_restore: 265.4677402973175\n",
      "  time_this_iter_s: 6.387178182601929\n",
      "  time_total_s: 265.4677402973175\n",
      "  timers:\n",
      "    learn_throughput: 1171.192\n",
      "    learn_time_ms: 3415.325\n",
      "    load_throughput: 3447491.215\n",
      "    load_time_ms: 1.16\n",
      "    sample_throughput: 1208.585\n",
      "    sample_time_ms: 3309.655\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1628179052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         265.468</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  191.33</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            191.33</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         259.289</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  198.15</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.15</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-32\n",
      "  done: false\n",
      "  episode_len_mean: 199.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.07\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1011\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.45100852847099304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013020322658121586\n",
      "          model: {}\n",
      "          policy_loss: -0.02289060689508915\n",
      "          total_loss: 363.9102478027344\n",
      "          vf_explained_var: 0.2748624086380005\n",
      "          vf_loss: 363.9272766113281\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.19999999999999\n",
      "    ram_util_percent: 69.34444444444445\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04625085066852498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05046576718239452\n",
      "    mean_inference_ms: 0.6651056069433714\n",
      "    mean_raw_obs_processing_ms: 0.06100019875272182\n",
      "  time_since_restore: 265.6748206615448\n",
      "  time_this_iter_s: 6.386317253112793\n",
      "  time_total_s: 265.6748206615448\n",
      "  timers:\n",
      "    learn_throughput: 1162.196\n",
      "    learn_time_ms: 3441.76\n",
      "    load_throughput: 3245423.348\n",
      "    load_time_ms: 1.233\n",
      "    sample_throughput: 1209.864\n",
      "    sample_time_ms: 3306.157\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1628179052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 191.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.33\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1050\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.46145889163017273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013417061418294907\n",
      "          model: {}\n",
      "          policy_loss: -0.007734974380582571\n",
      "          total_loss: 161.98690795898438\n",
      "          vf_explained_var: 0.7479246854782104\n",
      "          vf_loss: 161.9886016845703\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.25454545454546\n",
      "    ram_util_percent: 69.40909090909089\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047570720902570314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050797319697935474\n",
      "    mean_inference_ms: 0.662110915988024\n",
      "    mean_raw_obs_processing_ms: 0.06055427992372147\n",
      "  time_since_restore: 272.6185894012451\n",
      "  time_this_iter_s: 7.150849103927612\n",
      "  time_total_s: 272.6185894012451\n",
      "  timers:\n",
      "    learn_throughput: 1171.477\n",
      "    learn_time_ms: 3414.494\n",
      "    load_throughput: 3473184.142\n",
      "    load_time_ms: 1.152\n",
      "    sample_throughput: 1213.935\n",
      "    sample_time_ms: 3295.07\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1628179059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         272.619</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  191.33</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            191.33</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         265.675</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  199.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            199.07</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 199.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.29\n",
      "  episode_reward_min: 131.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1031\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4462071657180786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015167959965765476\n",
      "          model: {}\n",
      "          policy_loss: -0.030004551634192467\n",
      "          total_loss: 258.063720703125\n",
      "          vf_explained_var: 0.46815529465675354\n",
      "          vf_loss: 258.08697509765625\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.7\n",
      "    ram_util_percent: 69.41\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04624150137058572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05045403663131858\n",
      "    mean_inference_ms: 0.6649660944341345\n",
      "    mean_raw_obs_processing_ms: 0.06097838250985512\n",
      "  time_since_restore: 272.87577867507935\n",
      "  time_this_iter_s: 7.200958013534546\n",
      "  time_total_s: 272.87577867507935\n",
      "  timers:\n",
      "    learn_throughput: 1162.682\n",
      "    learn_time_ms: 3440.322\n",
      "    load_throughput: 3297603.239\n",
      "    load_time_ms: 1.213\n",
      "    sample_throughput: 1213.062\n",
      "    sample_time_ms: 3297.441\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1628179059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 190.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.75\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1071\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.463103324174881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016733823344111443\n",
      "          model: {}\n",
      "          policy_loss: -0.010638998821377754\n",
      "          total_loss: 134.30804443359375\n",
      "          vf_explained_var: 0.7803204655647278\n",
      "          vf_loss: 134.3111572265625\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.64000000000001\n",
      "    ram_util_percent: 69.36999999999999\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0475768984917768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050803858728435694\n",
      "    mean_inference_ms: 0.6622645641295455\n",
      "    mean_raw_obs_processing_ms: 0.06054599943735244\n",
      "  time_since_restore: 279.74033641815186\n",
      "  time_this_iter_s: 7.121747016906738\n",
      "  time_total_s: 279.74033641815186\n",
      "  timers:\n",
      "    learn_throughput: 1160.622\n",
      "    learn_time_ms: 3446.429\n",
      "    load_throughput: 3519892.581\n",
      "    load_time_ms: 1.136\n",
      "    sample_throughput: 1214.561\n",
      "    sample_time_ms: 3293.372\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1628179066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         279.74 </td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  190.75</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            190.75</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         272.876</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  199.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            199.29</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-46\n",
      "  done: false\n",
      "  episode_len_mean: 199.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.87\n",
      "  episode_reward_min: 189.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1051\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4386503994464874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015519056469202042\n",
      "          model: {}\n",
      "          policy_loss: -0.027365466579794884\n",
      "          total_loss: 292.3857727050781\n",
      "          vf_explained_var: 0.36509066820144653\n",
      "          vf_loss: 292.4061584472656\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.64000000000001\n",
      "    ram_util_percent: 69.36999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04624559410842997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05045617110041697\n",
      "    mean_inference_ms: 0.6650867508703349\n",
      "    mean_raw_obs_processing_ms: 0.060981913392395753\n",
      "  time_since_restore: 279.9963357448578\n",
      "  time_this_iter_s: 7.120557069778442\n",
      "  time_total_s: 279.9963357448578\n",
      "  timers:\n",
      "    learn_throughput: 1156.932\n",
      "    learn_time_ms: 3457.42\n",
      "    load_throughput: 3292813.88\n",
      "    load_time_ms: 1.215\n",
      "    sample_throughput: 1209.92\n",
      "    sample_time_ms: 3306.003\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1628179066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 192.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.98\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1091\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4897519648075104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019797788932919502\n",
      "          model: {}\n",
      "          policy_loss: -0.01268028561025858\n",
      "          total_loss: 316.6623840332031\n",
      "          vf_explained_var: 0.5362884402275085\n",
      "          vf_loss: 316.6661682128906\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.7\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047578017460276775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050804799068878255\n",
      "    mean_inference_ms: 0.6623442402993139\n",
      "    mean_raw_obs_processing_ms: 0.060533188635450645\n",
      "  time_since_restore: 286.64205598831177\n",
      "  time_this_iter_s: 6.901719570159912\n",
      "  time_total_s: 286.64205598831177\n",
      "  timers:\n",
      "    learn_throughput: 1142.496\n",
      "    learn_time_ms: 3501.107\n",
      "    load_throughput: 3583650.034\n",
      "    load_time_ms: 1.116\n",
      "    sample_throughput: 1225.072\n",
      "    sample_time_ms: 3265.115\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1628179073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         286.642</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  192.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            192.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         279.996</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  199.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 189</td><td style=\"text-align: right;\">            199.87</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 199.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.87\n",
      "  episode_reward_min: 189.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1071\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4426604211330414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015439558774232864\n",
      "          model: {}\n",
      "          policy_loss: -0.030230877920985222\n",
      "          total_loss: 284.50604248046875\n",
      "          vf_explained_var: 0.41533058881759644\n",
      "          vf_loss: 284.5293273925781\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.60999999999999\n",
      "    ram_util_percent: 69.49000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046254013378625246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0504616959227572\n",
      "    mean_inference_ms: 0.6652661597075081\n",
      "    mean_raw_obs_processing_ms: 0.060988908617613156\n",
      "  time_since_restore: 286.8792407512665\n",
      "  time_this_iter_s: 6.882905006408691\n",
      "  time_total_s: 286.8792407512665\n",
      "  timers:\n",
      "    learn_throughput: 1142.316\n",
      "    learn_time_ms: 3501.659\n",
      "    load_throughput: 3323537.242\n",
      "    load_time_ms: 1.204\n",
      "    sample_throughput: 1217.968\n",
      "    sample_time_ms: 3284.16\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1628179073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 195.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.98\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1111\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.48659420013427734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014859908260405064\n",
      "          model: {}\n",
      "          policy_loss: -0.008004746399819851\n",
      "          total_loss: 218.59378051757812\n",
      "          vf_explained_var: 0.6418334245681763\n",
      "          vf_loss: 218.5950927734375\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.32\n",
      "    ram_util_percent: 69.51\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047571769870977436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0507964326011537\n",
      "    mean_inference_ms: 0.6622818647343227\n",
      "    mean_raw_obs_processing_ms: 0.0605080614137855\n",
      "  time_since_restore: 293.47748136520386\n",
      "  time_this_iter_s: 6.83542537689209\n",
      "  time_total_s: 293.47748136520386\n",
      "  timers:\n",
      "    learn_throughput: 1139.907\n",
      "    learn_time_ms: 3509.057\n",
      "    load_throughput: 3648331.231\n",
      "    load_time_ms: 1.096\n",
      "    sample_throughput: 1226.239\n",
      "    sample_time_ms: 3262.006\n",
      "    update_time_ms: 1.729\n",
      "  timestamp: 1628179080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         293.477</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">  195.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            195.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         286.879</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  199.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 189</td><td style=\"text-align: right;\">            199.87</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 198.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.84\n",
      "  episode_reward_min: 123.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1091\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.440842866897583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012930660508573055\n",
      "          model: {}\n",
      "          policy_loss: -0.022974802181124687\n",
      "          total_loss: 455.3226318359375\n",
      "          vf_explained_var: 0.3721712529659271\n",
      "          vf_loss: 455.33978271484375\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.4\n",
      "    ram_util_percent: 69.51\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046256932592164846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05046070993749573\n",
      "    mean_inference_ms: 0.665323924273451\n",
      "    mean_raw_obs_processing_ms: 0.060985653807310955\n",
      "  time_since_restore: 293.7724051475525\n",
      "  time_this_iter_s: 6.893164396286011\n",
      "  time_total_s: 293.7724051475525\n",
      "  timers:\n",
      "    learn_throughput: 1137.808\n",
      "    learn_time_ms: 3515.532\n",
      "    load_throughput: 3307680.297\n",
      "    load_time_ms: 1.209\n",
      "    sample_throughput: 1218.695\n",
      "    sample_time_ms: 3282.199\n",
      "    update_time_ms: 1.604\n",
      "  timestamp: 1628179080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 198.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.83\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1131\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4698285460472107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02050042897462845\n",
      "          model: {}\n",
      "          policy_loss: -0.010055511258542538\n",
      "          total_loss: 258.257568359375\n",
      "          vf_explained_var: 0.5740774273872375\n",
      "          vf_loss: 258.2583923339844\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.41111111111111\n",
      "    ram_util_percent: 69.47777777777777\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04756311241140487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050784047524103856\n",
      "    mean_inference_ms: 0.6621181925495732\n",
      "    mean_raw_obs_processing_ms: 0.06047858614220017\n",
      "  time_since_restore: 299.7180597782135\n",
      "  time_this_iter_s: 6.2405784130096436\n",
      "  time_total_s: 299.7180597782135\n",
      "  timers:\n",
      "    learn_throughput: 1153.576\n",
      "    learn_time_ms: 3467.478\n",
      "    load_throughput: 3615233.909\n",
      "    load_time_ms: 1.106\n",
      "    sample_throughput: 1215.143\n",
      "    sample_time_ms: 3291.795\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1628179086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         299.718</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  198.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.83</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         293.772</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">  198.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 123</td><td style=\"text-align: right;\">            198.84</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 198.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.84\n",
      "  episode_reward_min: 123.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1111\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.45375922322273254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017828019335865974\n",
      "          model: {}\n",
      "          policy_loss: -0.035208579152822495\n",
      "          total_loss: 226.9047088623047\n",
      "          vf_explained_var: 0.5231276154518127\n",
      "          vf_loss: 226.93191528320312\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.27777777777777\n",
      "    ram_util_percent: 69.47777777777777\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04625570924266548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05045487742783517\n",
      "    mean_inference_ms: 0.6652411394230591\n",
      "    mean_raw_obs_processing_ms: 0.06097697139215571\n",
      "  time_since_restore: 300.0016779899597\n",
      "  time_this_iter_s: 6.229272842407227\n",
      "  time_total_s: 300.0016779899597\n",
      "  timers:\n",
      "    learn_throughput: 1155.113\n",
      "    learn_time_ms: 3462.864\n",
      "    load_throughput: 3215935.901\n",
      "    load_time_ms: 1.244\n",
      "    sample_throughput: 1209.146\n",
      "    sample_time_ms: 3308.119\n",
      "    update_time_ms: 1.562\n",
      "  timestamp: 1628179086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-13\n",
      "  done: false\n",
      "  episode_len_mean: 198.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.98\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1151\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4629519581794739\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01458141952753067\n",
      "          model: {}\n",
      "          policy_loss: -0.010210716165602207\n",
      "          total_loss: 253.43775939941406\n",
      "          vf_explained_var: 0.6014117002487183\n",
      "          vf_loss: 253.43809509277344\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.83333333333333\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04754341479611686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05076172387870202\n",
      "    mean_inference_ms: 0.6617535139437907\n",
      "    mean_raw_obs_processing_ms: 0.060436823630998296\n",
      "  time_since_restore: 306.5661954879761\n",
      "  time_this_iter_s: 6.848135709762573\n",
      "  time_total_s: 306.5661954879761\n",
      "  timers:\n",
      "    learn_throughput: 1145.106\n",
      "    learn_time_ms: 3493.126\n",
      "    load_throughput: 3513700.26\n",
      "    load_time_ms: 1.138\n",
      "    sample_throughput: 1219.889\n",
      "    sample_time_ms: 3278.986\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1628179093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>RUNNING   </td><td>10.10.33.168:156935</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         306.566</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  198.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 108</td><td style=\"text-align: right;\">            198.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         300.002</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  198.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 123</td><td style=\"text-align: right;\">            198.84</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 198.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.14\n",
      "  episode_reward_min: 123.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1132\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4505503177642822\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015780692920088768\n",
      "          model: {}\n",
      "          policy_loss: -0.027095604687929153\n",
      "          total_loss: 505.6729736328125\n",
      "          vf_explained_var: 0.2777933180332184\n",
      "          vf_loss: 505.6930236816406\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.27999999999999\n",
      "    ram_util_percent: 69.49\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046243122002083296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05043925386604455\n",
      "    mean_inference_ms: 0.6650005220502347\n",
      "    mean_raw_obs_processing_ms: 0.06095572829686219\n",
      "  time_since_restore: 307.09423446655273\n",
      "  time_this_iter_s: 7.092556476593018\n",
      "  time_total_s: 307.09423446655273\n",
      "  timers:\n",
      "    learn_throughput: 1140.225\n",
      "    learn_time_ms: 3508.081\n",
      "    load_throughput: 3127335.359\n",
      "    load_time_ms: 1.279\n",
      "    sample_throughput: 1211.497\n",
      "    sample_time_ms: 3301.7\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1628179094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-20\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1171\n",
      "  experiment_id: 4e8ef5779c704c4096f8e8d908f89ac7\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4478537440299988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013967836275696754\n",
      "          model: {}\n",
      "          policy_loss: -0.008986329659819603\n",
      "          total_loss: 237.18568420410156\n",
      "          vf_explained_var: 0.6012701392173767\n",
      "          vf_loss: 237.18521118164062\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.56\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156935\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04752249870894604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0507379918368207\n",
      "    mean_inference_ms: 0.6613572987644701\n",
      "    mean_raw_obs_processing_ms: 0.060399546104871915\n",
      "  time_since_restore: 313.52448177337646\n",
      "  time_this_iter_s: 6.958286285400391\n",
      "  time_total_s: 313.52448177337646\n",
      "  timers:\n",
      "    learn_throughput: 1138.059\n",
      "    learn_time_ms: 3514.756\n",
      "    load_throughput: 3521517.988\n",
      "    load_time_ms: 1.136\n",
      "    sample_throughput: 1215.031\n",
      "    sample_time_ms: 3292.096\n",
      "    update_time_ms: 1.743\n",
      "  timestamp: 1628179100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 332a6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         307.094</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  198.14</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 123</td><td style=\"text-align: right;\">            198.14</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 197.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.89\n",
      "  episode_reward_min: 123.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1152\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.44142693281173706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016537733376026154\n",
      "          model: {}\n",
      "          policy_loss: -0.027904724702239037\n",
      "          total_loss: 360.8098449707031\n",
      "          vf_explained_var: 0.35866525769233704\n",
      "          vf_loss: 360.8302917480469\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.19999999999999\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046224980326295476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050418310974001714\n",
      "    mean_inference_ms: 0.6646672419861315\n",
      "    mean_raw_obs_processing_ms: 0.06092008181301819\n",
      "  time_since_restore: 314.0374264717102\n",
      "  time_this_iter_s: 6.943192005157471\n",
      "  time_total_s: 314.0374264717102\n",
      "  timers:\n",
      "    learn_throughput: 1133.534\n",
      "    learn_time_ms: 3528.786\n",
      "    load_throughput: 3126461.183\n",
      "    load_time_ms: 1.279\n",
      "    sample_throughput: 1207.43\n",
      "    sample_time_ms: 3312.822\n",
      "    update_time_ms: 1.61\n",
      "  timestamp: 1628179100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 332a6_00001\n",
      "  \n",
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-26\n",
      "  done: false\n",
      "  episode_len_mean: 197.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.29\n",
      "  episode_reward_min: 123.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1172\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4311208426952362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014920400455594063\n",
      "          model: {}\n",
      "          policy_loss: -0.02859341725707054\n",
      "          total_loss: 233.7085723876953\n",
      "          vf_explained_var: 0.6557323932647705\n",
      "          vf_loss: 233.73045349121094\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.900000000000006\n",
      "    ram_util_percent: 67.72500000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0462084726225096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050399761767889274\n",
      "    mean_inference_ms: 0.6643709396643138\n",
      "    mean_raw_obs_processing_ms: 0.06089086796189877\n",
      "  time_since_restore: 319.43144512176514\n",
      "  time_this_iter_s: 5.394018650054932\n",
      "  time_total_s: 319.43144512176514\n",
      "  timers:\n",
      "    learn_throughput: 1167.209\n",
      "    learn_time_ms: 3426.98\n",
      "    load_throughput: 3224279.51\n",
      "    load_time_ms: 1.241\n",
      "    sample_throughput: 1206.895\n",
      "    sample_time_ms: 3314.291\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1628179106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.2/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         319.431</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">  197.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 123</td><td style=\"text-align: right;\">            197.29</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-32\n",
      "  done: false\n",
      "  episode_len_mean: 197.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.33\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1193\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4329637587070465\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019406717270612717\n",
      "          model: {}\n",
      "          policy_loss: -0.03418668732047081\n",
      "          total_loss: 469.3811950683594\n",
      "          vf_explained_var: 0.39013785123825073\n",
      "          vf_loss: 469.4066162109375\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.51111111111112\n",
      "    ram_util_percent: 67.72222222222223\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0461935626673365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05038344833449668\n",
      "    mean_inference_ms: 0.6640717485313716\n",
      "    mean_raw_obs_processing_ms: 0.0608646678514622\n",
      "  time_since_restore: 325.63008975982666\n",
      "  time_this_iter_s: 6.198644638061523\n",
      "  time_total_s: 325.63008975982666\n",
      "  timers:\n",
      "    learn_throughput: 1204.429\n",
      "    learn_time_ms: 3321.076\n",
      "    load_throughput: 3344873.4\n",
      "    load_time_ms: 1.196\n",
      "    sample_throughput: 1210.237\n",
      "    sample_time_ms: 3305.138\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1628179112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 CPU_group_0_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_5cb7d6fa5c04f93cd090c82b2f1bfdbe, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         325.63 </td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  197.33</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            197.33</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 198.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.05\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1213\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4257086217403412\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014656580053269863\n",
      "          model: {}\n",
      "          policy_loss: -0.02996397204697132\n",
      "          total_loss: 357.79107666015625\n",
      "          vf_explained_var: 0.34916365146636963\n",
      "          vf_loss: 357.81451416015625\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.30000000000001\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04618655397059299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05037561490086933\n",
      "    mean_inference_ms: 0.6639588280928522\n",
      "    mean_raw_obs_processing_ms: 0.060848249401319346\n",
      "  time_since_restore: 331.8782112598419\n",
      "  time_this_iter_s: 6.248121500015259\n",
      "  time_total_s: 331.8782112598419\n",
      "  timers:\n",
      "    learn_throughput: 1211.87\n",
      "    learn_time_ms: 3300.685\n",
      "    load_throughput: 3287200.909\n",
      "    load_time_ms: 1.217\n",
      "    sample_throughput: 1207.838\n",
      "    sample_time_ms: 3311.703\n",
      "    update_time_ms: 1.521\n",
      "  timestamp: 1628179118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         331.878</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  198.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            198.05</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 197.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.2\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1233\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.42366787791252136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019695047289133072\n",
      "          model: {}\n",
      "          policy_loss: -0.03343779966235161\n",
      "          total_loss: 387.1997375488281\n",
      "          vf_explained_var: 0.3867698013782501\n",
      "          vf_loss: 387.224365234375\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.75000000000001\n",
      "    ram_util_percent: 67.775\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04616449888020057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05035079194204759\n",
      "    mean_inference_ms: 0.6635817894364561\n",
      "    mean_raw_obs_processing_ms: 0.06081084986507532\n",
      "  time_since_restore: 337.6033225059509\n",
      "  time_this_iter_s: 5.725111246109009\n",
      "  time_total_s: 337.6033225059509\n",
      "  timers:\n",
      "    learn_throughput: 1250.672\n",
      "    learn_time_ms: 3198.279\n",
      "    load_throughput: 3329341.165\n",
      "    load_time_ms: 1.201\n",
      "    sample_throughput: 1224.535\n",
      "    sample_time_ms: 3266.545\n",
      "    update_time_ms: 1.536\n",
      "  timestamp: 1628179124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.2/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         337.603</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">   197.2</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">             197.2</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 197.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.0\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1254\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4118739664554596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019230959936976433\n",
      "          model: {}\n",
      "          policy_loss: -0.034081004559993744\n",
      "          total_loss: 310.0793151855469\n",
      "          vf_explained_var: 0.5388343334197998\n",
      "          vf_loss: 310.104736328125\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.15\n",
      "    ram_util_percent: 67.7875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046133745958225836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05031687548187706\n",
      "    mean_inference_ms: 0.6630455522335182\n",
      "    mean_raw_obs_processing_ms: 0.06076369678261189\n",
      "  time_since_restore: 343.5993158817291\n",
      "  time_this_iter_s: 5.995993375778198\n",
      "  time_total_s: 343.5993158817291\n",
      "  timers:\n",
      "    learn_throughput: 1282.385\n",
      "    learn_time_ms: 3119.188\n",
      "    load_throughput: 3375561.547\n",
      "    load_time_ms: 1.185\n",
      "    sample_throughput: 1237.096\n",
      "    sample_time_ms: 3233.38\n",
      "    update_time_ms: 1.572\n",
      "  timestamp: 1628179130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.1/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         343.599</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">     197</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">               197</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 196.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.24\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1274\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4153776168823242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019101545214653015\n",
      "          model: {}\n",
      "          policy_loss: -0.02731156349182129\n",
      "          total_loss: 299.5924072265625\n",
      "          vf_explained_var: 0.5266053676605225\n",
      "          vf_loss: 299.61114501953125\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.225\n",
      "    ram_util_percent: 67.7875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046114774740066394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05029567851213309\n",
      "    mean_inference_ms: 0.6627141026682832\n",
      "    mean_raw_obs_processing_ms: 0.06073499302081241\n",
      "  time_since_restore: 348.96731066703796\n",
      "  time_this_iter_s: 5.367994785308838\n",
      "  time_total_s: 348.96731066703796\n",
      "  timers:\n",
      "    learn_throughput: 1357.799\n",
      "    learn_time_ms: 2945.943\n",
      "    load_throughput: 3357995.276\n",
      "    load_time_ms: 1.191\n",
      "    sample_throughput: 1228.838\n",
      "    sample_time_ms: 3255.108\n",
      "    update_time_ms: 1.591\n",
      "  timestamp: 1628179136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.2/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         348.967</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  196.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            196.24</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-02\n",
      "  done: false\n",
      "  episode_len_mean: 197.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.06\n",
      "  episode_reward_min: 116.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1294\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.420093297958374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018573272973299026\n",
      "          model: {}\n",
      "          policy_loss: -0.032940395176410675\n",
      "          total_loss: 362.9814147949219\n",
      "          vf_explained_var: 0.5598229169845581\n",
      "          vf_loss: 363.0060119628906\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.0888888888889\n",
      "    ram_util_percent: 67.87777777777778\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046096961266541586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050276241878311596\n",
      "    mean_inference_ms: 0.6624154579565591\n",
      "    mean_raw_obs_processing_ms: 0.06070715413639878\n",
      "  time_since_restore: 355.42476177215576\n",
      "  time_this_iter_s: 6.457451105117798\n",
      "  time_total_s: 355.42476177215576\n",
      "  timers:\n",
      "    learn_throughput: 1380.915\n",
      "    learn_time_ms: 2896.63\n",
      "    load_throughput: 3374339.501\n",
      "    load_time_ms: 1.185\n",
      "    sample_throughput: 1226.666\n",
      "    sample_time_ms: 3260.871\n",
      "    update_time_ms: 1.575\n",
      "  timestamp: 1628179142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.2/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         355.425</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  197.06</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 116</td><td style=\"text-align: right;\">            197.06</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 194.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.79\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1315\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41224345564842224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018032433465123177\n",
      "          model: {}\n",
      "          policy_loss: -0.03208855539560318\n",
      "          total_loss: 310.4964599609375\n",
      "          vf_explained_var: 0.5906192660331726\n",
      "          vf_loss: 310.5204772949219\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.81111111111112\n",
      "    ram_util_percent: 68.16666666666667\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04607175363765236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050246959884801205\n",
      "    mean_inference_ms: 0.6619269810962828\n",
      "    mean_raw_obs_processing_ms: 0.06066861213815427\n",
      "  time_since_restore: 361.47112584114075\n",
      "  time_this_iter_s: 6.046364068984985\n",
      "  time_total_s: 361.47112584114075\n",
      "  timers:\n",
      "    learn_throughput: 1386.359\n",
      "    learn_time_ms: 2885.256\n",
      "    load_throughput: 3418758.609\n",
      "    load_time_ms: 1.17\n",
      "    sample_throughput: 1229.295\n",
      "    sample_time_ms: 3253.897\n",
      "    update_time_ms: 1.586\n",
      "  timestamp: 1628179148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         361.471</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  194.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            194.79</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-14\n",
      "  done: false\n",
      "  episode_len_mean: 193.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.42\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1337\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41715651750564575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01810598559677601\n",
      "          model: {}\n",
      "          policy_loss: -0.03020014986395836\n",
      "          total_loss: 296.5931701660156\n",
      "          vf_explained_var: 0.5879465341567993\n",
      "          vf_loss: 296.61529541015625\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.512499999999996\n",
      "    ram_util_percent: 68.2\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04603625918193389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0502055923418985\n",
      "    mean_inference_ms: 0.6612658159287248\n",
      "    mean_raw_obs_processing_ms: 0.06061355846175442\n",
      "  time_since_restore: 366.91315054893494\n",
      "  time_this_iter_s: 5.4420247077941895\n",
      "  time_total_s: 366.91315054893494\n",
      "  timers:\n",
      "    learn_throughput: 1442.387\n",
      "    learn_time_ms: 2773.182\n",
      "    load_throughput: 3481328.021\n",
      "    load_time_ms: 1.149\n",
      "    sample_throughput: 1249.63\n",
      "    sample_time_ms: 3200.948\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1628179154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         366.913</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  193.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            193.42</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 193.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.24\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1357\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.42990419268608093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016981227323412895\n",
      "          model: {}\n",
      "          policy_loss: -0.024283934384584427\n",
      "          total_loss: 293.3227233886719\n",
      "          vf_explained_var: 0.5635008215904236\n",
      "          vf_loss: 293.3393859863281\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.85\n",
      "    ram_util_percent: 68.1875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04600399479589811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050169144547161204\n",
      "    mean_inference_ms: 0.6606480681871535\n",
      "    mean_raw_obs_processing_ms: 0.06056452941271317\n",
      "  time_since_restore: 372.72884011268616\n",
      "  time_this_iter_s: 5.815689563751221\n",
      "  time_total_s: 372.72884011268616\n",
      "  timers:\n",
      "    learn_throughput: 1489.838\n",
      "    learn_time_ms: 2684.856\n",
      "    load_throughput: 3532491.683\n",
      "    load_time_ms: 1.132\n",
      "    sample_throughput: 1259.224\n",
      "    sample_time_ms: 3176.561\n",
      "    update_time_ms: 1.559\n",
      "  timestamp: 1628179159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         372.729</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  193.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            193.24</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 193.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.36\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1378\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4224745035171509\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018949197605252266\n",
      "          model: {}\n",
      "          policy_loss: -0.03435221686959267\n",
      "          total_loss: 294.0758056640625\n",
      "          vf_explained_var: 0.5646730661392212\n",
      "          vf_loss: 294.1015930175781\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.7625\n",
      "    ram_util_percent: 68.2\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04596928378965853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0501308962284559\n",
      "    mean_inference_ms: 0.6599784739939729\n",
      "    mean_raw_obs_processing_ms: 0.06051277091480234\n",
      "  time_since_restore: 378.37958002090454\n",
      "  time_this_iter_s: 5.650739908218384\n",
      "  time_total_s: 378.37958002090454\n",
      "  timers:\n",
      "    learn_throughput: 1486.007\n",
      "    learn_time_ms: 2691.778\n",
      "    load_throughput: 3460789.637\n",
      "    load_time_ms: 1.156\n",
      "    sample_throughput: 1251.773\n",
      "    sample_time_ms: 3195.468\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1628179165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         378.38 </td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  193.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            193.36</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 193.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.01\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1398\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41338858008384705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014778202399611473\n",
      "          model: {}\n",
      "          policy_loss: -0.024954568594694138\n",
      "          total_loss: 309.35125732421875\n",
      "          vf_explained_var: 0.4585186243057251\n",
      "          vf_loss: 309.3695068359375\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.025\n",
      "    ram_util_percent: 68.375\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045915118138375996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.050070795634064504\n",
      "    mean_inference_ms: 0.6589765007247722\n",
      "    mean_raw_obs_processing_ms: 0.060431760980389874\n",
      "  time_since_restore: 383.8333327770233\n",
      "  time_this_iter_s: 5.453752756118774\n",
      "  time_total_s: 383.8333327770233\n",
      "  timers:\n",
      "    learn_throughput: 1500.433\n",
      "    learn_time_ms: 2665.898\n",
      "    load_throughput: 3469018.878\n",
      "    load_time_ms: 1.153\n",
      "    sample_throughput: 1271.107\n",
      "    sample_time_ms: 3146.863\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1628179171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         383.833</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  193.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            193.01</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 194.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.64\n",
      "  episode_reward_min: 90.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1418\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.42368966341018677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015973446890711784\n",
      "          model: {}\n",
      "          policy_loss: -0.027611687779426575\n",
      "          total_loss: 271.9852294921875\n",
      "          vf_explained_var: 0.5766222476959229\n",
      "          vf_loss: 272.0056457519531\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.05000000000001\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045856502381960054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05000752360034458\n",
      "    mean_inference_ms: 0.65791634999727\n",
      "    mean_raw_obs_processing_ms: 0.06034962551990836\n",
      "  time_since_restore: 389.6076261997223\n",
      "  time_this_iter_s: 5.774293422698975\n",
      "  time_total_s: 389.6076261997223\n",
      "  timers:\n",
      "    learn_throughput: 1502.818\n",
      "    learn_time_ms: 2661.666\n",
      "    load_throughput: 3482195.102\n",
      "    load_time_ms: 1.149\n",
      "    sample_throughput: 1288.804\n",
      "    sample_time_ms: 3103.652\n",
      "    update_time_ms: 1.539\n",
      "  timestamp: 1628179176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         389.608</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  194.64</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  90</td><td style=\"text-align: right;\">            194.64</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-42\n",
      "  done: false\n",
      "  episode_len_mean: 195.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.13\n",
      "  episode_reward_min: 97.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1439\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.43191787600517273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016810070723295212\n",
      "          model: {}\n",
      "          policy_loss: -0.02729855850338936\n",
      "          total_loss: 177.31219482421875\n",
      "          vf_explained_var: 0.7011333107948303\n",
      "          vf_loss: 177.3319091796875\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.6125\n",
      "    ram_util_percent: 68.3\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04581112791012821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04995943553336334\n",
      "    mean_inference_ms: 0.6570852536379607\n",
      "    mean_raw_obs_processing_ms: 0.06028952080933218\n",
      "  time_since_restore: 394.8476436138153\n",
      "  time_this_iter_s: 5.240017414093018\n",
      "  time_total_s: 394.8476436138153\n",
      "  timers:\n",
      "    learn_throughput: 1540.91\n",
      "    learn_time_ms: 2595.868\n",
      "    load_throughput: 3580896.44\n",
      "    load_time_ms: 1.117\n",
      "    sample_throughput: 1281.643\n",
      "    sample_time_ms: 3120.993\n",
      "    update_time_ms: 1.521\n",
      "  timestamp: 1628179182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         394.848</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  195.13</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  97</td><td style=\"text-align: right;\">            195.13</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-48\n",
      "  done: false\n",
      "  episode_len_mean: 194.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.42\n",
      "  episode_reward_min: 97.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4226169288158417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01898089610040188\n",
      "          model: {}\n",
      "          policy_loss: -0.030220475047826767\n",
      "          total_loss: 226.53839111328125\n",
      "          vf_explained_var: 0.6351476311683655\n",
      "          vf_loss: 226.56004333496094\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.76249999999999\n",
      "    ram_util_percent: 68.375\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045771341810924376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04991606116053882\n",
      "    mean_inference_ms: 0.6563443453877715\n",
      "    mean_raw_obs_processing_ms: 0.060234883325632115\n",
      "  time_since_restore: 400.7882990837097\n",
      "  time_this_iter_s: 5.940655469894409\n",
      "  time_total_s: 400.7882990837097\n",
      "  timers:\n",
      "    learn_throughput: 1546.368\n",
      "    learn_time_ms: 2586.706\n",
      "    load_throughput: 3608003.441\n",
      "    load_time_ms: 1.109\n",
      "    sample_throughput: 1280.125\n",
      "    sample_time_ms: 3124.694\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1628179188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         400.788</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  194.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  97</td><td style=\"text-align: right;\">            194.42</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 195.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.52\n",
      "  episode_reward_min: 97.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.42397332191467285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01694333367049694\n",
      "          model: {}\n",
      "          policy_loss: -0.03025299310684204\n",
      "          total_loss: 242.12823486328125\n",
      "          vf_explained_var: 0.5707805156707764\n",
      "          vf_loss: 242.15090942382812\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.43333333333334\n",
      "    ram_util_percent: 68.37777777777778\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04573723583130723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049877337986104935\n",
      "    mean_inference_ms: 0.6556838696601175\n",
      "    mean_raw_obs_processing_ms: 0.060187468823273305\n",
      "  time_since_restore: 406.63795280456543\n",
      "  time_this_iter_s: 5.849653720855713\n",
      "  time_total_s: 406.63795280456543\n",
      "  timers:\n",
      "    learn_throughput: 1517.584\n",
      "    learn_time_ms: 2635.768\n",
      "    load_throughput: 3602967.035\n",
      "    load_time_ms: 1.11\n",
      "    sample_throughput: 1280.491\n",
      "    sample_time_ms: 3123.801\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1628179193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.3/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         406.638</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  195.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  97</td><td style=\"text-align: right;\">            195.52</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_17-59-59\n",
      "  done: false\n",
      "  episode_len_mean: 195.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.42\n",
      "  episode_reward_min: 97.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41151705384254456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015579917468130589\n",
      "          model: {}\n",
      "          policy_loss: -0.028479265049099922\n",
      "          total_loss: 215.0211639404297\n",
      "          vf_explained_var: 0.6022013425827026\n",
      "          vf_loss: 215.0426483154297\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.10000000000001\n",
      "    ram_util_percent: 68.44285714285715\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04569636343298621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04983181291872801\n",
      "    mean_inference_ms: 0.6549313231525548\n",
      "    mean_raw_obs_processing_ms: 0.06013193604335577\n",
      "  time_since_restore: 411.9135880470276\n",
      "  time_this_iter_s: 5.275635242462158\n",
      "  time_total_s: 411.9135880470276\n",
      "  timers:\n",
      "    learn_throughput: 1546.018\n",
      "    learn_time_ms: 2587.292\n",
      "    load_throughput: 3677601.052\n",
      "    load_time_ms: 1.088\n",
      "    sample_throughput: 1309.698\n",
      "    sample_time_ms: 3054.139\n",
      "    update_time_ms: 1.484\n",
      "  timestamp: 1628179199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.4/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         411.914</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  195.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  97</td><td style=\"text-align: right;\">            195.42</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-05\n",
      "  done: false\n",
      "  episode_len_mean: 196.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.06\n",
      "  episode_reward_min: 97.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4160889685153961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021153289824724197\n",
      "          model: {}\n",
      "          policy_loss: -0.033865757286548615\n",
      "          total_loss: 185.3018341064453\n",
      "          vf_explained_var: 0.6368469595909119\n",
      "          vf_loss: 185.32614135742188\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.14444444444445\n",
      "    ram_util_percent: 68.52222222222221\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04565804392328888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04978905647755878\n",
      "    mean_inference_ms: 0.6542218241079493\n",
      "    mean_raw_obs_processing_ms: 0.060079149254605134\n",
      "  time_since_restore: 418.16654348373413\n",
      "  time_this_iter_s: 6.252955436706543\n",
      "  time_total_s: 418.16654348373413\n",
      "  timers:\n",
      "    learn_throughput: 1524.467\n",
      "    learn_time_ms: 2623.869\n",
      "    load_throughput: 3701700.24\n",
      "    load_time_ms: 1.081\n",
      "    sample_throughput: 1316.556\n",
      "    sample_time_ms: 3038.23\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1628179205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         418.167</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  196.06</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  97</td><td style=\"text-align: right;\">            196.06</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-11\n",
      "  done: false\n",
      "  episode_len_mean: 196.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.34\n",
      "  episode_reward_min: 113.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1541\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41742828488349915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013075286522507668\n",
      "          model: {}\n",
      "          policy_loss: -0.03137164190411568\n",
      "          total_loss: 291.5762023925781\n",
      "          vf_explained_var: 0.5629850625991821\n",
      "          vf_loss: 291.5987854003906\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.25555555555555\n",
      "    ram_util_percent: 68.88888888888889\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04562616373694679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04975298048779655\n",
      "    mean_inference_ms: 0.6536142755363447\n",
      "    mean_raw_obs_processing_ms: 0.060034071128833695\n",
      "  time_since_restore: 424.05407309532166\n",
      "  time_this_iter_s: 5.887529611587524\n",
      "  time_total_s: 424.05407309532166\n",
      "  timers:\n",
      "    learn_throughput: 1532.001\n",
      "    learn_time_ms: 2610.964\n",
      "    load_throughput: 3632375.509\n",
      "    load_time_ms: 1.101\n",
      "    sample_throughput: 1292.082\n",
      "    sample_time_ms: 3095.778\n",
      "    update_time_ms: 1.464\n",
      "  timestamp: 1628179211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         424.054</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  196.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 113</td><td style=\"text-align: right;\">            196.34</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 195.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.06\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1562\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4045521318912506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012929368764162064\n",
      "          model: {}\n",
      "          policy_loss: -0.027882788330316544\n",
      "          total_loss: 315.2381896972656\n",
      "          vf_explained_var: 0.5200291872024536\n",
      "          vf_loss: 315.25738525390625\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.6125\n",
      "    ram_util_percent: 68.975\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04558240765592128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04970392790374143\n",
      "    mean_inference_ms: 0.6528377091372761\n",
      "    mean_raw_obs_processing_ms: 0.05997185524174561\n",
      "  time_since_restore: 429.6711394786835\n",
      "  time_this_iter_s: 5.617066383361816\n",
      "  time_total_s: 429.6711394786835\n",
      "  timers:\n",
      "    learn_throughput: 1531.485\n",
      "    learn_time_ms: 2611.844\n",
      "    load_throughput: 3612820.535\n",
      "    load_time_ms: 1.107\n",
      "    sample_throughput: 1300.806\n",
      "    sample_time_ms: 3075.016\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1628179217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.5/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         429.671</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  195.06</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  70</td><td style=\"text-align: right;\">            195.06</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 192.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.9\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1583\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41019922494888306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011833840981125832\n",
      "          model: {}\n",
      "          policy_loss: -0.022873634472489357\n",
      "          total_loss: 314.8786315917969\n",
      "          vf_explained_var: 0.5303192138671875\n",
      "          vf_loss: 314.89349365234375\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.00909090909092\n",
      "    ram_util_percent: 69.52727272727273\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045583578041027153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04970227506008643\n",
      "    mean_inference_ms: 0.6529627954827292\n",
      "    mean_raw_obs_processing_ms: 0.05996823699227373\n",
      "  time_since_restore: 437.757018327713\n",
      "  time_this_iter_s: 8.085878849029541\n",
      "  time_total_s: 437.757018327713\n",
      "  timers:\n",
      "    learn_throughput: 1464.585\n",
      "    learn_time_ms: 2731.149\n",
      "    load_throughput: 3600647.28\n",
      "    load_time_ms: 1.111\n",
      "    sample_throughput: 1250.421\n",
      "    sample_time_ms: 3198.923\n",
      "    update_time_ms: 1.524\n",
      "  timestamp: 1628179225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         437.757</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">   192.9</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">             192.9</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 190.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.07\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1605\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4050407111644745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012986048124730587\n",
      "          model: {}\n",
      "          policy_loss: -0.02809065766632557\n",
      "          total_loss: 309.3748474121094\n",
      "          vf_explained_var: 0.5722286105155945\n",
      "          vf_loss: 309.3941955566406\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.425\n",
      "    ram_util_percent: 69.80000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04561683778362589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04973473666689682\n",
      "    mean_inference_ms: 0.6536246183817078\n",
      "    mean_raw_obs_processing_ms: 0.060008118462887984\n",
      "  time_since_restore: 443.4042794704437\n",
      "  time_this_iter_s: 5.647261142730713\n",
      "  time_total_s: 443.4042794704437\n",
      "  timers:\n",
      "    learn_throughput: 1488.792\n",
      "    learn_time_ms: 2686.742\n",
      "    load_throughput: 3556378.59\n",
      "    load_time_ms: 1.125\n",
      "    sample_throughput: 1225.995\n",
      "    sample_time_ms: 3262.657\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1628179230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         443.404</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  190.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            190.07</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 186.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.56\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1627\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.41493597626686096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012884089723229408\n",
      "          model: {}\n",
      "          policy_loss: -0.0256610456854105\n",
      "          total_loss: 255.9141082763672\n",
      "          vf_explained_var: 0.623573899269104\n",
      "          vf_loss: 255.93109130859375\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.27000000000001\n",
      "    ram_util_percent: 69.69\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04565598747601405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0497739896621413\n",
      "    mean_inference_ms: 0.6543737152742543\n",
      "    mean_raw_obs_processing_ms: 0.060052830487326646\n",
      "  time_since_restore: 449.91619539260864\n",
      "  time_this_iter_s: 6.511915922164917\n",
      "  time_total_s: 449.91619539260864\n",
      "  timers:\n",
      "    learn_throughput: 1460.368\n",
      "    learn_time_ms: 2739.036\n",
      "    load_throughput: 3524403.084\n",
      "    load_time_ms: 1.135\n",
      "    sample_throughput: 1217.991\n",
      "    sample_time_ms: 3284.097\n",
      "    update_time_ms: 1.566\n",
      "  timestamp: 1628179237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         449.916</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  186.56</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            186.56</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 185.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.04\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1648\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4130059480667114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015329480171203613\n",
      "          model: {}\n",
      "          policy_loss: -0.0315755158662796\n",
      "          total_loss: 296.76910400390625\n",
      "          vf_explained_var: 0.5468905568122864\n",
      "          vf_loss: 296.7903137207031\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.05000000000001\n",
      "    ram_util_percent: 69.4875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0456974181041377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049816410551291324\n",
      "    mean_inference_ms: 0.6551485581882415\n",
      "    mean_raw_obs_processing_ms: 0.06010034159565622\n",
      "  time_since_restore: 456.02355909347534\n",
      "  time_this_iter_s: 6.107363700866699\n",
      "  time_total_s: 456.02355909347534\n",
      "  timers:\n",
      "    learn_throughput: 1424.376\n",
      "    learn_time_ms: 2808.247\n",
      "    load_throughput: 3437531.451\n",
      "    load_time_ms: 1.164\n",
      "    sample_throughput: 1211.542\n",
      "    sample_time_ms: 3301.577\n",
      "    update_time_ms: 1.569\n",
      "  timestamp: 1628179243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         456.024</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  185.04</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            185.04</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 186.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.68\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1669\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4050362706184387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013222305104136467\n",
      "          model: {}\n",
      "          policy_loss: -0.023442843928933144\n",
      "          total_loss: 370.9500427246094\n",
      "          vf_explained_var: 0.4983457028865814\n",
      "          vf_loss: 370.9645690917969\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.7875\n",
      "    ram_util_percent: 69.4625\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04571755173030006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0498371906286232\n",
      "    mean_inference_ms: 0.6555152277437882\n",
      "    mean_raw_obs_processing_ms: 0.06012003798156774\n",
      "  time_since_restore: 461.5217046737671\n",
      "  time_this_iter_s: 5.498145580291748\n",
      "  time_total_s: 461.5217046737671\n",
      "  timers:\n",
      "    learn_throughput: 1424.975\n",
      "    learn_time_ms: 2807.066\n",
      "    load_throughput: 3428819.947\n",
      "    load_time_ms: 1.167\n",
      "    sample_throughput: 1227.581\n",
      "    sample_time_ms: 3258.441\n",
      "    update_time_ms: 1.582\n",
      "  timestamp: 1628179249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         461.522</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  186.68</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            186.68</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 185.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.45\n",
      "  episode_reward_min: 56.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1691\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.40613552927970886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013213295489549637\n",
      "          model: {}\n",
      "          policy_loss: -0.031231259927153587\n",
      "          total_loss: 341.5130310058594\n",
      "          vf_explained_var: 0.5202457904815674\n",
      "          vf_loss: 341.5352783203125\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.26666666666665\n",
      "    ram_util_percent: 69.43333333333334\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04570874816490349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04982888040622333\n",
      "    mean_inference_ms: 0.6553111698393966\n",
      "    mean_raw_obs_processing_ms: 0.06010484952746345\n",
      "  time_since_restore: 467.6434895992279\n",
      "  time_this_iter_s: 6.121784925460815\n",
      "  time_total_s: 467.6434895992279\n",
      "  timers:\n",
      "    learn_throughput: 1416.624\n",
      "    learn_time_ms: 2823.614\n",
      "    load_throughput: 3310094.9\n",
      "    load_time_ms: 1.208\n",
      "    sample_throughput: 1223.649\n",
      "    sample_time_ms: 3268.912\n",
      "    update_time_ms: 1.594\n",
      "  timestamp: 1628179255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         467.643</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  185.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  56</td><td style=\"text-align: right;\">            185.45</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-00\n",
      "  done: false\n",
      "  episode_len_mean: 189.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.12\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1711\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4062081277370453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011976910755038261\n",
      "          model: {}\n",
      "          policy_loss: -0.023504702374339104\n",
      "          total_loss: 282.2123718261719\n",
      "          vf_explained_var: 0.5510677099227905\n",
      "          vf_loss: 282.2278137207031\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.5625\n",
      "    ram_util_percent: 69.475\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04569940491068838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04982088576933902\n",
      "    mean_inference_ms: 0.655126046235713\n",
      "    mean_raw_obs_processing_ms: 0.06009073606869947\n",
      "  time_since_restore: 473.3855473995209\n",
      "  time_this_iter_s: 5.742057800292969\n",
      "  time_total_s: 473.3855473995209\n",
      "  timers:\n",
      "    learn_throughput: 1431.255\n",
      "    learn_time_ms: 2794.75\n",
      "    load_throughput: 3152130.766\n",
      "    load_time_ms: 1.269\n",
      "    sample_throughput: 1196.001\n",
      "    sample_time_ms: 3344.478\n",
      "    update_time_ms: 1.552\n",
      "  timestamp: 1628179260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         473.386</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  189.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  65</td><td style=\"text-align: right;\">            189.12</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 191.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.88\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1731\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.40035924315452576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014645910821855068\n",
      "          model: {}\n",
      "          policy_loss: -0.026821190491318703\n",
      "          total_loss: 191.00888061523438\n",
      "          vf_explained_var: 0.7007655501365662\n",
      "          vf_loss: 191.02584838867188\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.37777777777777\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045677790586101635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049799084629246176\n",
      "    mean_inference_ms: 0.6547359551737558\n",
      "    mean_raw_obs_processing_ms: 0.060061000245371886\n",
      "  time_since_restore: 479.25560235977173\n",
      "  time_this_iter_s: 5.8700549602508545\n",
      "  time_total_s: 479.25560235977173\n",
      "  timers:\n",
      "    learn_throughput: 1441.062\n",
      "    learn_time_ms: 2775.731\n",
      "    load_throughput: 3051845.6\n",
      "    load_time_ms: 1.311\n",
      "    sample_throughput: 1202.977\n",
      "    sample_time_ms: 3325.084\n",
      "    update_time_ms: 1.576\n",
      "  timestamp: 1628179266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         479.256</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  191.88</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  65</td><td style=\"text-align: right;\">            191.88</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 193.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.97\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1751\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4014213979244232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012170402333140373\n",
      "          model: {}\n",
      "          policy_loss: -0.022284550592303276\n",
      "          total_loss: 409.15533447265625\n",
      "          vf_explained_var: 0.36539894342422485\n",
      "          vf_loss: 409.16937255859375\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.02222222222223\n",
      "    ram_util_percent: 69.4\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045666297594007116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04978794777578626\n",
      "    mean_inference_ms: 0.6544933973997145\n",
      "    mean_raw_obs_processing_ms: 0.06004486922549123\n",
      "  time_since_restore: 485.617059469223\n",
      "  time_this_iter_s: 6.361457109451294\n",
      "  time_total_s: 485.617059469223\n",
      "  timers:\n",
      "    learn_throughput: 1425.723\n",
      "    learn_time_ms: 2805.594\n",
      "    load_throughput: 3128735.058\n",
      "    load_time_ms: 1.278\n",
      "    sample_throughput: 1196.708\n",
      "    sample_time_ms: 3342.504\n",
      "    update_time_ms: 1.619\n",
      "  timestamp: 1628179273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         485.617</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">  193.97</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  79</td><td style=\"text-align: right;\">            193.97</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 194.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.94\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1772\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3845668435096741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013039869256317616\n",
      "          model: {}\n",
      "          policy_loss: -0.03081037662923336\n",
      "          total_loss: 407.6966247558594\n",
      "          vf_explained_var: 0.42912596464157104\n",
      "          vf_loss: 407.7186584472656\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.375\n",
      "    ram_util_percent: 69.42500000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04567070284953813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04979369401607861\n",
      "    mean_inference_ms: 0.6544871329157654\n",
      "    mean_raw_obs_processing_ms: 0.06005030663319411\n",
      "  time_since_restore: 491.18908882141113\n",
      "  time_this_iter_s: 5.57202935218811\n",
      "  time_total_s: 491.18908882141113\n",
      "  timers:\n",
      "    learn_throughput: 1445.002\n",
      "    learn_time_ms: 2768.162\n",
      "    load_throughput: 3191283.573\n",
      "    load_time_ms: 1.253\n",
      "    sample_throughput: 1185.016\n",
      "    sample_time_ms: 3375.483\n",
      "    update_time_ms: 1.613\n",
      "  timestamp: 1628179278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         491.189</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  194.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            194.94</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 196.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.7\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1792\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.38910651206970215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013027211651206017\n",
      "          model: {}\n",
      "          policy_loss: -0.022979939356446266\n",
      "          total_loss: 487.1108093261719\n",
      "          vf_explained_var: 0.36539092659950256\n",
      "          vf_loss: 487.1249694824219\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.55555555555556\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04567341060487694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04979759817808191\n",
      "    mean_inference_ms: 0.6544680405008829\n",
      "    mean_raw_obs_processing_ms: 0.060052475126388505\n",
      "  time_since_restore: 497.5082573890686\n",
      "  time_this_iter_s: 6.319168567657471\n",
      "  time_total_s: 497.5082573890686\n",
      "  timers:\n",
      "    learn_throughput: 1478.152\n",
      "    learn_time_ms: 2706.082\n",
      "    load_throughput: 3186858.391\n",
      "    load_time_ms: 1.255\n",
      "    sample_throughput: 1226.603\n",
      "    sample_time_ms: 3261.038\n",
      "    update_time_ms: 1.589\n",
      "  timestamp: 1628179285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         497.508</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">   196.7</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">             196.7</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 196.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.08\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1813\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3926548361778259\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011718779802322388\n",
      "          model: {}\n",
      "          policy_loss: -0.021902337670326233\n",
      "          total_loss: 476.8957214355469\n",
      "          vf_explained_var: 0.3998912572860718\n",
      "          vf_loss: 476.9097595214844\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.74444444444445\n",
      "    ram_util_percent: 69.41111111111111\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045671595082223015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049796168263251755\n",
      "    mean_inference_ms: 0.6543513152554914\n",
      "    mean_raw_obs_processing_ms: 0.0600496008202181\n",
      "  time_since_restore: 503.81137919425964\n",
      "  time_this_iter_s: 6.30312180519104\n",
      "  time_total_s: 503.81137919425964\n",
      "  timers:\n",
      "    learn_throughput: 1430.308\n",
      "    learn_time_ms: 2796.6\n",
      "    load_throughput: 3200781.441\n",
      "    load_time_ms: 1.25\n",
      "    sample_throughput: 1236.012\n",
      "    sample_time_ms: 3236.215\n",
      "    update_time_ms: 1.577\n",
      "  timestamp: 1628179291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         503.811</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  196.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            196.08</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 195.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.16\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1834\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3827652037143707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012984300032258034\n",
      "          model: {}\n",
      "          policy_loss: -0.027918986976146698\n",
      "          total_loss: 395.299072265625\n",
      "          vf_explained_var: 0.4118335247039795\n",
      "          vf_loss: 395.3182373046875\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.787499999999994\n",
      "    ram_util_percent: 69.475\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04566413017468692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04978978156985169\n",
      "    mean_inference_ms: 0.6541579985244718\n",
      "    mean_raw_obs_processing_ms: 0.06003858030920565\n",
      "  time_since_restore: 509.86062359809875\n",
      "  time_this_iter_s: 6.049244403839111\n",
      "  time_total_s: 509.86062359809875\n",
      "  timers:\n",
      "    learn_throughput: 1429.513\n",
      "    learn_time_ms: 2798.156\n",
      "    load_throughput: 3188978.521\n",
      "    load_time_ms: 1.254\n",
      "    sample_throughput: 1254.536\n",
      "    sample_time_ms: 3188.43\n",
      "    update_time_ms: 1.542\n",
      "  timestamp: 1628179297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         509.861</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  195.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            195.16</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 195.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.63\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1854\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.38380783796310425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014294006861746311\n",
      "          model: {}\n",
      "          policy_loss: -0.024271387606859207\n",
      "          total_loss: 373.2356262207031\n",
      "          vf_explained_var: 0.42107656598091125\n",
      "          vf_loss: 373.250244140625\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.87\n",
      "    ram_util_percent: 69.49000000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045657244938068296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0497842309544251\n",
      "    mean_inference_ms: 0.6539961594954349\n",
      "    mean_raw_obs_processing_ms: 0.06002815319012706\n",
      "  time_since_restore: 516.529764175415\n",
      "  time_this_iter_s: 6.669140577316284\n",
      "  time_total_s: 516.529764175415\n",
      "  timers:\n",
      "    learn_throughput: 1411.401\n",
      "    learn_time_ms: 2834.064\n",
      "    load_throughput: 3158837.174\n",
      "    load_time_ms: 1.266\n",
      "    sample_throughput: 1246.639\n",
      "    sample_time_ms: 3208.629\n",
      "    update_time_ms: 1.544\n",
      "  timestamp: 1628179304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         516.53 </td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  195.63</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            195.63</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 195.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.52\n",
      "  episode_reward_min: 59.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1874\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3828762471675873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014033171348273754\n",
      "          model: {}\n",
      "          policy_loss: -0.02921140566468239\n",
      "          total_loss: 371.4660339355469\n",
      "          vf_explained_var: 0.41017091274261475\n",
      "          vf_loss: 371.4858093261719\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.449999999999996\n",
      "    ram_util_percent: 69.625\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04564904761073148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0497773528479688\n",
      "    mean_inference_ms: 0.6538324935526119\n",
      "    mean_raw_obs_processing_ms: 0.060015283409922654\n",
      "  time_since_restore: 522.0666143894196\n",
      "  time_this_iter_s: 5.536850214004517\n",
      "  time_total_s: 522.0666143894196\n",
      "  timers:\n",
      "    learn_throughput: 1432.262\n",
      "    learn_time_ms: 2792.785\n",
      "    load_throughput: 3203776.424\n",
      "    load_time_ms: 1.249\n",
      "    sample_throughput: 1229.312\n",
      "    sample_time_ms: 3253.853\n",
      "    update_time_ms: 1.547\n",
      "  timestamp: 1628179309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         522.067</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  195.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  59</td><td style=\"text-align: right;\">            195.52</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 195.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.54\n",
      "  episode_reward_min: 59.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1894\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3811798691749573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011637048795819283\n",
      "          model: {}\n",
      "          policy_loss: -0.021971547976136208\n",
      "          total_loss: 394.9667053222656\n",
      "          vf_explained_var: 0.28018298745155334\n",
      "          vf_loss: 394.9808349609375\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.975\n",
      "    ram_util_percent: 69.57499999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045642536682259836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0497719659337692\n",
      "    mean_inference_ms: 0.6536820112574209\n",
      "    mean_raw_obs_processing_ms: 0.06000483187363748\n",
      "  time_since_restore: 528.0305647850037\n",
      "  time_this_iter_s: 5.9639503955841064\n",
      "  time_total_s: 528.0305647850037\n",
      "  timers:\n",
      "    learn_throughput: 1440.29\n",
      "    learn_time_ms: 2777.218\n",
      "    load_throughput: 3313363.484\n",
      "    load_time_ms: 1.207\n",
      "    sample_throughput: 1229.336\n",
      "    sample_time_ms: 3253.788\n",
      "    update_time_ms: 1.562\n",
      "  timestamp: 1628179315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         528.031</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  195.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  59</td><td style=\"text-align: right;\">            195.54</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 196.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.5\n",
      "  episode_reward_min: 59.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1914\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3823666274547577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011337587609887123\n",
      "          model: {}\n",
      "          policy_loss: -0.019418273121118546\n",
      "          total_loss: 416.4377136230469\n",
      "          vf_explained_var: 0.23032282292842865\n",
      "          vf_loss: 416.44952392578125\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.47777777777779\n",
      "    ram_util_percent: 69.52222222222223\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045636312688397886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049766725134269656\n",
      "    mean_inference_ms: 0.6535317957697575\n",
      "    mean_raw_obs_processing_ms: 0.05999371194362916\n",
      "  time_since_restore: 533.9437901973724\n",
      "  time_this_iter_s: 5.913225412368774\n",
      "  time_total_s: 533.9437901973724\n",
      "  timers:\n",
      "    learn_throughput: 1420.232\n",
      "    learn_time_ms: 2816.442\n",
      "    load_throughput: 3426858.94\n",
      "    load_time_ms: 1.167\n",
      "    sample_throughput: 1237.799\n",
      "    sample_time_ms: 3231.541\n",
      "    update_time_ms: 1.609\n",
      "  timestamp: 1628179321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         533.944</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">   196.5</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  59</td><td style=\"text-align: right;\">             196.5</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-07\n",
      "  done: false\n",
      "  episode_len_mean: 196.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.98\n",
      "  episode_reward_min: 59.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1935\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3803980052471161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01320729311555624\n",
      "          model: {}\n",
      "          policy_loss: -0.0293984804302454\n",
      "          total_loss: 460.9645080566406\n",
      "          vf_explained_var: 0.33173295855522156\n",
      "          vf_loss: 460.9850158691406\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8625\n",
      "    ram_util_percent: 69.5625\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04563308321989188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04976351291230831\n",
      "    mean_inference_ms: 0.6534090887848144\n",
      "    mean_raw_obs_processing_ms: 0.05998450254266025\n",
      "  time_since_restore: 539.7138884067535\n",
      "  time_this_iter_s: 5.7700982093811035\n",
      "  time_total_s: 539.7138884067535\n",
      "  timers:\n",
      "    learn_throughput: 1423.72\n",
      "    learn_time_ms: 2809.541\n",
      "    load_throughput: 3444447.729\n",
      "    load_time_ms: 1.161\n",
      "    sample_throughput: 1238.967\n",
      "    sample_time_ms: 3228.497\n",
      "    update_time_ms: 1.594\n",
      "  timestamp: 1628179327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         539.714</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  196.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  59</td><td style=\"text-align: right;\">            196.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 196.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.98\n",
      "  episode_reward_min: 59.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1955\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3821806311607361\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013513820245862007\n",
      "          model: {}\n",
      "          policy_loss: -0.029129348695278168\n",
      "          total_loss: 742.463623046875\n",
      "          vf_explained_var: 0.4129478931427002\n",
      "          vf_loss: 742.4837036132812\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64444444444445\n",
      "    ram_util_percent: 69.6\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045625078869297016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04975480989837422\n",
      "    mean_inference_ms: 0.6532007173604054\n",
      "    mean_raw_obs_processing_ms: 0.05996988223832982\n",
      "  time_since_restore: 545.7721345424652\n",
      "  time_this_iter_s: 6.05824613571167\n",
      "  time_total_s: 545.7721345424652\n",
      "  timers:\n",
      "    learn_throughput: 1430.572\n",
      "    learn_time_ms: 2796.085\n",
      "    load_throughput: 3467154.932\n",
      "    load_time_ms: 1.154\n",
      "    sample_throughput: 1245.458\n",
      "    sample_time_ms: 3211.671\n",
      "    update_time_ms: 1.583\n",
      "  timestamp: 1628179333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         545.772</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">  196.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  59</td><td style=\"text-align: right;\">            196.98</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 198.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.84\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1975\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3871819078922272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012799806892871857\n",
      "          model: {}\n",
      "          policy_loss: -0.024639738723635674\n",
      "          total_loss: 532.048583984375\n",
      "          vf_explained_var: 0.36509087681770325\n",
      "          vf_loss: 532.0646362304688\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.837500000000006\n",
      "    ram_util_percent: 69.6\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0456201228572944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049748772136593324\n",
      "    mean_inference_ms: 0.6530221748506222\n",
      "    mean_raw_obs_processing_ms: 0.05996044397706998\n",
      "  time_since_restore: 551.2964823246002\n",
      "  time_this_iter_s: 5.52434778213501\n",
      "  time_total_s: 551.2964823246002\n",
      "  timers:\n",
      "    learn_throughput: 1435.368\n",
      "    learn_time_ms: 2786.741\n",
      "    load_throughput: 3368648.301\n",
      "    load_time_ms: 1.187\n",
      "    sample_throughput: 1243.724\n",
      "    sample_time_ms: 3216.148\n",
      "    update_time_ms: 1.596\n",
      "  timestamp: 1628179339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         551.296</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  198.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            198.84</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 198.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.49\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1995\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.37104225158691406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011458917520940304\n",
      "          model: {}\n",
      "          policy_loss: -0.020639728754758835\n",
      "          total_loss: 489.6668395996094\n",
      "          vf_explained_var: 0.38636961579322815\n",
      "          vf_loss: 489.6797790527344\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.025\n",
      "    ram_util_percent: 69.58749999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04561386709619967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04974149101489587\n",
      "    mean_inference_ms: 0.6528340224006952\n",
      "    mean_raw_obs_processing_ms: 0.05994881698017441\n",
      "  time_since_restore: 557.2636966705322\n",
      "  time_this_iter_s: 5.967214345932007\n",
      "  time_total_s: 557.2636966705322\n",
      "  timers:\n",
      "    learn_throughput: 1453.144\n",
      "    learn_time_ms: 2752.651\n",
      "    load_throughput: 3372440.299\n",
      "    load_time_ms: 1.186\n",
      "    sample_throughput: 1244.174\n",
      "    sample_time_ms: 3214.984\n",
      "    update_time_ms: 1.599\n",
      "  timestamp: 1628179345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.7/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         557.264</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">  198.49</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            198.49</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 196.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.67\n",
      "  episode_reward_min: 84.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2016\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.37344682216644287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013202335685491562\n",
      "          model: {}\n",
      "          policy_loss: -0.0248835738748312\n",
      "          total_loss: 351.51593017578125\n",
      "          vf_explained_var: 0.5383337736129761\n",
      "          vf_loss: 351.5318603515625\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66666666666667\n",
      "    ram_util_percent: 69.53333333333333\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04560961224231375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049736474742008\n",
      "    mean_inference_ms: 0.652680275036009\n",
      "    mean_raw_obs_processing_ms: 0.05994065526074276\n",
      "  time_since_restore: 563.1819734573364\n",
      "  time_this_iter_s: 5.918276786804199\n",
      "  time_total_s: 563.1819734573364\n",
      "  timers:\n",
      "    learn_throughput: 1475.136\n",
      "    learn_time_ms: 2711.614\n",
      "    load_throughput: 3392009.058\n",
      "    load_time_ms: 1.179\n",
      "    sample_throughput: 1243.23\n",
      "    sample_time_ms: 3217.425\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1628179351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         563.182</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  196.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  84</td><td style=\"text-align: right;\">            196.67</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 197.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.34\n",
      "  episode_reward_min: 132.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2036\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3735901117324829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014144321903586388\n",
      "          model: {}\n",
      "          policy_loss: -0.028622640296816826\n",
      "          total_loss: 346.98797607421875\n",
      "          vf_explained_var: 0.555620551109314\n",
      "          vf_loss: 347.00701904296875\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.39999999999999\n",
      "    ram_util_percent: 69.85714285714285\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04559942328413814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0497253310876987\n",
      "    mean_inference_ms: 0.6524507181980573\n",
      "    mean_raw_obs_processing_ms: 0.05992546102389604\n",
      "  time_since_restore: 568.4291622638702\n",
      "  time_this_iter_s: 5.2471888065338135\n",
      "  time_total_s: 568.4291622638702\n",
      "  timers:\n",
      "    learn_throughput: 1514.462\n",
      "    learn_time_ms: 2641.202\n",
      "    load_throughput: 3265574.587\n",
      "    load_time_ms: 1.225\n",
      "    sample_throughput: 1247.026\n",
      "    sample_time_ms: 3207.632\n",
      "    update_time_ms: 1.604\n",
      "  timestamp: 1628179356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         568.429</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">  197.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 132</td><td style=\"text-align: right;\">            197.34</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 195.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.48\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2057\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.37489011883735657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01384010724723339\n",
      "          model: {}\n",
      "          policy_loss: -0.029799245297908783\n",
      "          total_loss: 498.4578857421875\n",
      "          vf_explained_var: 0.32493647933006287\n",
      "          vf_loss: 498.4783935546875\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.3777777777778\n",
      "    ram_util_percent: 69.83333333333333\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04558527544176126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049710118890263055\n",
      "    mean_inference_ms: 0.6521726368471762\n",
      "    mean_raw_obs_processing_ms: 0.059905987254650316\n",
      "  time_since_restore: 574.6824905872345\n",
      "  time_this_iter_s: 6.253328323364258\n",
      "  time_total_s: 574.6824905872345\n",
      "  timers:\n",
      "    learn_throughput: 1523.11\n",
      "    learn_time_ms: 2626.205\n",
      "    load_throughput: 3306311.412\n",
      "    load_time_ms: 1.21\n",
      "    sample_throughput: 1257.432\n",
      "    sample_time_ms: 3181.086\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1628179362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         574.682</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">  195.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  79</td><td style=\"text-align: right;\">            195.48</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 193.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.74\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2078\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.382097065448761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01263633742928505\n",
      "          model: {}\n",
      "          policy_loss: -0.0272548608481884\n",
      "          total_loss: 551.0740356445312\n",
      "          vf_explained_var: 0.42587462067604065\n",
      "          vf_loss: 551.0926513671875\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.0375\n",
      "    ram_util_percent: 69.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04557079648700658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04969424032559203\n",
      "    mean_inference_ms: 0.6518929890308711\n",
      "    mean_raw_obs_processing_ms: 0.05988556040997735\n",
      "  time_since_restore: 580.4323446750641\n",
      "  time_this_iter_s: 5.74985408782959\n",
      "  time_total_s: 580.4323446750641\n",
      "  timers:\n",
      "    learn_throughput: 1513.321\n",
      "    learn_time_ms: 2643.194\n",
      "    load_throughput: 3238469.675\n",
      "    load_time_ms: 1.235\n",
      "    sample_throughput: 1255.705\n",
      "    sample_time_ms: 3185.462\n",
      "    update_time_ms: 1.577\n",
      "  timestamp: 1628179368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         580.432</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">  193.74</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  79</td><td style=\"text-align: right;\">            193.74</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-02-54\n",
      "  done: false\n",
      "  episode_len_mean: 192.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.68\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2099\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.38636547327041626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012077879160642624\n",
      "          model: {}\n",
      "          policy_loss: -0.025688640773296356\n",
      "          total_loss: 595.2634887695312\n",
      "          vf_explained_var: 0.3094271123409271\n",
      "          vf_loss: 595.2810668945312\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.5111111111111\n",
      "    ram_util_percent: 69.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04555019542913749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0496713354525318\n",
      "    mean_inference_ms: 0.6515050617328229\n",
      "    mean_raw_obs_processing_ms: 0.059858597410077555\n",
      "  time_since_restore: 586.5208115577698\n",
      "  time_this_iter_s: 6.0884668827056885\n",
      "  time_total_s: 586.5208115577698\n",
      "  timers:\n",
      "    learn_throughput: 1491.276\n",
      "    learn_time_ms: 2682.268\n",
      "    load_throughput: 3255183.547\n",
      "    load_time_ms: 1.229\n",
      "    sample_throughput: 1266.273\n",
      "    sample_time_ms: 3158.876\n",
      "    update_time_ms: 1.569\n",
      "  timestamp: 1628179374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         586.521</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  192.68</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td><td style=\"text-align: right;\">            192.68</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 193.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.94\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2119\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3837595582008362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011082426644861698\n",
      "          model: {}\n",
      "          policy_loss: -0.02437983825802803\n",
      "          total_loss: 388.4988098144531\n",
      "          vf_explained_var: 0.4380887746810913\n",
      "          vf_loss: 388.5157470703125\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.03333333333333\n",
      "    ram_util_percent: 69.82222222222221\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04553504440531082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04965401976115753\n",
      "    mean_inference_ms: 0.6512245124332773\n",
      "    mean_raw_obs_processing_ms: 0.05983880412084983\n",
      "  time_since_restore: 592.6590163707733\n",
      "  time_this_iter_s: 6.13820481300354\n",
      "  time_total_s: 592.6590163707733\n",
      "  timers:\n",
      "    learn_throughput: 1487.478\n",
      "    learn_time_ms: 2689.116\n",
      "    load_throughput: 3262272.692\n",
      "    load_time_ms: 1.226\n",
      "    sample_throughput: 1260.049\n",
      "    sample_time_ms: 3174.481\n",
      "    update_time_ms: 1.562\n",
      "  timestamp: 1628179380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         592.659</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">  193.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td><td style=\"text-align: right;\">            193.94</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 194.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.53\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2139\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3768491744995117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013423400931060314\n",
      "          model: {}\n",
      "          policy_loss: -0.024057649075984955\n",
      "          total_loss: 347.3879089355469\n",
      "          vf_explained_var: 0.49265819787979126\n",
      "          vf_loss: 347.4029846191406\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.362500000000004\n",
      "    ram_util_percent: 69.8125\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04552490845357898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04964210694469142\n",
      "    mean_inference_ms: 0.6510217378193931\n",
      "    mean_raw_obs_processing_ms: 0.05982599350741018\n",
      "  time_since_restore: 598.0094330310822\n",
      "  time_this_iter_s: 5.350416660308838\n",
      "  time_total_s: 598.0094330310822\n",
      "  timers:\n",
      "    learn_throughput: 1513.84\n",
      "    learn_time_ms: 2642.288\n",
      "    load_throughput: 3377736.259\n",
      "    load_time_ms: 1.184\n",
      "    sample_throughput: 1258.102\n",
      "    sample_time_ms: 3179.393\n",
      "    update_time_ms: 1.566\n",
      "  timestamp: 1628179386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         598.009</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  194.53</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td><td style=\"text-align: right;\">            194.53</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-11\n",
      "  done: false\n",
      "  episode_len_mean: 195.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.35\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3778355121612549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01151098869740963\n",
      "          model: {}\n",
      "          policy_loss: -0.021898292005062103\n",
      "          total_loss: 462.0832214355469\n",
      "          vf_explained_var: 0.29864004254341125\n",
      "          vf_loss: 462.0973205566406\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6375\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045509449933203786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04962485272056753\n",
      "    mean_inference_ms: 0.6507476061671356\n",
      "    mean_raw_obs_processing_ms: 0.059805993333283604\n",
      "  time_since_restore: 603.7822284698486\n",
      "  time_this_iter_s: 5.7727954387664795\n",
      "  time_total_s: 603.7822284698486\n",
      "  timers:\n",
      "    learn_throughput: 1517.78\n",
      "    learn_time_ms: 2635.428\n",
      "    load_throughput: 3388925.787\n",
      "    load_time_ms: 1.18\n",
      "    sample_throughput: 1266.742\n",
      "    sample_time_ms: 3157.707\n",
      "    update_time_ms: 1.569\n",
      "  timestamp: 1628179391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         603.782</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">  195.35</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td><td style=\"text-align: right;\">            195.35</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 196.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.06\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2180\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.367702454328537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01119557861238718\n",
      "          model: {}\n",
      "          policy_loss: -0.025101684033870697\n",
      "          total_loss: 313.301513671875\n",
      "          vf_explained_var: 0.46952590346336365\n",
      "          vf_loss: 313.3190612792969\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.1\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04549158514256225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04960578985934256\n",
      "    mean_inference_ms: 0.6504504686264228\n",
      "    mean_raw_obs_processing_ms: 0.05978314931645946\n",
      "  time_since_restore: 609.638694524765\n",
      "  time_this_iter_s: 5.856466054916382\n",
      "  time_total_s: 609.638694524765\n",
      "  timers:\n",
      "    learn_throughput: 1490.676\n",
      "    learn_time_ms: 2683.347\n",
      "    load_throughput: 3433099.92\n",
      "    load_time_ms: 1.165\n",
      "    sample_throughput: 1272.647\n",
      "    sample_time_ms: 3143.056\n",
      "    update_time_ms: 1.561\n",
      "  timestamp: 1628179397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         609.639</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">  196.06</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  63</td><td style=\"text-align: right;\">            196.06</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 195.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.86\n",
      "  episode_reward_min: 116.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2201\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3569049835205078\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012694666162133217\n",
      "          model: {}\n",
      "          policy_loss: -0.02541191689670086\n",
      "          total_loss: 351.5746154785156\n",
      "          vf_explained_var: 0.47841423749923706\n",
      "          vf_loss: 351.5914611816406\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.825\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0454623898739433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04957432722852386\n",
      "    mean_inference_ms: 0.6499673111728651\n",
      "    mean_raw_obs_processing_ms: 0.05974259663088132\n",
      "  time_since_restore: 614.9150953292847\n",
      "  time_this_iter_s: 5.276400804519653\n",
      "  time_total_s: 614.9150953292847\n",
      "  timers:\n",
      "    learn_throughput: 1495.503\n",
      "    learn_time_ms: 2674.685\n",
      "    load_throughput: 3398605.49\n",
      "    load_time_ms: 1.177\n",
      "    sample_throughput: 1297.577\n",
      "    sample_time_ms: 3082.668\n",
      "    update_time_ms: 1.554\n",
      "  timestamp: 1628179402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         614.915</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">  195.86</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 116</td><td style=\"text-align: right;\">            195.86</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-28\n",
      "  done: false\n",
      "  episode_len_mean: 195.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.1\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2222\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.35509711503982544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012356155551970005\n",
      "          model: {}\n",
      "          policy_loss: -0.028002548962831497\n",
      "          total_loss: 346.592529296875\n",
      "          vf_explained_var: 0.43579837679862976\n",
      "          vf_loss: 346.6121826171875\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0625\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04542935255339229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04953922988536242\n",
      "    mean_inference_ms: 0.6494089687395491\n",
      "    mean_raw_obs_processing_ms: 0.05969781077264858\n",
      "  time_since_restore: 620.6634519100189\n",
      "  time_this_iter_s: 5.748356580734253\n",
      "  time_total_s: 620.6634519100189\n",
      "  timers:\n",
      "    learn_throughput: 1498.644\n",
      "    learn_time_ms: 2669.079\n",
      "    load_throughput: 3398812.042\n",
      "    load_time_ms: 1.177\n",
      "    sample_throughput: 1302.381\n",
      "    sample_time_ms: 3071.297\n",
      "    update_time_ms: 1.553\n",
      "  timestamp: 1628179408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         620.663</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">   195.1</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">             195.1</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">   200  </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">             200  </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 194.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.11\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2242\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3610852360725403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013528716750442982\n",
      "          model: {}\n",
      "          policy_loss: -0.027964027598500252\n",
      "          total_loss: 391.24774169921875\n",
      "          vf_explained_var: 0.45320814847946167\n",
      "          vf_loss: 391.26654052734375\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.5125\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04540376202629101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04951207369010396\n",
      "    mean_inference_ms: 0.64897962242569\n",
      "    mean_raw_obs_processing_ms: 0.05966411392450554\n",
      "  time_since_restore: 625.9493238925934\n",
      "  time_this_iter_s: 5.285871982574463\n",
      "  time_total_s: 625.9493238925934\n",
      "  timers:\n",
      "    learn_throughput: 1522.474\n",
      "    learn_time_ms: 2627.302\n",
      "    load_throughput: 3544507.215\n",
      "    load_time_ms: 1.129\n",
      "    sample_throughput: 1283.357\n",
      "    sample_time_ms: 3116.825\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1628179414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         625.949</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  194.11</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            194.11</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 194.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.35\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2262\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3651285469532013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012637129053473473\n",
      "          model: {}\n",
      "          policy_loss: -0.02760150283575058\n",
      "          total_loss: 328.8589172363281\n",
      "          vf_explained_var: 0.5898259878158569\n",
      "          vf_loss: 328.8778991699219\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.67500000000001\n",
      "    ram_util_percent: 69.8875\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04538345970611854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049490175967748955\n",
      "    mean_inference_ms: 0.6486215424391397\n",
      "    mean_raw_obs_processing_ms: 0.05963679512983076\n",
      "  time_since_restore: 631.92413854599\n",
      "  time_this_iter_s: 5.9748146533966064\n",
      "  time_total_s: 631.92413854599\n",
      "  timers:\n",
      "    learn_throughput: 1541.059\n",
      "    learn_time_ms: 2595.617\n",
      "    load_throughput: 3555700.237\n",
      "    load_time_ms: 1.125\n",
      "    sample_throughput: 1281.78\n",
      "    sample_time_ms: 3120.659\n",
      "    update_time_ms: 1.551\n",
      "  timestamp: 1628179420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         631.924</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">  194.35</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            194.35</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-45\n",
      "  done: false\n",
      "  episode_len_mean: 192.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.79\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2284\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3562777638435364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014734353870153427\n",
      "          model: {}\n",
      "          policy_loss: -0.03269150108098984\n",
      "          total_loss: 353.5531005859375\n",
      "          vf_explained_var: 0.5147519111633301\n",
      "          vf_loss: 353.57586669921875\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.875\n",
      "    ram_util_percent: 69.8\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04536171297124497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04946707786847245\n",
      "    mean_inference_ms: 0.6482492783086093\n",
      "    mean_raw_obs_processing_ms: 0.05960889029757597\n",
      "  time_since_restore: 637.6583864688873\n",
      "  time_this_iter_s: 5.734247922897339\n",
      "  time_total_s: 637.6583864688873\n",
      "  timers:\n",
      "    learn_throughput: 1530.017\n",
      "    learn_time_ms: 2614.35\n",
      "    load_throughput: 3572280.635\n",
      "    load_time_ms: 1.12\n",
      "    sample_throughput: 1290.186\n",
      "    sample_time_ms: 3100.328\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1628179425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         637.658</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">  192.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            192.79</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 191.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.85\n",
      "  episode_reward_min: 64.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2305\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.36702266335487366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013364387676119804\n",
      "          model: {}\n",
      "          policy_loss: -0.030146503821015358\n",
      "          total_loss: 436.73760986328125\n",
      "          vf_explained_var: 0.5080179572105408\n",
      "          vf_loss: 436.75872802734375\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.8125\n",
      "    ram_util_percent: 69.8375\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04534221338161101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04944586666471367\n",
      "    mean_inference_ms: 0.6479116364756008\n",
      "    mean_raw_obs_processing_ms: 0.059582363747420125\n",
      "  time_since_restore: 642.9586775302887\n",
      "  time_this_iter_s: 5.300291061401367\n",
      "  time_total_s: 642.9586775302887\n",
      "  timers:\n",
      "    learn_throughput: 1560.411\n",
      "    learn_time_ms: 2563.427\n",
      "    load_throughput: 3618899.051\n",
      "    load_time_ms: 1.105\n",
      "    sample_throughput: 1301.874\n",
      "    sample_time_ms: 3072.495\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1628179431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         642.959</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">  191.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            191.85</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 194.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.01\n",
      "  episode_reward_min: 64.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2325\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.35764968395233154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01238748338073492\n",
      "          model: {}\n",
      "          policy_loss: -0.026236126199364662\n",
      "          total_loss: 351.7875061035156\n",
      "          vf_explained_var: 0.5061427354812622\n",
      "          vf_loss: 351.8053894042969\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.16\n",
      "    ram_util_percent: 69.83999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04534157017447435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04944573421468647\n",
      "    mean_inference_ms: 0.6479230001414336\n",
      "    mean_raw_obs_processing_ms: 0.0595801392305716\n",
      "  time_since_restore: 650.195478439331\n",
      "  time_this_iter_s: 7.236800909042358\n",
      "  time_total_s: 650.195478439331\n",
      "  timers:\n",
      "    learn_throughput: 1530.398\n",
      "    learn_time_ms: 2613.699\n",
      "    load_throughput: 3565296.555\n",
      "    load_time_ms: 1.122\n",
      "    sample_throughput: 1277.087\n",
      "    sample_time_ms: 3132.128\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1628179438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         650.195</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">  194.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            194.01</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-05\n",
      "  done: false\n",
      "  episode_len_mean: 193.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.54\n",
      "  episode_reward_min: 64.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2346\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.35993048548698425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014601487666368484\n",
      "          model: {}\n",
      "          policy_loss: -0.02727537229657173\n",
      "          total_loss: 313.8140563964844\n",
      "          vf_explained_var: 0.5500389933586121\n",
      "          vf_loss: 313.8314208984375\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36\n",
      "    ram_util_percent: 69.81999999999998\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0453475507159418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04945218901581699\n",
      "    mean_inference_ms: 0.648076924633973\n",
      "    mean_raw_obs_processing_ms: 0.05958679832161232\n",
      "  time_since_restore: 657.139285326004\n",
      "  time_this_iter_s: 6.943806886672974\n",
      "  time_total_s: 657.139285326004\n",
      "  timers:\n",
      "    learn_throughput: 1472.883\n",
      "    learn_time_ms: 2715.761\n",
      "    load_throughput: 3462861.153\n",
      "    load_time_ms: 1.155\n",
      "    sample_throughput: 1254.123\n",
      "    sample_time_ms: 3189.479\n",
      "    update_time_ms: 1.482\n",
      "  timestamp: 1628179445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         657.139</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  193.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            193.54</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 193.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.41\n",
      "  episode_reward_min: 64.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2366\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3695808947086334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012333186343312263\n",
      "          model: {}\n",
      "          policy_loss: -0.027377253398299217\n",
      "          total_loss: 382.0662841796875\n",
      "          vf_explained_var: 0.4022582173347473\n",
      "          vf_loss: 382.0853576660156\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.82\n",
      "    ram_util_percent: 69.89\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045353334707140036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049458510515614444\n",
      "    mean_inference_ms: 0.6482219370447382\n",
      "    mean_raw_obs_processing_ms: 0.05959198027754233\n",
      "  time_since_restore: 664.0987265110016\n",
      "  time_this_iter_s: 6.959441184997559\n",
      "  time_total_s: 664.0987265110016\n",
      "  timers:\n",
      "    learn_throughput: 1418.184\n",
      "    learn_time_ms: 2820.508\n",
      "    load_throughput: 3495326.153\n",
      "    load_time_ms: 1.144\n",
      "    sample_throughput: 1248.716\n",
      "    sample_time_ms: 3203.291\n",
      "    update_time_ms: 1.529\n",
      "  timestamp: 1628179452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         664.099</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">  193.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            193.41</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-20\n",
      "  done: false\n",
      "  episode_len_mean: 196.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.0\n",
      "  episode_reward_min: 64.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2386\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3714533746242523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013785102404654026\n",
      "          model: {}\n",
      "          policy_loss: -0.026647105813026428\n",
      "          total_loss: 310.72265625\n",
      "          vf_explained_var: 0.43057888746261597\n",
      "          vf_loss: 310.7400207519531\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.2\n",
      "    ram_util_percent: 69.89999999999999\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04536834728056542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04947350145763957\n",
      "    mean_inference_ms: 0.6485095902515531\n",
      "    mean_raw_obs_processing_ms: 0.05960645981728909\n",
      "  time_since_restore: 672.278332233429\n",
      "  time_this_iter_s: 8.179605722427368\n",
      "  time_total_s: 672.278332233429\n",
      "  timers:\n",
      "    learn_throughput: 1321.328\n",
      "    learn_time_ms: 3027.258\n",
      "    load_throughput: 3472249.679\n",
      "    load_time_ms: 1.152\n",
      "    sample_throughput: 1238.861\n",
      "    sample_time_ms: 3228.771\n",
      "    update_time_ms: 1.59\n",
      "  timestamp: 1628179460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         672.278</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">     196</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">               196</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 197.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.01\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2407\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.35475674271583557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013835608027875423\n",
      "          model: {}\n",
      "          policy_loss: -0.028649931773543358\n",
      "          total_loss: 349.0696105957031\n",
      "          vf_explained_var: 0.5599295496940613\n",
      "          vf_loss: 349.08892822265625\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.125\n",
      "    ram_util_percent: 69.91250000000001\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04539565200117944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04950235457639931\n",
      "    mean_inference_ms: 0.6490219972018733\n",
      "    mean_raw_obs_processing_ms: 0.059635831992999114\n",
      "  time_since_restore: 677.7903020381927\n",
      "  time_this_iter_s: 5.511969804763794\n",
      "  time_total_s: 677.7903020381927\n",
      "  timers:\n",
      "    learn_throughput: 1342.151\n",
      "    learn_time_ms: 2980.29\n",
      "    load_throughput: 3595015.0\n",
      "    load_time_ms: 1.113\n",
      "    sample_throughput: 1212.349\n",
      "    sample_time_ms: 3299.38\n",
      "    update_time_ms: 1.574\n",
      "  timestamp: 1628179466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.9/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         677.79 </td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">  197.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            197.01</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 197.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.01\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2427\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.37213364243507385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012581554241478443\n",
      "          model: {}\n",
      "          policy_loss: -0.024072594940662384\n",
      "          total_loss: 349.59686279296875\n",
      "          vf_explained_var: 0.44327160716056824\n",
      "          vf_loss: 349.6124267578125\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.92222222222223\n",
      "    ram_util_percent: 69.96666666666665\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045409712255651866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04951680016256219\n",
      "    mean_inference_ms: 0.6492682129507468\n",
      "    mean_raw_obs_processing_ms: 0.05964137726814195\n",
      "  time_since_restore: 683.9467968940735\n",
      "  time_this_iter_s: 6.156494855880737\n",
      "  time_total_s: 683.9467968940735\n",
      "  timers:\n",
      "    learn_throughput: 1332.782\n",
      "    learn_time_ms: 3001.241\n",
      "    load_throughput: 3576773.973\n",
      "    load_time_ms: 1.118\n",
      "    sample_throughput: 1205.11\n",
      "    sample_time_ms: 3319.198\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1628179472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         683.947</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">  197.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            197.01</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 198.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.15\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2447\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.36857321858406067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012331070378422737\n",
      "          model: {}\n",
      "          policy_loss: -0.02313370630145073\n",
      "          total_loss: 385.8861389160156\n",
      "          vf_explained_var: 0.4216752350330353\n",
      "          vf_loss: 385.90093994140625\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.13749999999999\n",
      "    ram_util_percent: 69.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045421891621160296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04953004657163452\n",
      "    mean_inference_ms: 0.6494481758365828\n",
      "    mean_raw_obs_processing_ms: 0.05963720446668723\n",
      "  time_since_restore: 690.1143047809601\n",
      "  time_this_iter_s: 6.167507886886597\n",
      "  time_total_s: 690.1143047809601\n",
      "  timers:\n",
      "    learn_throughput: 1301.147\n",
      "    learn_time_ms: 3074.21\n",
      "    load_throughput: 3612276.025\n",
      "    load_time_ms: 1.107\n",
      "    sample_throughput: 1199.57\n",
      "    sample_time_ms: 3334.528\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1628179478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         690.114</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">  198.15</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            198.15</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 198.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.46\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2467\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3787177503108978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014013472013175488\n",
      "          model: {}\n",
      "          policy_loss: -0.026225773617625237\n",
      "          total_loss: 399.9950256347656\n",
      "          vf_explained_var: 0.36959755420684814\n",
      "          vf_loss: 400.0118408203125\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.3111111111111\n",
      "    ram_util_percent: 69.95555555555556\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04542696486824721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049535663464897614\n",
      "    mean_inference_ms: 0.6495083170793446\n",
      "    mean_raw_obs_processing_ms: 0.05961737371627617\n",
      "  time_since_restore: 695.891993522644\n",
      "  time_this_iter_s: 5.77768874168396\n",
      "  time_total_s: 695.891993522644\n",
      "  timers:\n",
      "    learn_throughput: 1294.793\n",
      "    learn_time_ms: 3089.297\n",
      "    load_throughput: 3598253.335\n",
      "    load_time_ms: 1.112\n",
      "    sample_throughput: 1212.218\n",
      "    sample_time_ms: 3299.736\n",
      "    update_time_ms: 1.6\n",
      "  timestamp: 1628179484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.9/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         695.892</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">  198.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            198.46</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-50\n",
      "  done: false\n",
      "  episode_len_mean: 197.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.58\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2487\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.363986611366272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01118430681526661\n",
      "          model: {}\n",
      "          policy_loss: -0.023665951564908028\n",
      "          total_loss: 362.8332824707031\n",
      "          vf_explained_var: 0.41077038645744324\n",
      "          vf_loss: 362.8493957519531\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.0875\n",
      "    ram_util_percent: 69.9375\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04542857102788281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049538972261599465\n",
      "    mean_inference_ms: 0.6495088151967474\n",
      "    mean_raw_obs_processing_ms: 0.0595902855623661\n",
      "  time_since_restore: 701.9190449714661\n",
      "  time_this_iter_s: 6.0270514488220215\n",
      "  time_total_s: 701.9190449714661\n",
      "  timers:\n",
      "    learn_throughput: 1289.882\n",
      "    learn_time_ms: 3101.059\n",
      "    load_throughput: 3604128.034\n",
      "    load_time_ms: 1.11\n",
      "    sample_throughput: 1205.835\n",
      "    sample_time_ms: 3317.202\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1628179490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         701.919</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">  197.58</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            197.58</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-04-55\n",
      "  done: false\n",
      "  episode_len_mean: 199.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.12\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2507\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.36868149042129517\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014417246915400028\n",
      "          model: {}\n",
      "          policy_loss: -0.024215713143348694\n",
      "          total_loss: 345.9048156738281\n",
      "          vf_explained_var: 0.4456365704536438\n",
      "          vf_loss: 345.9193420410156\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.699999999999996\n",
      "    ram_util_percent: 69.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045429821310795475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0495421251574658\n",
      "    mean_inference_ms: 0.649484619582718\n",
      "    mean_raw_obs_processing_ms: 0.05956645280663121\n",
      "  time_since_restore: 707.512444972992\n",
      "  time_this_iter_s: 5.593400001525879\n",
      "  time_total_s: 707.512444972992\n",
      "  timers:\n",
      "    learn_throughput: 1301.196\n",
      "    learn_time_ms: 3074.096\n",
      "    load_throughput: 3534277.649\n",
      "    load_time_ms: 1.132\n",
      "    sample_throughput: 1185.7\n",
      "    sample_time_ms: 3373.535\n",
      "    update_time_ms: 1.597\n",
      "  timestamp: 1628179495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         707.512</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">  199.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            199.12</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-05-01\n",
      "  done: false\n",
      "  episode_len_mean: 199.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.12\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2527\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.36549803614616394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012148592621088028\n",
      "          model: {}\n",
      "          policy_loss: -0.023493217304348946\n",
      "          total_loss: 300.038818359375\n",
      "          vf_explained_var: 0.507102370262146\n",
      "          vf_loss: 300.0540771484375\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.97777777777779\n",
      "    ram_util_percent: 69.97777777777777\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045428481303302314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04954191728872409\n",
      "    mean_inference_ms: 0.6494218977469177\n",
      "    mean_raw_obs_processing_ms: 0.05954582944319892\n",
      "  time_since_restore: 713.4413917064667\n",
      "  time_this_iter_s: 5.9289467334747314\n",
      "  time_total_s: 713.4413917064667\n",
      "  timers:\n",
      "    learn_throughput: 1326.921\n",
      "    learn_time_ms: 3014.497\n",
      "    load_throughput: 3610021.948\n",
      "    load_time_ms: 1.108\n",
      "    sample_throughput: 1211.234\n",
      "    sample_time_ms: 3302.418\n",
      "    update_time_ms: 1.598\n",
      "  timestamp: 1628179501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         713.441</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">  199.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            199.12</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 199.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.12\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2547\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3555450439453125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011739378795027733\n",
      "          model: {}\n",
      "          policy_loss: -0.0191735178232193\n",
      "          total_loss: 308.2414855957031\n",
      "          vf_explained_var: 0.46304503083229065\n",
      "          vf_loss: 308.25274658203125\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.9125\n",
      "    ram_util_percent: 69.9\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04542362460133969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04953780942657723\n",
      "    mean_inference_ms: 0.6493026028869328\n",
      "    mean_raw_obs_processing_ms: 0.05952829235888633\n",
      "  time_since_restore: 719.3884043693542\n",
      "  time_this_iter_s: 5.947012662887573\n",
      "  time_total_s: 719.3884043693542\n",
      "  timers:\n",
      "    learn_throughput: 1357.345\n",
      "    learn_time_ms: 2946.93\n",
      "    load_throughput: 3657158.801\n",
      "    load_time_ms: 1.094\n",
      "    sample_throughput: 1223.151\n",
      "    sample_time_ms: 3270.243\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1628179507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         719.388</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">  199.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            199.12</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-05-13\n",
      "  done: false\n",
      "  episode_len_mean: 199.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.12\n",
      "  episode_reward_min: 112.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2567\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3525487780570984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01263553649187088\n",
      "          model: {}\n",
      "          policy_loss: -0.025240056216716766\n",
      "          total_loss: 286.8860778808594\n",
      "          vf_explained_var: 0.4467475414276123\n",
      "          vf_loss: 286.9028625488281\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.9375\n",
      "    ram_util_percent: 69.9375\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0454182095906736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.049533067902028056\n",
      "    mean_inference_ms: 0.6491861035242027\n",
      "    mean_raw_obs_processing_ms: 0.059516656133642806\n",
      "  time_since_restore: 724.8413763046265\n",
      "  time_this_iter_s: 5.452971935272217\n",
      "  time_total_s: 724.8413763046265\n",
      "  timers:\n",
      "    learn_throughput: 1414.291\n",
      "    learn_time_ms: 2828.273\n",
      "    load_throughput: 3556604.766\n",
      "    load_time_ms: 1.125\n",
      "    sample_throughput: 1235.18\n",
      "    sample_time_ms: 3238.394\n",
      "    update_time_ms: 1.561\n",
      "  timestamp: 1628179513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.9/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>RUNNING   </td><td>10.10.33.168:156937</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         724.841</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">  199.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 112</td><td style=\"text-align: right;\">            199.12</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_332a6_00001:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-05_18-05-19\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2587\n",
      "  experiment_id: 9c2fe91b66d04124acea9d143b60da96\n",
      "  hostname: Yan-ThinkBook\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.3423589766025543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016062701120972633\n",
      "          model: {}\n",
      "          policy_loss: -0.028343388810753822\n",
      "          total_loss: 215.68992614746094\n",
      "          vf_explained_var: 0.6008973121643066\n",
      "          vf_loss: 215.7074432373047\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 10.10.33.168\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.65555555555555\n",
      "    ram_util_percent: 69.94444444444444\n",
      "  pid: 156937\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045410378539863065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04952572220278339\n",
      "    mean_inference_ms: 0.6490351921876291\n",
      "    mean_raw_obs_processing_ms: 0.05950706815906411\n",
      "  time_since_restore: 730.7010128498077\n",
      "  time_this_iter_s: 5.859636545181274\n",
      "  time_total_s: 730.7010128498077\n",
      "  timers:\n",
      "    learn_throughput: 1526.162\n",
      "    learn_time_ms: 2620.953\n",
      "    load_throughput: 3445579.561\n",
      "    load_time_ms: 1.161\n",
      "    sample_throughput: 1244.637\n",
      "    sample_time_ms: 3213.788\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1628179519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 332a6_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         730.701</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 21.8/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/7.5 GiB heap, 0.0/3.75 GiB objects (0.0/1.0 accelerator_type:GTX, 0.0/1.0 CPU_group_1_c98924d5bf480338d5d0ce2fb232d47d, 0.0/2.0 CPU_group_c98924d5bf480338d5d0ce2fb232d47d, 0.0/1.0 CPU_group_0_c98924d5bf480338d5d0ce2fb232d47d)<br>Result logdir: /home/xiongyan/ray_results/PPO<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v0_332a6_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         313.524</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         730.701</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n<tr><td>PPO_CartPole-v0_332a6_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         101.068</td><td style=\"text-align: right;\"> 52000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-05 18:05:19,948\tINFO tune.py:550 -- Total run time: 745.88 seconds (743.34 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fea9dd09d68>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}