{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Soft Actor-Critic (SAC)\n",
    "\n",
    "# Hyperparameters optimization for reward function using Ray.Tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trained in reward_training.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "import gym_gyroscope_env\n",
    "import spinup\n",
    "import torch\n",
    "from functools import partial\n",
    "import argparse\n",
    "\n",
    "from custom_functions.custom_functions import env_fn\n",
    "from custom_functions.custom_functions import create_env\n",
    "from custom_functions.custom_functions import load_agent\n",
    "from custom_functions.custom_functions import test_agent\n",
    "from custom_functions.custom_functions import plot_test\n",
    "from custom_functions.custom_functions import evaluate_control\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_reward(config):\n",
    "    times_test=10\n",
    "    steps=1\n",
    "\n",
    "    # Env function\n",
    "    env_name = 'GyroscopeEnv-v1'   # GyroscopeRealEnv-v0， GyroscopeEnv-v1, GyroscopeIntegralEnv-v1\n",
    "\n",
    "    for step in range(steps):\n",
    "        simu_args = {\n",
    "            'dt': 0.05,\n",
    "            'ep_len': 100,\n",
    "            'seed': 2\n",
    "        }\n",
    "\n",
    "        reward_func = 'Normalized with bonus'\n",
    "        reward_args = {\n",
    "            'k': 1,\n",
    "            'qx2': 1,\n",
    "            'qx4': 1,\n",
    "            'pu1': 0,\n",
    "            'pu2': 0,\n",
    "            'bound': 0.05,\n",
    "            'bonus': 2\n",
    "        }\n",
    "\n",
    "        env_fn_ = partial(env_fn, env_name, simu_args = simu_args, reward_func = reward_func, reward_args = reward_args)\n",
    "        print(env_fn_)\n",
    "\n",
    "        # Baseline 0 training\n",
    "        spinup.sac_pytorch(env_fn_,\n",
    "                           ac_kwargs= dict(hidden_sizes=[128,32], activation=torch.nn.ReLU),\n",
    "                           seed=0,\n",
    "                           steps_per_epoch=1500,\n",
    "                           epochs=500,\n",
    "                           replay_size=1000000,\n",
    "                           gamma=config[\"gamma\"],\n",
    "                           polyak=config[\"polyak\"],\n",
    "                           lr=config[\"lr\"],\n",
    "                           alpha=config[\"alpha\"],   # Entropy regularization coefficient. (Equivalent to inverse of reward scale in the original SAC paper.\n",
    "                           batch_size=config[\"batch_size\"],\n",
    "                           start_steps=10000,\n",
    "                           update_after=1000,\n",
    "                           update_every=50,\n",
    "                           num_test_episodes=10,\n",
    "                           max_ep_len=100,\n",
    "                           logger_kwargs=dict(output_dir='sac_reward_opt', exp_name='sac_reward_opt')\n",
    "                           )\n",
    "\n",
    "        # Test paramaters\n",
    "        init_state = np.array([0,0,0,0,45/180*np.pi,-60/180*np.pi,200/60*2*np.pi])\n",
    "        env = create_env(env_name,state=init_state)\n",
    "        print(env)\n",
    "\n",
    "        agent_paths = ['sac_reward_opt']\n",
    "        agent = load_agent(agent_paths[0])\n",
    "        t_end = times_test   # 测试步数\n",
    "\n",
    "        score, state_record, obs_record, action_record, reward_record = test_agent(env,agent,t_end)\n",
    "        plot_test(state_record, action_record, t_end, 4)   # 显示所有的测试，更价值观清晰\n",
    "\n",
    "        tune.report(score)\n",
    "\n",
    "    print(\"Finished Training\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "num_samples=10\n",
    "max_num_epochs=10\n",
    "gpus_per_trial=0\n",
    "\n",
    "config = {\n",
    "    \"gamma\": tune.loguniform(1e-4, 1),\n",
    "    \"polyak\": tune.loguniform(1e-4, 1),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"alpha\": tune.loguniform(1e-4, 100),\n",
    "    \"batch_size\": tune.choice([64, 128, 516, 1024]),\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"score\",\n",
    "    mode=\"max\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=[\"gamma\", \"polyak\", \"lr\", \"alpha\", \"batch_size\"],\n",
    "    metric_columns=[\"score\"]\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    partial(train_reward),\n",
    "    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"gamma\": 0.5,\n",
    "    \"polyak\": 0.1,\n",
    "    \"lr\": 0.05,\n",
    "    \"alpha\": 0.5,\n",
    "    \"batch_size\": 1024,\n",
    "}\n",
    "\n",
    "train_reward(config, times_test=10, steps=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}