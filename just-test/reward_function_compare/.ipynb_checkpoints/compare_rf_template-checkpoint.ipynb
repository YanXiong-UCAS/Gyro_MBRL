{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af4e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import gym, ray\n",
    "import ray.rllib.agents.ddpg as ddpg\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from os import path\n",
    "from scipy.integrate import solve_ivp\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from functools import partial\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.tune.registry import register_env\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c11fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义环境 --- Custom environment #\n",
    "\n",
    "'''\n",
    "Difference with GyroscopeEnvV0:\n",
    "-- Gimbal voelocity no longer clipped into [-max velocity, max velocity] range\n",
    "-- Add several reward functions: exponential, power, sparse, etc...\n",
    "-- Add a termination condition for sparse reward funciton\n",
    "'''\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from os import path\n",
    "from scipy.integrate import solve_ivp\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "class GyroscopeEnvV1(gym.Env):\n",
    "\n",
    "    \"\"\"\n",
    "    GyroscopeEnv:\n",
    "        GyroscopeEnv is a GYM environment for Quanser 3-DOF gyroscope. The gyroscope consists of a disk mounted \n",
    "        inside an inner gimbal which in turn is mounted inside an outer gimbal.\n",
    "        The two gimbals are controlled by a RL controller, and the disk is controlled by a PID controller.\n",
    "    \n",
    "    State:   # 状态值 --- 7D   都是原始值？？？？？？？\n",
    "        state = [x1, x2, x3, x4, x1_ref, x3_ref, w] (7 dimensions)   # 状态空间\n",
    "        Outer red gimbal:   # 外部红色的万向节\n",
    "            x1, or theta: angular position [rad]   # 角度位置\n",
    "            x2, or dot(theta): angular velocity [rad/s]   # 角速度\n",
    "            x1_ref: angular position reference [rad]   # 目标角度位置\n",
    "            u1: motor voltage [V]   # 电机电压\n",
    "        Inner blue gimbal:   # 内部蓝色万向节\n",
    "            x3, or phi: angular position [rad]   # 角度位置\n",
    "            x4, or dot(phi): angular velocity [rad/s]   # 角速度\n",
    "            x3_ref: angular position reference [rad]   # 目标角度位置\n",
    "            u2: motor voltage [V]   # 电机电压\n",
    "        Golden disk:   # 金黄色转盘\n",
    "            w: angular velocity [rad/s]   # 角速度\n",
    "            u3: motor voltage [V]   # 电机电压\n",
    "        Mechanical constraints:   # 机械约束，参数边界\n",
    "            motor voltage: [-10, 10] [V]   # 电机电压\n",
    "            gimbal velocity: [-100, 100] [rpm]   # 万向节转速\n",
    "            disk velocity: [-300, 300] [rpm]   # 转盘转速\n",
    "    \n",
    "    Observation:   # 观测值 --- 9D   所有数字需要归一化？？？？？？？\n",
    "        observation = [cos(x1), sin(x1), x2, cos(x3), sin(x3), x4, x1_ref, x3_ref, w] (9 dimensions)   # 观测空间\n",
    "        The angles have been replaced with their cosine and sine to prevent the discontinuity at -pi and pi.\n",
    "        The observation space is thus larger than the state space.\n",
    "        \n",
    "    Action:   # 动作空间\n",
    "        action = [a1, a2]   # 动作空间\n",
    "        Note: a1, a2 are normalized voltages   # 归一化电压\n",
    "              u1, u2 = 10*a1, 10*a2 are actual voltages   # 实际电压指\n",
    "              T1, T2 = KtotRed*u1, KtotBlue*u2 are motor torques   # 电机扭矩，KtoRed和KtoBlue是电机属性参数\n",
    "        \n",
    "    Initialization:   # 初始化\n",
    "        Some versions of Gym may not support initialization with arguments, so initialize it manully with: \n",
    "        # create env   # 创建环境\n",
    "        env = GyroscopeEnv()\n",
    "        env.init(simu_args = simu_args, reward_func = reward_func, reward_args = reward_args)\n",
    "        # simu_args, with optional simulation step (dt), episode length (ep_len), and random seed (seed)   # 仿真三个基本参数\n",
    "        simu_args = {'dt': 0.05, 'ep_len': 100, 'seed': 2， ‘friction’: False}   # 未考虑摩擦\n",
    "        # reward_func, optional reward function, default value is 'Quadratic'   # 奖励函数，修改后没有默认值，必须附值\n",
    "        reward_func = 'Quadratic'\n",
    "        # reward_args, optional reward parameters  # 用于计算奖励函数的权重参数\n",
    "        reward_args = {'qx1': 1, 'qx2': 0.01, 'qx3': 1, 'qx4': 0.01, 'pu1': 0, 'pu2': 0}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Initialization ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #        \n",
    "                \n",
    "    def __init__(self, env_config):\n",
    "        \n",
    "        # Initialize mechanical parameters of the gyroscope   # 初始化Gyro机械参数\n",
    "        self.init_gyro()\n",
    "        \n",
    "        # Initialize simulation parameters   # 初始化仿真参数\n",
    "        self.init_simu(**env_config[\"simu_args\"])\n",
    "        \n",
    "        # Initialize reward parameters   # 初始化奖励函数参数，用于选择奖励函数类型\n",
    "        self.init_reward(env_config[\"reward_func\"], env_config[\"reward_args\"])\n",
    "        \n",
    "        # State space, 7D   # 状态空间初始化设置\n",
    "        self.state_bound = np.array([self.maxAngle, self.maxGimbalSpeed, self.maxAngle, self.maxGimbalSpeed, \n",
    "                                   self.maxAngle, self.maxAngle, self.maxDiskSpeed], dtype = np.float32)   # 状态空间初始化空间大小，设置最大值\n",
    "        self.state_space = spaces.Box(low = -self.state_bound, high = self.state_bound, dtype = np.float32)   # 状态空间上下限设置\n",
    "        \n",
    "        # Observation space (normalized), 9D   # 观测空间初始化设置（归一化）\n",
    "        self.observation_bound = np.array([1.0] * 9, dtype = np.float32)   # 观测空间初始化空间大小，设置空间维数 \n",
    "        self.observation_space = spaces.Box(low = -self.observation_bound, high = self.observation_bound, \n",
    "                                            dtype = np.float32)   # 观测空间上下限设置\n",
    "#         print(\"self.observation_bound：\", self.observation_bound)\n",
    "#         print(\"self.observation_space：\", self.observation_space)\n",
    "        \n",
    "        # Action space (normalized), 2D   # 动作空间（归一化）\n",
    "        self.action_bound = np.array([1.0] * 2, dtype = np.float32)   # 动作空间设置，设置空间维数大小\n",
    "        self.action_space = spaces.Box(low = -self.action_bound, high = self.action_bound, dtype = np.float32)   # 动作空间上下限设置\n",
    "    \n",
    "    # Initialize fixed parameters of the gyroscope   # 初始化Gyro的特定机械参数\n",
    "    def init_gyro(self):\n",
    "        \n",
    "        # Inertias in Kg*m2, from SP report page 23, table 2   # 设备固定参数\n",
    "        self.Jrx1 = 0.0179\n",
    "        self.Jbx1 = 0.0019\n",
    "        self.Jbx2 = 0.0008\n",
    "        self.Jbx3 = 0.0012\n",
    "        self.Jdx1 = 0.0028\n",
    "        self.Jdx2 = 0.0056\n",
    "        self.Jdx3 = 0.0056\n",
    "#         print(\"Gyro的特定机械参数：\")\n",
    "#         print(\"self.Jrx1=\" + str(self.Jrx1),\n",
    "#              \"self.Jbx1=\" + str(self.Jbx1),\n",
    "#              \"self.Jbx2\" + str(self.Jbx2),\n",
    "#              \"self.Jbx3\" + str(self.Jbx3),\n",
    "#              \"self.Jdx1\" + str(self.Jdx1),\n",
    "#              \"self.Jdx2\" + str(self.Jdx2),\n",
    "#              \"self.Jdx3\" + str(self.Jdx3))\n",
    "    \n",
    "\n",
    "        # Combined inertias to simplify equations, from SP report page 22, state space equations   # 状态空间空间方程，组合惯量\n",
    "        self.J1 = self.Jbx1 - self.Jbx3 + self.Jdx1 - self.Jdx3\n",
    "        self.J2 = self.Jbx1 + self.Jdx1 + self.Jrx1\n",
    "        self.J3 = self.Jbx2 + self.Jdx1\n",
    "\n",
    "        # Motor constants, from SP report page 23, table 1   # 电机参数\n",
    "        self.Kamp = 0.5 # current gain, A/V   # 电流增益\n",
    "        self.Ktorque = 0.0704 # motor gain, Nm/A   # 电机增益\n",
    "        self.eff = 0.86 # motor efficiency   # 电机效率\n",
    "        self.nRed = 1.5 # red gearbox eatio   # 红色万向节变速箱传动比\n",
    "        self.nBlue = 1 # blue gearbox eatio   # 蓝色万向节变速箱传动比\n",
    "        self.KtotRed = self.Kamp * self.Ktorque * self.eff * self.nRed # Nm/V   # 红色万向节扭矩，特定计算公式\n",
    "        self.KtotBlue = self.Kamp * self.Ktorque * self.eff * self.nBlue # Nm/V   # 蓝色万向节扭矩，特定计算公式\n",
    "        \n",
    "        # Mechanical constraints   机械系统约束\n",
    "        self.maxVoltage = 10 # V   # 最大电压\n",
    "        self.maxAngle = np.pi # rad   # 最大角度\n",
    "        self.maxGimbalSpeed = 100 * 2 * np.pi / 60 # rad/s   # 最大万向节转速\n",
    "        self.maxDiskSpeed = 300 * 2 * np.pi / 60 # rad/s   # 最大转盘转速\n",
    "#         print(\"Max speed of Gimbal:\", self.maxGimbalSpeed )\n",
    "#         print(\"Max speed of Disk:\", self.maxDiskSpeed)\n",
    "        \n",
    "    # Initialize simulation parameters   # 初始化仿真参数\n",
    "    def init_simu(self, dt = 0.05, ep_len = 100, seed = 2, friction = False):\n",
    "        \n",
    "        # Gyroscope state and observation   # Gyro状态空间与观测值空间\n",
    "        self.state = np.array([0] * 7)   # Gyro状态空间\n",
    "        self.observe()   # Gyro观测空间   自动打印出观测空间？？？？？？\n",
    "\n",
    "        # Time step in s   # 时间步长\n",
    "        self.dt = dt\n",
    "        self.eval_per_dt = int(dt / 0.01) # run evaluation every 0.01s   # ??????????????????????????????????\n",
    "        \n",
    "        # Episode length and current episode   # 剧集长度，一个epoch由多个episode组成，所有episode运行结束进行一次判断\n",
    "        self.ep_len = ep_len   # 剧集长度设置\n",
    "        self.ep_cur = 0   # 当前剧集数\n",
    "#         print(\"ep_len：\" + str(ep_len))\n",
    "        \n",
    "        # Seed for random number generation   # 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed()值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。\n",
    "        self.seed(seed)\n",
    "        self.viewer = None\n",
    "#         print(\"seed：\" + str(seed))\n",
    "        \n",
    "        # Friction   摩擦力\n",
    "        self.fvr = 0.002679 if friction else 0   # 存在选项，如果考虑摩擦，则赋值，如果不考虑摩擦则归零\n",
    "        self.fcr = 0\n",
    "        self.fvb = 0.005308 if friction else 0   # 存在选项，如果考虑摩擦，则赋值，如果不考虑摩擦则归零\n",
    "        self.fcb = 0\n",
    "        \n",
    "    # Initialize reward parameters   # 初始化奖励参数，用于定义各类奖励函数的ID，便于调用\n",
    "    def init_reward(self, reward_func, reward_args):\n",
    "                \n",
    "        reward_dict = {\n",
    "            # continuous reward functions, part one   # 连续奖励函数，part one\n",
    "            'Quadratic': self.quad_reward,\n",
    "            'Quadratic with bonus':self.quad_bon_reward,\n",
    "            'Quadratic with exponential': self.quad_exp_reward,\n",
    "            'Quadratic with ending penalty': self.quad_end_pen_reward,\n",
    "            'Quadratic with penalty': self.quad_pen_reward,\n",
    "            'Absolute': self.abs_reward,\n",
    "            'Normalized': self.norm_reward,\n",
    "            'Normalized with bonus': self.norm_bon_reward,\n",
    "            # continuous reward functions, part two   # 连续奖励函数，part two\n",
    "            'Power':self.power_reward,\n",
    "            'Exponential': self.exp_reward,\n",
    "            'PE': self.power_exp_reward,\n",
    "            # sparse reward functions   # 稀疏奖励函数\n",
    "            'Sparse':self.sparse_reward,\n",
    "            'Sparse with exp': self.sparse_reward_with_exp,\n",
    "            'Sparse with exp 2': self.sparse_reward_with_exp_2\n",
    "        }\n",
    "        if reward_func in ['Sparse']: # 'Sparse with exp'\n",
    "            self.sparse = True\n",
    "        else:\n",
    "            self.sparse = False\n",
    "        self.reward_func = reward_dict[reward_func]\n",
    "        self.reward_args = reward_args\n",
    "#         print(\"reward_dict[reward_func]:\" + str(reward_dict[reward_func]))\n",
    "#         print(\"reward_args:\" + str(reward_args))\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ----------------------------------------------- Step ----------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    # Simulate the environment fot one step dt   # 计算一个步长(step)的仿真环境，也就是计算了ep_len(100)次\n",
    "    def step(self, a):\n",
    "        \n",
    "        # extract states and actions   # 提取状态值和动作值\n",
    "        x1, x2, x3, x4, x1_ref, x3_ref, w = self.state   # 状态空间取值\n",
    "        a1, a2 = a   # 动作空间取值\n",
    "        u1, u2 = self.maxVoltage * a1, self.maxVoltage * a2   # 输出电压赋值\n",
    "\n",
    "        # Increment episode   # 递增量，每次递增1\n",
    "        self.ep_cur += 1\n",
    "#         print('/'+'-'*42+f'{datetime.now()} 开始训练' + '-'*36 + '\\\\')\n",
    "#         print(f\"Episode {self.ep_cur}\\n >> >> >> >> >> >> >>\")\n",
    "        \n",
    "\n",
    "        # For quad_end_pen_reward, check if terminal state is reached   # 检查是否达到终止状态\n",
    "        if self.reward_func == self.quad_end_pen_reward and self.ep_cur == self.ep_len:   # ？？？？？？\n",
    "            self.reward_args['end_horizon'] = 1   # ？？？？？？？？？？\n",
    "            \n",
    "#         print(self.quad_end_pen_reward)   # ???????\n",
    "\n",
    "        # run simulation for a step   # 跑一个步长的仿真\n",
    "        results = solve_ivp(   # solve_ivp, 该函数对给定初始值的常微分方程组进行数值积分, https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html\n",
    "                    fun = self.dxdt,    # 调用函数fun(t, y)\n",
    "                    t_span = (0, self.dt), # solver starts with t = 0 and integrates until it reaches t = self.dt\n",
    "                    y0 = [x1, x2, x3, x4], # initial state\n",
    "                    method = 'RK45',   # ODE solver --- Explicit Runge-Kutta method of order 5(4)\n",
    "                    t_eval = np.linspace(0, self.dt, self.eval_per_dt), # times at which to store the computed solution\n",
    "                    args = (u1, u2) \n",
    "                )\n",
    "        # print(\"results:\", results)   # 计算了ep_len(100)次，\n",
    "        \n",
    "        # evaluated states, each contains eval_per_dt points   # 已评估的状态，每个都包含eval_per_dt点\n",
    "        x1_eval = results.y[0]\n",
    "        x2_eval = results.y[1]\n",
    "        x3_eval = results.y[2]\n",
    "        x4_eval = results.y[3]\n",
    "#         print(\"已评估的状态：\")\n",
    "#         print(\"x2_eval：\" + str(x2_eval))\n",
    "#         print(\"x4_eval：\" + str(x4_eval))\n",
    "\n",
    "        # change in velocity, or acceleration   # 速度变化，评估值与状态值之间的变化\n",
    "        dx2 = x2_eval[-1] - x2\n",
    "        dx4 = x4_eval[-1] - x4\n",
    "#         print(\"万向节转速变化：\")\n",
    "#         print(\"x2_eval[-1]：\" +str(x2_eval[-1]))\n",
    "#         print(\"x2：\" + str(x2))\n",
    "#         print(\"dx2：\" + str(dx2))\n",
    "#         print(\"x4_eval[-1]：\" +str(x4_eval[-1]))\n",
    "#         print(\"x4：\" + str(x4))\n",
    "#         print(\"dx4：\" + str(dx4))        \n",
    "        \n",
    "        \n",
    "        # keep only the last evaluation value   # 只保留最后的评估值\n",
    "        x1 = x1_eval[-1]\n",
    "        x2 = x2_eval[-1]\n",
    "        x3 = x3_eval[-1]\n",
    "        x4 = x4_eval[-1]\n",
    "#         print(\"只保留最后的评估值：\")\n",
    "#         print(\"x2_eval[-1]:\" + str(x2_eval[-1]))\n",
    "#         print(\"x4_eval[-1]:\" + str(x4_eval[-1]))\n",
    "        \n",
    "        # Angle error (normalized between pi and -pi to get smallest distance)   # 角度误差（在pi和-pi之间归一化，以得到最小的距离\n",
    "        x1_diff = self.angle_normalize(x1 - x1_ref)\n",
    "        x3_diff = self.angle_normalize(x3 - x3_ref)\n",
    "#         print(\"角度误差归一化：\")\n",
    "#         print(\"x1_diff:\" + str(x1_diff))\n",
    "#         print(\"x3_diff:\" + str(x3_diff))\n",
    "        \n",
    "        # update state and observation   # 更新状态空间和观测空间，其中的观测空间应该归一化？？？？？？？\n",
    "        self.state = np.array([x1, x2, x3, x4, x1_ref, x3_ref, w])   # 状态空间\n",
    "        self.observe()   # 观测空间，自动打印出观测空间？？？？？？\n",
    "#         print(\"更新状态空间和观测空间：\")\n",
    "#         print(\"self.state：\" + str(self.state))\n",
    "#         print(\"self.observe():\" + str(self.observe()))\n",
    "\n",
    "        # Reward(float), normalized everything in advance\n",
    "        reward = self.reward_func(x1_diff/self.maxAngle, x3_diff/self.maxAngle, \n",
    "                                  x2/self.maxGimbalSpeed, x4/self.maxGimbalSpeed, \n",
    "                                  dx2/self.maxGimbalSpeed, dx4/self.maxGimbalSpeed,\n",
    "                                  a1, a2, **self.reward_args)\n",
    "#         print(\"奖励函数类型：\" + str(env_config[\"reward_func\"]))\n",
    "#         print(\"reward：\" + str(reward))\n",
    "#         print('/'+'-'*42+f'{datetime.now()} 训练结束' + '-'*36 + '\\\\')\n",
    "        \n",
    "        # Done(bool): whether it’s time to reset the environment again.\n",
    "        if self.sparse:\n",
    "            # in sparse reward functions, terminate the episode when the speed is too large\n",
    "            # otherwise the exploration will happen mainly in high speed area, which is not desired\n",
    "            done = self.ep_cur > self.ep_len or x2 > 2*self.maxGimbalSpeed or x4 > 2 * self.maxGimbalSpeed\n",
    "        else:\n",
    "            # in other reward functions, terminating the episode early will encourage the agent to \n",
    "            # speed up the gyroscope and end the episode, because the reward is negative\n",
    "            done = self.ep_cur > self.ep_len\n",
    "        \n",
    "        # Info(dict): diagnostic information useful for debugging. \n",
    "        info = {'state': self.state, 'observation': self.observation}\n",
    "        \n",
    "        return self.scaled_observation(), reward, done, info #return self.observation, reward, done, info\n",
    "    \n",
    "    # Compute the derivative of the state, here u is NOT normalized   # 计算状态值的导数，此处的u未归一化\n",
    "    def dxdt(self, t, x, u1, u2):\n",
    "\n",
    "        J1, J2, J3, Jdx3 = self.J1, self.J2, self.J3, self.Jdx3\n",
    "        w = self.state[-1]\n",
    "\n",
    "        # Convert input voltage to input torque   # 将输入电压转换为输入扭矩\n",
    "        T1, T2 = self.KtotRed * u1, self.KtotBlue * u2   # 此处的计算方式是设备固定特定计算公式\n",
    "        \n",
    "        # Friction   # 摩擦力\n",
    "        T1 = T1 - self.fvr*x[1] - self.fcr*np.sign(x[1])\n",
    "        T2 = T2 - self.fvb*x[3] - self.fcb*np.sign(x[3])\n",
    "        \n",
    "        # Equations of motion   # 运动方程\n",
    "        dx_dt = [0, 0, 0, 0]\n",
    "        dx_dt[0] = x[1]\n",
    "        dx_dt[1] = (T1+J1*np.sin(2*x[2])*x[1]*x[3]-Jdx3*np.cos(x[2])*x[3]*w)/(J2 + J1*np.power(np.sin(x[2]),2))\n",
    "        dx_dt[2] = x[3]\n",
    "        dx_dt[3] = (T2-J1*np.cos(x[2])*np.sin(x[2])*np.power(x[1],2)+Jdx3*np.cos(x[2])*x[1]*w)/J3\n",
    "        \n",
    "        return dx_dt\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part I ------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def abs_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*abs(x1_diff) + qx3*abs(x3_diff) + qx2*abs(x2) + qx4*abs(x4) + pu1*abs(u1) + pu2*abs(u2))\n",
    "\n",
    "    def norm_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def norm_bon_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, k = 0.2, qx2 = 0, qx4 = 0, pu1 = 0, pu2 = 0, bound = 0.001, bonus = 1):\n",
    "        return -((abs(x1_diff)/k)/(1 + (abs(x1_diff)/k)) + (abs(x3_diff)/k)/(1 + (abs(x3_diff)/k)) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    def quad_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2))\n",
    "\n",
    "    def quad_exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, eax1 = 10, ebx1 = 10, eax3 = 10, ebx3 = 10):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + eax1*(1-np.exp(-ebx1*(x1_diff**2))) + eax3*(1-np.exp(-ebx3*(x3_diff**2))))\n",
    "\n",
    "    def quad_end_pen_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, sx1 = 10, sx3 = 10, end_horizon = 0):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2) + end_horizon*(sx1*(x1_diff**2)+sx3*(x3_diff**2)))\n",
    "\n",
    "    def quad_pen_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, penalty = 50):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) - penalty*(abs(x1_diff) >= bound or abs(x3_diff) >= bound)\n",
    "\n",
    "    def quad_bon_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 0.01, qx3 = 1, qx4 = 0.01, pu1 = 0, pu2 = 0, bound = 0.1, bonus = 5):\n",
    "        return -(qx1*(x1_diff**2) + qx3*(x3_diff**2) + qx2*(x2**2) + qx4*(x4**2) + pu1*(u1**2) + pu2*(u2**2)) + bonus*(abs(x1_diff) <= bound or abs(x3_diff) <= bound)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part II ------------------------------------------ #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    def power_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.5):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p)\n",
    "        \n",
    "    def exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    def power_exp_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, p = 0.1, e = 10):\n",
    "        return -(qx1*abs(x1_diff)**p + qx3*abs(x3_diff)**p + qx2*abs(x2)**p + qx4*abs(x4)**p + pu1*abs(u1)**p + pu2*abs(u2)**p) -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2))))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ------------------------------------------ Reward Part III ----------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "    def sparse_reward(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, bx = 0.01, rx = 1, bv = 0.01, rv = 0, bu = 0.01, ru = 0):\n",
    "        r = 0\n",
    "        if abs(x1_diff) <= bx and abs(x3_diff) <= bx:\n",
    "            if abs(x2) <= bv and abs(x4) <= bv:\n",
    "                r += rv\n",
    "            if abs(dx2) <= bu and abs(dx4) <= bu:\n",
    "                r += ru\n",
    "            r += rx\n",
    "        return r\n",
    "\n",
    "    def sparse_reward_with_exp(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10, bound = 0.01, reward = 1):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2)))) + reward*(abs(x1_diff) <= bound and abs(x3_diff) <= bound)\n",
    "    \n",
    "    def sparse_reward_with_exp_2(self, x1_diff, x3_diff, x2, x4, dx2, dx4, u1, u2, qx1 = 1, qx2 = 1, qx3 = 1, qx4 = 1, pu1 = 0, pu2 = 0, e = 10, bound = 0.01, reward = 1):\n",
    "        return -(qx1*(1-np.exp(-e*abs(x1_diff))) + qx3*(1-np.exp(-e*abs(x3_diff))) + qx2*(1-np.exp(-e*abs(x2))) + qx4*(1-np.exp(-e*abs(x4))) + pu1*(1-np.exp(-e*abs(u1))) + pu2*(1-np.exp(-e*abs(u2)))) + reward*(abs(x1_diff) <= bound) + reward*(abs(x3_diff) <= bound)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    # ---------------------------------------------- Helper ---------------------------------------------- #\n",
    "    # ---------------------------------------------------------------------------------------------------- #\n",
    "    \n",
    "    # reset system to a given or random initial state\n",
    "    def reset(self, x_0 = None):\n",
    "        \n",
    "        # reset state\n",
    "        if x_0 is None:\n",
    "            self.state = self.state_space.sample()\n",
    "        else:\n",
    "            self.state = x_0\n",
    "        # update observation\n",
    "        self.observe()\n",
    "        #print(self.observe())\n",
    "        # reset counter\n",
    "        self.ep_cur = 0\n",
    "        \n",
    "        return self.observation\n",
    "        \n",
    "    # return normalized observation  返回归一化观测值，从状态空间求解得到观测值空间\n",
    "    def observe(self):\n",
    "        s = self.state\n",
    "        self.observation = np.array([np.cos(s[0]), np.sin(s[0]), s[1]/self.maxGimbalSpeed, \n",
    "                                     np.cos(s[2]), np.sin(s[2]), s[3]/self.maxGimbalSpeed, \n",
    "                                     s[4]/self.maxAngle, s[5]/self.maxAngle, s[6]/self.maxDiskSpeed])\n",
    "#         print(\"s[1]:\" + str(s[1]))\n",
    "#         print(\"self.maxGimbalSpeed:\" + str(self.maxGimbalSpeed))\n",
    "#         print(\"s[3]:\" + str(s[3]))\n",
    "#         print(\"self.maxGimbalSpeed:\" + str(self.maxGimbalSpeed))\n",
    "#         print(\"self.observation:\", self.observation)\n",
    "#         print(\"self.observation_space:\", self.observation_space)\n",
    "        return self.observation\n",
    "    \n",
    "    def scaled_observation(self):\n",
    "        return self.observation * 0.01  # Keep the angles between -lim and lim   # 将角度设定在上下限之间，归一化，是否可以采用其他归一化方法呢？？？？？？\n",
    "    \n",
    "    def angle_normalize(self, x, lim = np.pi):\n",
    "        return ((x + lim) % (2 * lim)) - lim   # % --- 取模 - 返回除法的余数\n",
    "    \n",
    "    def seed(self, seed=None):   # # 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed()值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            \n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06a3ee",
   "metadata": {},
   "source": [
    "## Quadratic reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c180bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 11:47:59,679\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-07-06 11:48:01,561\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-07-06 11:48:01,561\tINFO trainer.py:698 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-07-06 11:48:01,562\tWARNING ddpg.py:182 -- `simple_optimizer` must be True (or unset) for DDPG!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xiongyan/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 11:48:01,788\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------\n",
      "Min/Mean/Max reward: -97.8420/-70.1189/-51.0386\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000001/checkpoint-1'}\n",
      "Time per epoch: 3.261704921722412 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2\n",
      "---------\n",
      "Min/Mean/Max reward: -104.3343/-69.8546/-37.4742\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000002/checkpoint-2'}\n",
      "Time per epoch: 18.78907871246338 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3\n",
      "---------\n",
      "Min/Mean/Max reward: -104.3343/-68.6296/-37.4742\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000003/checkpoint-3'}\n",
      "Time per epoch: 19.08636212348938 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4\n",
      "---------\n",
      "Min/Mean/Max reward: -104.3343/-67.6623/-37.4742\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000004/checkpoint-4'}\n",
      "Time per epoch: 18.69559597969055 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1454/-68.0350/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000005/checkpoint-5'}\n",
      "Time per epoch: 18.8266339302063 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6\n",
      "---------\n",
      "Min/Mean/Max reward: -121.5778/-69.4591/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000006/checkpoint-6'}\n",
      "Time per epoch: 18.95966863632202 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-69.5786/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000007/checkpoint-7'}\n",
      "Time per epoch: 19.280227661132812 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-68.3271/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000008/checkpoint-8'}\n",
      "Time per epoch: 17.990038871765137 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-67.6336/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000009/checkpoint-9'}\n",
      "Time per epoch: 18.521358966827393 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-67.1933/-31.8203\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000010/checkpoint-10'}\n",
      "Time per epoch: 18.493631601333618 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 11\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-67.9930/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000011/checkpoint-11'}\n",
      "Time per epoch: 17.95656442642212 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 12\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-68.5314/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000012/checkpoint-12'}\n",
      "Time per epoch: 17.8746075630188 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 13\n",
      "---------\n",
      "Min/Mean/Max reward: -127.5922/-68.2996/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000013/checkpoint-13'}\n",
      "Time per epoch: 18.221713542938232 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 14\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-69.5272/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000014/checkpoint-14'}\n",
      "Time per epoch: 18.3726327419281 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 15\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-69.9583/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000015/checkpoint-15'}\n",
      "Time per epoch: 18.17264199256897 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 16\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-69.4463/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000016/checkpoint-16'}\n",
      "Time per epoch: 18.078612327575684 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 17\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-69.8508/-25.8426\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000017/checkpoint-17'}\n",
      "Time per epoch: 18.482964754104614 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 18\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-69.2096/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000018/checkpoint-18'}\n",
      "Time per epoch: 18.609227180480957 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 19\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-68.3545/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000019/checkpoint-19'}\n",
      "Time per epoch: 18.201887845993042 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 20\n",
      "---------\n",
      "Min/Mean/Max reward: -115.1769/-68.4790/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000020/checkpoint-20'}\n",
      "Time per epoch: 18.197139024734497 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 21\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1139/-66.7066/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000021/checkpoint-21'}\n",
      "Time per epoch: 18.826714754104614 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 22\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1139/-66.6617/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000022/checkpoint-22'}\n",
      "Time per epoch: 18.49591612815857 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 23\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1139/-67.2442/-31.0424\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000023/checkpoint-23'}\n",
      "Time per epoch: 18.258501529693604 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 24\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1139/-66.9512/-32.5744\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000024/checkpoint-24'}\n",
      "Time per epoch: 18.163943767547607 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 25\n",
      "---------\n",
      "Min/Mean/Max reward: -113.1139/-66.1701/-32.5744\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000025/checkpoint-25'}\n",
      "Time per epoch: 18.58453941345215 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 26\n",
      "---------\n",
      "Min/Mean/Max reward: -109.8961/-64.9642/-32.5744\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000026/checkpoint-26'}\n",
      "Time per epoch: 18.802573919296265 s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27\n",
      "---------\n",
      "Min/Mean/Max reward: -109.8961/-64.8717/-32.5744\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000027/checkpoint-27'}\n",
      "Time per epoch: 18.532812356948853 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 28\n",
      "---------\n",
      "Min/Mean/Max reward: -109.8961/-65.3434/-32.5744\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000028/checkpoint-28'}\n",
      "Time per epoch: 18.372195720672607 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 29\n",
      "---------\n",
      "Min/Mean/Max reward: -102.1409/-64.8223/-34.0017\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000029/checkpoint-29'}\n",
      "Time per epoch: 18.598241567611694 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 30\n",
      "---------\n",
      "Min/Mean/Max reward: -86.2177/-65.9590/-34.8109\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000030/checkpoint-30'}\n",
      "Time per epoch: 18.663442611694336 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 31\n",
      "---------\n",
      "Min/Mean/Max reward: -86.2177/-65.7248/-34.8109\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000031/checkpoint-31'}\n",
      "Time per epoch: 18.53929615020752 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 32\n",
      "---------\n",
      "Min/Mean/Max reward: -86.2177/-66.7694/-36.5296\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000032/checkpoint-32'}\n",
      "Time per epoch: 18.70461630821228 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 33\n",
      "---------\n",
      "Min/Mean/Max reward: -83.8769/-66.4861/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000033/checkpoint-33'}\n",
      "Time per epoch: 18.99439001083374 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 34\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-66.7898/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000034/checkpoint-34'}\n",
      "Time per epoch: 19.432283639907837 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 35\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-66.8478/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000035/checkpoint-35'}\n",
      "Time per epoch: 18.595390558242798 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 36\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-67.2178/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000036/checkpoint-36'}\n",
      "Time per epoch: 18.329219102859497 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 37\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-66.9647/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000037/checkpoint-37'}\n",
      "Time per epoch: 18.911418199539185 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 38\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-66.8987/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000038/checkpoint-38'}\n",
      "Time per epoch: 18.816169023513794 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 39\n",
      "---------\n",
      "Min/Mean/Max reward: -95.6205/-67.1884/-36.1560\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000039/checkpoint-39'}\n",
      "Time per epoch: 18.79867672920227 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 40\n",
      "---------\n",
      "Min/Mean/Max reward: -93.0140/-66.8747/-38.4877\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000040/checkpoint-40'}\n",
      "Time per epoch: 18.911824226379395 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 41\n",
      "---------\n",
      "Min/Mean/Max reward: -97.9036/-67.3471/-38.4877\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000041/checkpoint-41'}\n",
      "Time per epoch: 19.141566038131714 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 42\n",
      "---------\n",
      "Min/Mean/Max reward: -97.9036/-66.4830/-38.4877\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000042/checkpoint-42'}\n",
      "Time per epoch: 19.455711841583252 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 43\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.0728/-38.4877\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000043/checkpoint-43'}\n",
      "Time per epoch: 18.726889848709106 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 44\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.1425/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000044/checkpoint-44'}\n",
      "Time per epoch: 18.4383807182312 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 45\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.0255/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000045/checkpoint-45'}\n",
      "Time per epoch: 18.829047203063965 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 46\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.9351/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000046/checkpoint-46'}\n",
      "Time per epoch: 18.620553016662598 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 47\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.7168/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000047/checkpoint-47'}\n",
      "Time per epoch: 18.709618091583252 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 48\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.5150/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000048/checkpoint-48'}\n",
      "Time per epoch: 18.913719415664673 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 49\n",
      "---------\n",
      "Min/Mean/Max reward: -109.0376/-67.6356/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000049/checkpoint-49'}\n",
      "Time per epoch: 19.250759840011597 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 50\n",
      "---------\n",
      "Min/Mean/Max reward: -102.2341/-67.3421/-37.4412\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000050/checkpoint-50'}\n",
      "Time per epoch: 18.890142679214478 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 51\n",
      "---------\n",
      "Min/Mean/Max reward: -102.2341/-67.1766/-37.6963\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000051/checkpoint-51'}\n",
      "Time per epoch: 18.695879459381104 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 52\n",
      "---------\n",
      "Min/Mean/Max reward: -102.2341/-67.7156/-37.6963\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000052/checkpoint-52'}\n",
      "Time per epoch: 18.568668603897095 s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53\n",
      "---------\n",
      "Min/Mean/Max reward: -101.8785/-67.0944/-37.6963\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000053/checkpoint-53'}\n",
      "Time per epoch: 18.709392070770264 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 54\n",
      "---------\n",
      "Min/Mean/Max reward: -103.5223/-67.9224/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000054/checkpoint-54'}\n",
      "Time per epoch: 18.89233636856079 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 55\n",
      "---------\n",
      "Min/Mean/Max reward: -103.5223/-69.4545/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000055/checkpoint-55'}\n",
      "Time per epoch: 18.748722553253174 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 56\n",
      "---------\n",
      "Min/Mean/Max reward: -103.5223/-70.1188/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000056/checkpoint-56'}\n",
      "Time per epoch: 18.64909076690674 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 57\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-70.3205/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000057/checkpoint-57'}\n",
      "Time per epoch: 19.105616331100464 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 58\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-69.4013/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000058/checkpoint-58'}\n",
      "Time per epoch: 18.9135901927948 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 59\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-70.1481/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000059/checkpoint-59'}\n",
      "Time per epoch: 18.93606734275818 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 60\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-69.1873/-35.6937\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000060/checkpoint-60'}\n",
      "Time per epoch: 18.741801023483276 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 61\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-69.5884/-36.2605\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000061/checkpoint-61'}\n",
      "Time per epoch: 18.996902227401733 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 62\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-68.6795/-36.2605\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000062/checkpoint-62'}\n",
      "Time per epoch: 19.312225580215454 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 63\n",
      "---------\n",
      "Min/Mean/Max reward: -104.0205/-68.6289/-45.1447\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000063/checkpoint-63'}\n",
      "Time per epoch: 18.935542345046997 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 64\n",
      "---------\n",
      "Min/Mean/Max reward: -91.1380/-68.1341/-45.2346\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000064/checkpoint-64'}\n",
      "Time per epoch: 18.791334867477417 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 65\n",
      "---------\n",
      "Min/Mean/Max reward: -103.7624/-68.5102/-42.4394\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000065/checkpoint-65'}\n",
      "Time per epoch: 19.15346384048462 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 66\n",
      "---------\n",
      "Min/Mean/Max reward: -103.7624/-67.8186/-38.2248\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000066/checkpoint-66'}\n",
      "Time per epoch: 19.214632272720337 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 67\n",
      "---------\n",
      "Min/Mean/Max reward: -103.7624/-67.0880/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000067/checkpoint-67'}\n",
      "Time per epoch: 19.001468420028687 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 68\n",
      "---------\n",
      "Min/Mean/Max reward: -103.7624/-66.9520/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000068/checkpoint-68'}\n",
      "Time per epoch: 18.869589805603027 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 69\n",
      "---------\n",
      "Min/Mean/Max reward: -103.7624/-66.6892/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000069/checkpoint-69'}\n",
      "Time per epoch: 19.40457844734192 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 70\n",
      "---------\n",
      "Min/Mean/Max reward: -105.1806/-67.0025/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000070/checkpoint-70'}\n",
      "Time per epoch: 19.27271580696106 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 71\n",
      "---------\n",
      "Min/Mean/Max reward: -105.1806/-66.3377/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000071/checkpoint-71'}\n",
      "Time per epoch: 18.954811811447144 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 72\n",
      "---------\n",
      "Min/Mean/Max reward: -117.1451/-66.3045/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000072/checkpoint-72'}\n",
      "Time per epoch: 18.828628540039062 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 73\n",
      "---------\n",
      "Min/Mean/Max reward: -117.1451/-66.0139/-35.1315\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000073/checkpoint-73'}\n",
      "Time per epoch: 19.198038339614868 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 74\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-66.4616/-36.8510\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000074/checkpoint-74'}\n",
      "Time per epoch: 19.128587007522583 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 75\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-66.8546/-36.8510\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000075/checkpoint-75'}\n",
      "Time per epoch: 19.21849036216736 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 76\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-66.9978/-36.8510\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000076/checkpoint-76'}\n",
      "Time per epoch: 19.18546152114868 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 77\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-66.2955/-36.8510\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000077/checkpoint-77'}\n",
      "Time per epoch: 19.202643394470215 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 78\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-65.6873/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000078/checkpoint-78'}\n",
      "Time per epoch: 19.266088008880615 s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-65.8302/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000079/checkpoint-79'}\n",
      "Time per epoch: 19.198232650756836 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 80\n",
      "---------\n",
      "Min/Mean/Max reward: -120.0188/-64.7650/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000080/checkpoint-80'}\n",
      "Time per epoch: 18.930631637573242 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 81\n",
      "---------\n",
      "Min/Mean/Max reward: -90.3431/-64.7516/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000081/checkpoint-81'}\n",
      "Time per epoch: 19.41119146347046 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 82\n",
      "---------\n",
      "Min/Mean/Max reward: -90.3431/-63.5982/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000082/checkpoint-82'}\n",
      "Time per epoch: 19.190257787704468 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 83\n",
      "---------\n",
      "Min/Mean/Max reward: -90.3431/-62.7778/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000083/checkpoint-83'}\n",
      "Time per epoch: 19.149203538894653 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 84\n",
      "---------\n",
      "Min/Mean/Max reward: -90.3431/-61.8465/-33.4494\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000084/checkpoint-84'}\n",
      "Time per epoch: 19.04833674430847 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 85\n",
      "---------\n",
      "Min/Mean/Max reward: -89.6116/-62.2093/-34.6331\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000085/checkpoint-85'}\n",
      "Time per epoch: 19.451520681381226 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 86\n",
      "---------\n",
      "Min/Mean/Max reward: -89.6116/-62.3845/-29.8861\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000086/checkpoint-86'}\n",
      "Time per epoch: 19.33259868621826 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 87\n",
      "---------\n",
      "Min/Mean/Max reward: -89.6116/-62.3625/-29.8861\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000087/checkpoint-87'}\n",
      "Time per epoch: 19.234923362731934 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 88\n",
      "---------\n",
      "Min/Mean/Max reward: -89.6116/-61.4468/-29.8861\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000088/checkpoint-88'}\n",
      "Time per epoch: 19.011390209197998 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 89\n",
      "---------\n",
      "Min/Mean/Max reward: -89.6116/-60.4287/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000089/checkpoint-89'}\n",
      "Time per epoch: 19.42410969734192 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 90\n",
      "---------\n",
      "Min/Mean/Max reward: -95.9829/-61.0250/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000090/checkpoint-90'}\n",
      "Time per epoch: 19.2612407207489 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 91\n",
      "---------\n",
      "Min/Mean/Max reward: -95.9829/-61.1932/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000091/checkpoint-91'}\n",
      "Time per epoch: 19.125566244125366 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 92\n",
      "---------\n",
      "Min/Mean/Max reward: -110.4461/-61.9173/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000092/checkpoint-92'}\n",
      "Time per epoch: 18.948581218719482 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 93\n",
      "---------\n",
      "Min/Mean/Max reward: -110.4461/-61.5884/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000093/checkpoint-93'}\n",
      "Time per epoch: 19.327796936035156 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 94\n",
      "---------\n",
      "Min/Mean/Max reward: -110.4461/-61.7841/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000094/checkpoint-94'}\n",
      "Time per epoch: 19.27335524559021 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 95\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-62.0935/-26.2467\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000095/checkpoint-95'}\n",
      "Time per epoch: 19.191442251205444 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 96\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-65.0323/-33.6816\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000096/checkpoint-96'}\n",
      "Time per epoch: 19.028627634048462 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 97\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-65.3251/-33.6816\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000097/checkpoint-97'}\n",
      "Time per epoch: 19.56485152244568 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 98\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-65.3986/-33.6816\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000098/checkpoint-98'}\n",
      "Time per epoch: 19.369258642196655 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 99\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-65.6899/-33.6816\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000099/checkpoint-99'}\n",
      "Time per epoch: 19.329789638519287 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 100\n",
      "---------\n",
      "Min/Mean/Max reward: -124.0355/-65.9715/-33.6816\n",
      "len mean: 101.0\n",
      "checkpoint saved to {'results/gyro_train/compare_rf/ddpg_Quadratic/checkpoint_000100/checkpoint-100'}\n",
      "Time per epoch: 19.181615829467773 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "总耗时： 1871.2903504371643 s\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1002'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"d29686ab-8671-45d7-8ef8-249bac285c42\" data-root-id=\"1002\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"183b895b-89d1-47ef-8ef0-4cd996c12a4e\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"source\":{\"id\":\"1037\"}},\"id\":\"1044\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"PanTool\"},{\"attributes\":{\"end\":101,\"reset_end\":101,\"reset_start\":-1,\"start\":-1,\"tags\":[[[\"index\",\"index\",null]]]},\"id\":\"1004\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#30a2da\",\"line_width\":3,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"0\"}},\"id\":\"1042\",\"type\":\"Line\"},{\"attributes\":{\"children\":[{\"id\":\"1003\"},{\"id\":\"1007\"},{\"id\":\"1072\"}],\"margin\":[0,0,0,0],\"name\":\"Row01623\",\"tags\":[\"embedded\"]},\"id\":\"1002\",\"type\":\"Row\"},{\"attributes\":{\"overlay\":{\"id\":\"1029\"}},\"id\":\"1027\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"1043\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"index\",\"@{index}\"],[\"0\",\"@{A_0}\"]]},\"id\":\"1006\",\"type\":\"HoverTool\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01627\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1003\",\"type\":\"Spacer\"},{\"attributes\":{\"end\":-59.43954811903875,\"reset_end\":-59.43954811903875,\"reset_start\":-71.30963366809917,\"start\":-71.30963366809917,\"tags\":[[[\"0\",\"0\",null]]]},\"id\":\"1005\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":3,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"0\"}},\"id\":\"1040\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"0\":{\"__ndarray__\":\"bRKArZyHUcCHMeFOsXZRwAr6LQZMKFHALD7ZnmPqUMAcQ7K2PQJRwPJrjFlhXVHA+vTpUAhlUcCfwbvx7hRRwIw/GnGN6FDAE6OttV/MUMCZNU3sjP9QwMSsJYwCIlHAvENINi0TUcAhOHOEvWFRwAuq+yVUfVHAJ4Ga65BcUcCGgzwydHZRwLr+iWJqTVHAaMoxybAWUcCSAK6dqB5RwOLFC2c4rVDAykSYm1iqUMDm7DwIoc9QwPssSuzgvFDACFojYuKKUMCcde/itT1QwL1AZcDKN1DAQxlNlPlVUMBbjoUuoTRQwIx0y+lffVDAUXicoWNuUMCjgFG7PbFQwK5hTHMcn1DAX9J73oyyUMDhIm/KQbZQwJEFuzXxzVDAcZrK8ry9UMDXpf4dhLlQwEPCQ0kPzFDAiZfKlfq3UMCO7DHCNtZQwKsQvtfonlDAAcICh6nEUMAQMRZLH8lQwBrjLuihwVDAKowK19j7UMCxYw8i4O1QwL4l+dT14FDAAU5N+K3oUMAmjKdm5dVQwIjz6wROy1DAmBs+08vtUMBfkRVuCsZQwGfuCRAI+1DAYuSX2BZdUcCgSw8Zm4dRwH/ZH2qClFHAZKXb0K5ZUcDbYR1ieolRwC/bYNn8S1HA6ptuQKhlUcDjtkEzfStRwBHqOlo/KFHASjJz0JUIUcC09FK6piBRwMDuh8Vj9FDAA75jbaLFUMCMh+Iu7rxQwP3FNEcbrFDAOpwnGynAUMCuwEGjnJVQwDqcS5Z9k1DAy1tkWuSAUMDGqYAjip1QwFzzCo+ytlDA6GMdxdu/UMDVcHHn6ZJQwJtMEoj8a1DAcpDNdCF1UMBFiG469jBQwN1JGSYaMFDApe3QPpPMT8CExLxMjmNPwFHtsQ1Z7E7ASLWe8soaT8AmIkScNzFPwEXVNLZlLk/ATh5ARjC5TsAUjxVc4DZOwJrNbPYxg07An06H/rqYTsARiyzkafVOwIZt/plQy07AEnGzAV7kTsBFhqT/+AtPwBT3HOURQlDAMcpe685UUMDKnWUxgllQwAin/qUmbFDAEzxEHi1+UMA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]},\"A_0\":{\"__ndarray__\":\"bRKArZyHUcCHMeFOsXZRwAr6LQZMKFHALD7ZnmPqUMAcQ7K2PQJRwPJrjFlhXVHA+vTpUAhlUcCfwbvx7hRRwIw/GnGN6FDAE6OttV/MUMCZNU3sjP9QwMSsJYwCIlHAvENINi0TUcAhOHOEvWFRwAuq+yVUfVHAJ4Ga65BcUcCGgzwydHZRwLr+iWJqTVHAaMoxybAWUcCSAK6dqB5RwOLFC2c4rVDAykSYm1iqUMDm7DwIoc9QwPssSuzgvFDACFojYuKKUMCcde/itT1QwL1AZcDKN1DAQxlNlPlVUMBbjoUuoTRQwIx0y+lffVDAUXicoWNuUMCjgFG7PbFQwK5hTHMcn1DAX9J73oyyUMDhIm/KQbZQwJEFuzXxzVDAcZrK8ry9UMDXpf4dhLlQwEPCQ0kPzFDAiZfKlfq3UMCO7DHCNtZQwKsQvtfonlDAAcICh6nEUMAQMRZLH8lQwBrjLuihwVDAKowK19j7UMCxYw8i4O1QwL4l+dT14FDAAU5N+K3oUMAmjKdm5dVQwIjz6wROy1DAmBs+08vtUMBfkRVuCsZQwGfuCRAI+1DAYuSX2BZdUcCgSw8Zm4dRwH/ZH2qClFHAZKXb0K5ZUcDbYR1ieolRwC/bYNn8S1HA6ptuQKhlUcDjtkEzfStRwBHqOlo/KFHASjJz0JUIUcC09FK6piBRwMDuh8Vj9FDAA75jbaLFUMCMh+Iu7rxQwP3FNEcbrFDAOpwnGynAUMCuwEGjnJVQwDqcS5Z9k1DAy1tkWuSAUMDGqYAjip1QwFzzCo+ytlDA6GMdxdu/UMDVcHHn6ZJQwJtMEoj8a1DAcpDNdCF1UMBFiG469jBQwN1JGSYaMFDApe3QPpPMT8CExLxMjmNPwFHtsQ1Z7E7ASLWe8soaT8AmIkScNzFPwEXVNLZlLk/ATh5ARjC5TsAUjxVc4DZOwJrNbPYxg07An06H/rqYTsARiyzkafVOwIZt/plQy07AEnGzAV7kTsBFhqT/+AtPwBT3HOURQlDAMcpe685UUMDKnWUxgllQwAin/qUmbFDAEzxEHi1+UMA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]},\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]},\"selected\":{\"id\":\"1038\"},\"selection_policy\":{\"id\":\"1060\"}},\"id\":\"1037\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_multi\":null,\"tools\":[{\"id\":\"1006\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1027\"},{\"id\":\"1028\"}]},\"id\":\"1030\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1060\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"below\":[{\"id\":\"1016\"}],\"center\":[{\"id\":\"1019\"},{\"id\":\"1023\"}],\"height\":400,\"left\":[{\"id\":\"1020\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"1043\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"1008\"},\"toolbar\":{\"id\":\"1030\"},\"width\":800,\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1012\"},\"y_range\":{\"id\":\"1005\"},\"y_scale\":{\"id\":\"1014\"}},\"id\":\"1007\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"text\":\"Reward Function\",\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"1008\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1029\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"Epoches\",\"formatter\":{\"id\":\"1047\"},\"major_label_policy\":{\"id\":\"1048\"},\"ticker\":{\"id\":\"1017\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#30a2da\",\"line_width\":3,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"0\"}},\"id\":\"1041\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1016\"},\"grid_line_color\":null,\"ticker\":null},\"id\":\"1019\",\"type\":\"Grid\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":3,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"0\"}},\"id\":\"1045\",\"type\":\"Line\"},{\"attributes\":{\"axis_label\":\"Value\",\"formatter\":{\"id\":\"1050\"},\"major_label_policy\":{\"id\":\"1051\"},\"ticker\":{\"id\":\"1021\"}},\"id\":\"1020\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"AllLabels\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01628\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1072\",\"type\":\"Spacer\"},{\"attributes\":{\"axis\":{\"id\":\"1020\"},\"dimension\":1,\"grid_line_color\":null,\"ticker\":null},\"id\":\"1023\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"1037\"},\"glyph\":{\"id\":\"1040\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1042\"},\"nonselection_glyph\":{\"id\":\"1041\"},\"selection_glyph\":{\"id\":\"1045\"},\"view\":{\"id\":\"1044\"}},\"id\":\"1043\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1038\",\"type\":\"Selection\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.2\"}};\n",
       "    var render_items = [{\"docid\":\"183b895b-89d1-47ef-8ef0-4cd996c12a4e\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"d29686ab-8671-45d7-8ef8-249bac285c42\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Curve   [index]   (0)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Quadratic',\n",
    "    \"reward_args\": {\n",
    "        'qx1': 1,\n",
    "        'qx2': 0,\n",
    "        'qx3': 1,\n",
    "        'qx4': 0,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0}\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_Quadratic\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da822de",
   "metadata": {},
   "source": [
    "## Absolute reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98a832a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-35c34036c7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# 注册环境 --- register_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GyroscopeEnv-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 第一版生效\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# 定义Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister_env\u001b[0;34m(name, env_creator)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second argument must be callable.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0m_global_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_CREATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, category, key, value)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_internal_kv_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mflush_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0m_internal_kv_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/experimental/internal_kv.py\u001b[0m in \u001b[0;36m_internal_kv_put\u001b[0;34m(key, value, overwrite)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             updated = ray.worker.global_worker.redis_client.hset(\n\u001b[0;32m---> 57\u001b[0;31m                 key, \"value\", value)\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             updated = ray.worker.global_worker.redis_client.hsetnx(\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36mhset\u001b[0;34m(self, name, key, value, mapping)\u001b[0m\n\u001b[1;32m   3048\u001b[0m                 \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HSET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhsetnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mcommand_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;31m# ensure this connection is connected to Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m             \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;31m# connections that the pool provides should be ready to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;31m# a command. if not, the connection was either returned to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout connecting to server\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;31m# connect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;31m# set the socket_timeout now that we're connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Absolute',\n",
    "    \"reward_args\": {\n",
    "        'qx1': 1,\n",
    "        'qx2': 0,\n",
    "        'qx3': 1,\n",
    "        'qx4': 0,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0}\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "# ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_absolute\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52712bd",
   "metadata": {},
   "source": [
    "## Normalized reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ffc824",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-953480672553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# 注册环境 --- register_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GyroscopeEnv-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 第一版生效\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# 定义Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister_env\u001b[0;34m(name, env_creator)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second argument must be callable.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0m_global_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_CREATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, category, key, value)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_internal_kv_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mflush_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0m_internal_kv_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/ray/experimental/internal_kv.py\u001b[0m in \u001b[0;36m_internal_kv_put\u001b[0;34m(key, value, overwrite)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             updated = ray.worker.global_worker.redis_client.hset(\n\u001b[0;32m---> 57\u001b[0;31m                 key, \"value\", value)\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             updated = ray.worker.global_worker.redis_client.hsetnx(\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36mhset\u001b[0;34m(self, name, key, value, mapping)\u001b[0m\n\u001b[1;32m   3048\u001b[0m                 \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HSET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhsetnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/client.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mcommand_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;31m# ensure this connection is connected to Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m             \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;31m# connections that the pool provides should be ready to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;31m# a command. if not, the connection was either returned to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout connecting to server\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gyro_mbrl/lib/python3.6/site-packages/redis/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;31m# connect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;31m# set the socket_timeout now that we're connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Normalized',\n",
    "    \"reward_args\": {\n",
    "        'k': 1,\n",
    "        'qx2': 1,\n",
    "        'qx4': 1,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0}\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "# ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_Normalized\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9658cd",
   "metadata": {},
   "source": [
    "## Quadratic reward with ending penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c70c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Quadratic with ending penalty',\n",
    "    \"reward_args\": {\n",
    "        'qx1': 1,\n",
    "        'qx2': 0,\n",
    "        'qx3': 1,\n",
    "        'qx4': 0,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0,\n",
    "        'sx1': 100,\n",
    "        'sx3':100,\n",
    "        'end_horizon': 0\n",
    "    }\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "# ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_q_ep\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7328a",
   "metadata": {},
   "source": [
    "## Quadratic reward with penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec095ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Quadratic with penalty',\n",
    "    \"reward_args\": {\n",
    "        'qx1': 1,\n",
    "        'qx2': 0,\n",
    "        'qx3': 1,\n",
    "        'qx4': 0,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0,\n",
    "        'bound': 0.2,\n",
    "        'penalty':10\n",
    "    }\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "# ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_q_p\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c50fe",
   "metadata": {},
   "source": [
    "## Quadratic reward with exponential term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给自定义环境定义id\n",
    "env_name = \"GyroscopeEnv-v1\"        # gym.make(\"GyroscopeEnvV0\")\n",
    "\n",
    "# 设置初始化参数 --- config\n",
    "config = {\n",
    "    \"env_config\":{\n",
    "    \"simu_args\": {\n",
    "        'dt': 0.05,\n",
    "        'ep_len': 100,\n",
    "        'seed': 2\n",
    "    },\n",
    "    \"reward_func\": 'Quadratic with exponential',\n",
    "    \"reward_args\": {\n",
    "        'qx1': 1,\n",
    "        'qx2': 0,\n",
    "        'qx3': 1,\n",
    "        'qx4': 0,\n",
    "        'pu1': 0,\n",
    "        'pu2': 0,\n",
    "        'eax1': 10,\n",
    "        'ebx1': 10,\n",
    "        'eax3': 10,\n",
    "        'ebx3': 10\n",
    "    }\n",
    "    },\n",
    "    #\"env\": \"GyroscopeEnv-v1\",\n",
    "    \"num_workers\": 0,  # parallelism\n",
    "    \"lr\": 0.01,\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"actor_hiddens\": [128, 32],\n",
    "    \"actor_hidden_activation\": \"relu\",\n",
    "    \"critic_hiddens\": [128, 32],\n",
    "    \"critic_hidden_activation\": \"relu\",\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"timesteps_per_iteration\": 1500,\n",
    "    \"evaluation_num_episodes\": 100,\n",
    "    \"train_batch_size\": 100,\n",
    "    \"target_noise\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"critic_lr\": 0.0025,\n",
    "    \"actor_lr\": 0.0025,\n",
    "    #\"framework\": \"torch\",   # 用于选择tensorflow/pytorch\n",
    "    \n",
    "}\n",
    "\n",
    "# 定义新建环境函数 --- env-creator\n",
    "def env_creator(env_config):\n",
    "    return GyroscopeEnvV1(env_config)  # return an env instance\n",
    "\n",
    "# Initialisation   初始化\n",
    "# ray.init()\n",
    "\n",
    "# 注册环境 --- register_env\n",
    "register_env(\"GyroscopeEnv-v1\", lambda config: env_creator(config))   # 第一版生效\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = ddpg.DDPGTrainer(\n",
    "    config=config,\n",
    "    env = env_name\n",
    ")\n",
    "\n",
    "agent = trainer\n",
    "\n",
    "# 开始训练\n",
    "#result = trainer.train()\n",
    "\n",
    "N_ITER = 100\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "rewards_space = []\n",
    "checkpoint_root = \"results/gyro_train/compare_rf/ddpg_q_e\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs\n",
    "\n",
    "start1 = time.time()\n",
    "for n in range(N_ITER):\n",
    "    \n",
    "    start0 = time.time()\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {\n",
    "        \"n\": n, \n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"], \n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"], \n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"], \n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "    rewards_space.append(result[\"episode_reward_mean\"])\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    end0 = time.time()\n",
    "    time0 = end0 - start0\n",
    "    print(f\"Epoch {n+1}\\n---------\")\n",
    "    print(f'Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')\n",
    "    print(\"len mean:\", result[\"episode_len_mean\"])\n",
    "    print(\"checkpoint saved to\", {file_name})\n",
    "    print(\"Time per epoch:\", time0, \"s\")\n",
    "    print('-'*100)\n",
    "          \n",
    "   \n",
    "end1 = time.time()\n",
    "time1 = end1 - start1\n",
    "print(\"总耗时：\", time1,\"s\")\n",
    "\n",
    "\n",
    "\n",
    "# 新绘图方法，hvplot，可视化效果更好\n",
    "# 绘制奖励函数变化曲线\n",
    "rewards_space = np.array(rewards_space)\n",
    "rewards_space = pd.DataFrame(rewards_space)\n",
    "rewards_space.hvplot.line(title = \"Reward Function\", xlabel = \"Epoches\", ylabel= \"Value\",\n",
    "                         height = 400, width = 800, line_width = 3,\n",
    "                         xlim = (-1, 101))   # ylim = (-(1e-4), 4e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
